[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Who am I?\nI’m a behavioral data scientist, writer, and content creator. I currently work as a data scientist for Universal Studios Orlando as part of their global strategies team. In addition to my day job, I work with my wife to consult folks who are looking to conduct survey research and quantitative analytics. Prior to that, I was a Director of Commercial Data Science at Morning Consult, where I led the data science work for our Advocacy and Government region. I earned my PhD in Political Science from the University of Florida in the fall of 2020, specializing in American Political Behavior and Research Methodology. I wrote my dissertation on the ways that video games can affect political attitudes and participation. It’s freely available online if the topic strikes you as interesting.\nApart from that, I am a husband to my incredible wife, Stephanie, a father to my boisterous daughter, Rosalina, fur father to a greyhound (Dude) and bombay (Asia), and a Quaker. (There. Now you can say that you know one who isn’t on the oatmeal box).\n\n\nWhat do I do?\nMuch of my work can categorized into four (often overlapping domains): Empirical, Methodological, Exploratory, and Ludic:\n\nMy empirical work focuses on political behavior. I am interested in the stuff that goes on in the space between our ears, how it is shaped by psychological, sociological, institutional, technological, and historical factors—and how it all comes to be expressed as attitudes and actions.\nMy methodological work is largely informed by the questions posed at my day job. (Some may call this relationship endogenous—and they’d be right). Accordingly, most of it centers around applied statistics and survey research: causal inference, survey experiments, multilevel regression modeling, scale construction and validation, time series forecasting, etc. Though most of my work is quantitative, my favorite stuff to consume and produce also incorporates qualitative and theoretical components. I’m deeply interested in the epistemological assumptions and implications undergirding our research practices as well as with the philosophical study of knowlege-generating practices more broadly.\nMy exploratory work tends to focus on sharing the science and history behind our endlessly fascinating social world. I love using my writing, YouTube channel, and data visualizations to tease apart—and find the common threads within—current events, academic research, pop-culture, and everyday observations.\nMy ludic work tends to boil down to puzzle-solving and tinkering. Luidic derives from the latin word for play (ludus), which pretty well describes what I do when I engage with it. Sometimes I stumble upon a question, idea, or challenge and I play around with models and solutions. I enjoy thinking about the interactions of actors, rules, incentives, and systems; I like to play within (and with) the constraints of everyday things to gain a deeper understanding of them—and sometimes create something new. I consider a lot of my programming, automation, and artistic outputs to fall under this category.\n\nWhen I’m not working, I can be found spending time with my wonderful family, reading, cooking, playing video games, training for my next road race, or overthinking popular media while binging it. (Often doing many of these things at the same time).\n\n\nWhy “Data and Other Fables”?\nGreat question! Fables are short, fantastical little yarns that speak to something true outside their context. The best data, given the right voice, are the same.\nThe title was inspired by two books and a quote: Economic Fables by Ariel Rubinstein, Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari and “All models are wrong, but some are useful” a la George Box. (Though if I had to trace the through-line further, I would say that it also owes some debt to the quip”artists use lies to tell the truth” from the film V for Vendetta. I was, uh, a bit into that movie growing up). The thread yoking them together is the idea that quantitative and theoretical models are fundamentally false in the same way that stories or other forms of narrative art are. At best, they are simplifications of reality. They have to be, lest they wind up being like Lewis Caroll’s map that’s a 1:1 replica of the terrain. That’s not to say that they contain zero truth—quite many powerful truths have been conveyed through fiction. Just that whatever true things are conveyed are only approximate articulations of the whole—and that these approximations come bundled up in emphases, omissions, explicit interpretations, tacit associations, and leaps of logic and faith. That is, comprised of all the stuff of human voice when it sets itself upon the task of storytelling.\nPlus, the title conveys the fact that I won’t just be talking data on this blog. If you couldn’t guess, I’ve got other interests outside of that. I like books, science, comics, heavy metal, philosophy, video games, poetry, art, and garbage anime. I figure that this can be just as much a home for reflections on that stuff as it is tutorials and sErIoUs CoMmEnTaRy."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\nHow to add a Variable Font to a Quarto PDF on Windows\n\n\nOr: How I was driven completely insane by Google Fonts\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\nWhat to do when R punches you in the mouth\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\nSimulating My Daughter’s First Board Game in Python\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\nBook Review: Theory and Reality\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\nThoughts on the 2020 Election\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\n\n\n\n\n\n\nHow to Do Unequal Randomization in Qualtrics Surveys\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\nSurveying is a Beautifully Foolish Endeavor\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\n\n\n\n\n\n\nCapturing a PowerPoint/Google Slides Lecture with Open Source Software\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data and Other Fables",
    "section": "",
    "text": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\nR\n\n\nstatistics\n\n\nsimulation\n\n\nsurveys\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nHow to add a Variable Font to a Quarto PDF on Windows\n\n\nOr: How I was driven completely insane by Google Fonts\n\n\n\n\nR\n\n\nQuarto\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nWhat to do when R punches you in the mouth\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nSimulating My Daughter’s First Board Game in Python\n\n\n\n\n\n\n\npython\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nBook Review: Theory and Reality\n\n\n\n\n\n\n\nreview\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on the 2020 Election\n\n\n\n\n\n\n\nelections\n\n\nreflection\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nHow to Do Unequal Randomization in Qualtrics Surveys\n\n\n\n\n\n\n\nsurveys\n\n\nqualtrics\n\n\nresearch design\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveying is a Beautifully Foolish Endeavor\n\n\n\n\n\n\n\nreview\n\n\nsurveys\n\n\nfiction\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nCapturing a PowerPoint/Google Slides Lecture with Open Source Software\n\n\n\n\n\n\n\ntutorial\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\nPeter Licari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data and Other Fables",
    "section": "",
    "text": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\n\n\nSep 13, 2023\n\n\nPeter Licari\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to add a Variable Font to a Quarto PDF on Windows\n\n\nOr: How I was driven completely insane by Google Fonts\n\n\n\n\n\n\nAug 1, 2023\n\n\nPeter Licari\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nWhat to do when R punches you in the mouth\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nPeter Licari\n\n\n25 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "other_things/dissertation/index.html",
    "href": "other_things/dissertation/index.html",
    "title": "Press B to March (Dissertation)",
    "section": "",
    "text": "Video games are one of the most popular entertainment mediums in America. The average player racks up an average of 7 hours a week and the industry hauls in billions of dollars a year. Despite decades of moral panic and hand-wringing that video games are corrupting the civic values of American youth, very little research has been done to actually see how games influence political behaviors: specifically, our attitudes and tendencies to participate in politics. This dissertation uses a mixture of quantitative and qualitative analyses to argue that certain game experiences—those that make players think about social, moral, and political issues as well as those that strengthen social ties—can affect political attitudes and inspire players to be more active in politics.\nThe dissertation uses a mix of survey data, archival work, content analysis, and randomized controlled experimentation to present 4 main arguments:\n\nJust like any other narrative medium, games tell stories to their consumers; stories that often concern or contain things that are relevant to our society at large. These stories are known to engender effects to our political behaviors when presented in the format of the news, movies, television shows, and novels. In this respect games are no different.\nUnlike those other forms of media, though, games are far more interactive. The player is acting upon their experiences in ways that they can’t with other media. They are active agents in these worlds. The socially, morally, and politically relevant content is often contingent or can otherwise be linked to the actions that players perform via their avatars. And the behaviors they practice and perform behind the screen can have ramifications beyond it.\nBut the effects of games are not limited to their content: video games have always been social experiences. They encouraging new relationships and strengthen those that already exist. We know that, generally, these behaviors lead to increased political action—and this dissertation argues that the networks established around games are no different. They also lead to increased activity through the creation and maintenance of social capital as well as increasing the opportunities people have to be exposed to political talk.\nThese effects are not haphazard. Game designers consciously put relevant content into their games and strive for their multiplayer experiences to create feelings of community. And they (treatments sociopolitical issues and multiplayer opportunities) can be readily found in the vast majority of the most popular games released from 2007–2017. And these effects are not rare or uncommon. Gaming is not just for the young, awkward, and/or antisocial. Nearly 2 out every 5 Americans play games that engage with sociopolitical issues—and more play games socially with friends.\n\nGames, in short, matter—in a lot of ways and for a lot of people. This dissertation provides one of Political Science’s first look into how and why.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/dumdum/index.html",
    "href": "other_things/dumdum/index.html",
    "title": "dumdum: an R pakage for dummy variables",
    "section": "",
    "text": "dumdum is an R package that makes making dummy variables easier. There are functions that do this sort of work in packages like caret, but I wanted to make a set of functions that would help other social scientists and/or analysts who were not as deep down the machine-learning rabbit hole.\nI made this fuction before I started doing more package development in R for my job. While I’m still proud of this little package—it was the best I could do at the time—I’m also proud of what I can do now. Such as waves arms at website.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/election-app/index.html",
    "href": "other_things/election-app/index.html",
    "title": "Florida Election App",
    "section": "",
    "text": "Inspired by all of the chaos and craziness that was mail-in and drop-off voting in the 2020 election, my students from my 2020 Election Data Science class and I created an app designed to help people find ballot-drop-off and early voting locations. The app received thousands of hits, helped at least a few people safely find ballot drop-off locations, and is now archived on my GitHub.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/kanye-map/index.html",
    "href": "other_things/kanye-map/index.html",
    "title": "2020 Write-In Requirements by State",
    "section": "",
    "text": "Inspired by all of the talk about whether Kanye could “successfully pull off a third-party independent run” (God remember when that was in the news cycle?), I made this tableau data visualization showing the requirements to be a write-in candidate in 2020. TL;DR: Kanye wouldn’t have been able to do it even if it were more than a transparent publicity stunt.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/wordle-letter-frequency/index.html",
    "href": "other_things/wordle-letter-frequency/index.html",
    "title": "Best Wordle Seed",
    "section": "",
    "text": "These are a pair of visualizations I made once Wordle started getting big (before it was bought out by the New York Times). There are a few different approaches to determining the best Wordle seed: The one I went with was to use the scrabble dictionary to identify 5-letter English words and identify the most frequently occurring letters as well as where they tend to occur.\n\n\nFor what it’s worth, I still use the seeds I identified from this analysis. My streak may have been broken by toddlers, illnesses, and busted phones but it hasn’t been done-in by the game itself: My accuracy rating is still at 100%.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things.html",
    "href": "other_things.html",
    "title": "Other Things I’ve Made",
    "section": "",
    "text": "Here are some of the things that I’ve made that aren’t blog posts and are outside the scope of my ongoing creative projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest Wordle Seed\n\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPress B to March (Dissertation)\n\n\n\ndoctoral dissertation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020 Write-In Requirements by State\n\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlorida Election App\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndumdum: an R pakage for dummy variables\n\n\n\nR Package\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/covid-video-lecture-tutorial/index.html",
    "href": "posts/covid-video-lecture-tutorial/index.html",
    "title": "Capturing a PowerPoint/Google Slides Lecture with Open Source Software",
    "section": "",
    "text": "A number of schools are either shutting down or transitioning classes to an online format due to the novel corona virus (also known as COVID-19). (UF just let us know Yesterday that, while it isn’t yet mandatory, we should start transitioning sooner rather than later). This isn’t going to be an easy transition for a lot of us—students and faculty alike. Not everyone makes living on the Internet basically a second job. I’m planning on writing a workflow for my intended solution—hosting lectures live in a Discord server with password-protected Google Forms as attendance—in the next few days. But I know that many people are going to feel that simpler is better for them. I got this text from a good friend of mine yesterday as we were talking about it:\nAbsolutely! And it’s easy to do with some Open Source software. In fact, you can do it with whatever your presentation software of choice (PowerPoint, Google Slides, LibreOffice, Beamer, etc.). This is especially helpful because, while PowerPoint offers an easy recording option, the same can’t be said for everything. So, for this post, I’m going to walk y’all through how to use OBS to record yourself giving a PowerPoint presentation so you can upload it to your students."
  },
  {
    "objectID": "posts/covid-video-lecture-tutorial/index.html#tips-for-your-presentation",
    "href": "posts/covid-video-lecture-tutorial/index.html#tips-for-your-presentation",
    "title": "Capturing a PowerPoint/Google Slides Lecture with Open Source Software",
    "section": " Tips for Your Presentation",
    "text": "Tips for Your Presentation\nAs I ever-so-slyly referenced in the last paragraph, I’ve been making multimedia content for the web for a while now—pretty close to about 6-7 years. I’m not by any means an unimpeachable expert, but I’ve picked up a few things as I’ve gone along. So I wanted to pass along a few tips for when you’re recording your presentation:\n\nKeep it brief: Unless you’ve got a helluva radio voice (and an engaging personality to boot), keep your presentation shorter rather than longer. The best video educators on YouTube don’t tend to go much longer than 10-15 minutes—and they’ve usually got some kind of visual animation and music to accompany them. You won’t keep your students’ eyes on the screen for much longer than that. Even if they do, their eyes will be totally glassed over or they’ll have Facebook open in another page. (That may not sound much different than class normally for some, but trust me. It is.) I know that’s frustrating because you probably have a lot more material than that. Trim it down as best you can.\nIf you can’t keep it brief, do it in chunks: If you absolutely cannot cut your material down to below 10-15 minutes, make multiple, shorter videos. Find natural stopping points in your presentation and cut your videos down to about 5-7 minutes. Set up a silent alarm for about 6 minutes that’ll help you wrap it up. This technique won’t let you get away with uploading, like, 10 videos—but you can use it to upload 3-6 that’ll get far better engagement than if you just kept talking for 20-30 uninterrupted minutes.\nDon’t eat the mic: watch how close you’re getting to your audio input. You don’t want your lecture to get interrupted by crackles or Darth Vader-esque heavy breathing.\nTalk slower. Then talk slower than that: Some people (like me) are cursed with a rapid cadence. When we talk to people face-to-face, it’s easier to understand us because a lot of our meaning is translated through non-verbal means. But when it’s just you and the slides, fast talking will be the death of comprehension. Plus, if you’re not used to presenting using a microphone, you’re probably nervous about it. Even if you don’t think you’re nervous, you probably are. That’ll bleed over into how quickly you’re going through the material. So if you just finished a video and/or chunk and are thinking “wow, I sure managed to clear through those slides really quickly!”—you’re probably going to have to re-record it if you want students to know what the hell you were actually saying.\nAvoid being monotone: Your students probably understand that this isn’t what you signed-up for; no one’s asking for an Oscar-worthy performance. And unless you’ve got experience with acting, improv, or public speaking, it’ll probably come off as more cringy if you shoot for one. But don’t be like the teacher from Ferris Bueller’s Day Off, droning on about the Smoot-Hawley Tariff Act. Try to add some variation in your voice every now and then. Change up your volume, pitch, and tone just as you would if you were speaking before a group.\nDon’t strive for perfection: Don’t try to make this a Ted Talk. Don’t try to make this a YouTube video. Don’t make this into a job talk or conference presentation. Don’t think that you have to have no pauses, no flubs, no imperfections. If you would say “oops, excuse me, what I meant was…” in class, you can say it here. It’s fine. 9 times out of 10, you’ll notice it far more than anyone else will.\nPractice before you upload: Do a couple of short practice runs (1-2 minutes) to make sure you’ve got your pacing, microphone placing, and all-around jitters sorted out before you dive in to the whole thing. As I mentioned, I’ve been doing this for years and I still have to do this. We all do. It’s better to get all the little kinks fixed before you upload your videos and you realize, to your sinking horror, there’s something that makes it utterly unwatchable.\n\nThis is probably going to be a stressful time for both you and your students. This transition (from offline to online) has been abrupt for and it’s having very real consequences on the day-to-day lives of everyone involved. Hopefully, this tutorial will be able to ease that transition, and diminish some of those smaller issues."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#scenario",
    "href": "posts/moe-subgroup-wrong/index.html#scenario",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Scenario",
    "text": "Scenario\nLet’s say that we are surveying some population of people with two sub-groups and we’re asking them about their favorite classic Pokemon starter plus Pikachu—because when you’re big enough to get your own balloon in the Thanksgiving Day parade, you get special privileges. So that brings us to four total: Charmander, Bulbasaur, Squirtle, and Pikachu. For the sake of this example, let’s say subgroup one are the folks who have played Pokemon and reflect 15% of the population. Among players, let’s imagine 40% choose Bulbasaur, 30% choose Charmander, 20% choose Squirtle, and 10% choose Pikachu4. Subgroup 2 reflects non-players and constitutes 85% of the population. 40% of these people choose Pikachu, 20% choose Charmander, 25% choose Squirtle, and 15% choose Bulbasaur. Among the full population then, 18.75% choose Bulbasaur, 24.25% choose Squirtle, 35.5% choose Pikachu, and 21.5% choose Charmander."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#creating-the-data",
    "href": "posts/moe-subgroup-wrong/index.html#creating-the-data",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Creating the Data",
    "text": "Creating the Data\nLet’s go ahead and simulate this population. I’ll be doing this in R (R version 4.2.2 (2022-10-31 ucrt)). I’m making it a total population of 10,000,000. The first part of the code chunk creates a dataframe that we’ll be using to sample from. The with_seed function comes from the {withr} package; I want to keep this reproducible on my machine since I’ll be using random sampling. The second part uses the {summarytools} package to make cross-tabs to illustrate that the data set construction worked as intended.\n\n\nShow the code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(summarytools))\nsuppressPackageStartupMessages(library(withr))\nsuppressPackageStartupMessages(library(questionr))\n\nwith_seed(13, {\n\nplayers <-\n  tibble(\n    type = \"Player\",\n    preference = sample(\n    c(\"Bulbasaur\", \"Charmander\", \"Squirtle\", \"Pikachu\"),\n    prob = c(.40, .30, .20, .10),\n    size = 1500000,\n    replace = TRUE\n  ))\n\n\nnon_players <-\n  tibble(\n    type = \"Non Player\",\n    preference = sample(\n    c(\"Bulbasaur\", \"Charmander\", \"Squirtle\", \"Pikachu\"),\n    prob = c(.15, .20, .25, .40),\n    size = 8500000,\n    replace = TRUE\n  ))\n\nfull <- rbind(players, non_players)\n\nfull <- full %>%\n  dplyr::mutate(pop_id = row_number())\n\n})\n\n\nctable(full$preference, full$type, prop = 'c')\n\n\nCross-Tabulation, Column Proportions  \npreference * type  \nData Frame: full  \n\n------------ ------ ------------------ ------------------ -------------------\n               type         Non Player             Player               Total\n  preference                                                                 \n   Bulbasaur          1276579 ( 15.0%)    599477 ( 40.0%)    1876056 ( 18.8%)\n  Charmander          1700113 ( 20.0%)    450589 ( 30.0%)    2150702 ( 21.5%)\n     Pikachu          3399748 ( 40.0%)    149398 ( 10.0%)    3549146 ( 35.5%)\n    Squirtle          2123560 ( 25.0%)    300536 ( 20.0%)    2424096 ( 24.2%)\n       Total          8500000 (100.0%)   1500000 (100.0%)   10000000 (100.0%)\n------------ ------ ------------------ ------------------ -------------------\n\n\nLooks like this worked great! The subgroup proportions and the overall group proportions are where we expect them to be."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#sampling",
    "href": "posts/moe-subgroup-wrong/index.html#sampling",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Sampling",
    "text": "Sampling\nNow let’s say that we’re going to sample 1,000 respondents from this population. But! We’d like to also do analyses of Players as a subgroup. We’d only expect to get about 150 people, which isn’t nothing but it’s not a whole lot either. So we’ll oversample an additional 500 game players for a total of 1,500 people. We’ll then weight respondents in our sample so that the overall values are unaffected.\n\n\nShow the code\n## Main 1,000 sample\nmain_sample <- full %>%\n  sample_n(1000)\n\nmain_sample_ids <- main_sample$pop_id # Ids of those sampled\n\n## Oversample of 500\noversample <- full %>%\n  filter(type %in% \"Player\") %>%\n  filter(!pop_id %in% main_sample_ids) %>% #no double sampling!\n  sample_n(500)\n\n## Full sample\nfull_sample <- rbind(main_sample, oversample)\n\n## Calculating weights\nweights_df <- full_sample %>%\n  count(type) %>%\n  mutate(weight_prop = ifelse(type %in% \"Player\",.15,.85)) %>%\n  mutate(weight = (weight_prop * 1500)/n) %>%\n  select(type, weight)\n\nfull_sample <- left_join(full_sample, weights_df, by = \"type\")\n\n\nLooking at the table below, which compares the weighted versus non-weighted and the actual population proportions, we can see that the weighting does its job; it brought the actual topline values to be far closer to the actual population values than the unweighted version did. Incidentally this shows why you should always downweight your oversample—otherwise, your overall values will be off by a lot more!\n\n\n\npreferencePop ValuesUnweightedWeightedBulbasaur0.18760560.24400000.1755069Charmander0.21507020.26666670.2249455Pikachu0.35491460.25466670.3345803Squirtle0.24240960.23466670.2649673\n\n\nBut the question isn’t whether downweighting an oversample gets us back in the right ballpark. It’s whether or not the purported MoE from the full 1,500 people is correct. If it is, we’d expect that, should this sample be conducted an arbitrarily large umber of times, 95% of those confidence intervals will contain the true population value."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#simulating-1000-pokemon-preference-surveys",
    "href": "posts/moe-subgroup-wrong/index.html#simulating-1000-pokemon-preference-surveys",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Simulating 1,000 Pokemon Preference Surveys",
    "text": "Simulating 1,000 Pokemon Preference Surveys\nComputers may be amazing, but they can’t do something infinitely many times. But, just like with random sampling, doing something a bunch of times instead will get us in the right ball park.\nSo let’s conduct 1,000 samples of 1,500 from our full population data—1,000 people randomly selected from the full data (players and non-players alike) and then 500 randomly selected just amongst the 15% of people that are Pokemon players. We’ll then weight these respondents correctly such that the players won’t have an outsized influence on the topline results. I’ll report out on 3 different MoEs.\n\nThe first assuming the oversample is included and not downweighted, calculated with the standard MoE.\nThe second weighting the oversample properly, calculated with the standard MoE.\nThe third weighting the oversample properly and calculating it with an MoE that takes the oversampling into account.\n\nGiven that these are 95% confidence intervals, we should expect approximately 95% of intervals to contain the true population value.\nThe chart below visualizes the results of the simulation. Each facet reflects a type of confidence interval (rows) and a Pokemon (column). The vertical bars reflect the MoEs for each of the 1,000 simulated samples. Their order up and down the Y axis is arbitrary, they happen to go bottom up from the first simulated run to the last. Moving horizontal on the X axis are smaller–larger proportions of the synthetic population. The black vertical lines reflect how popular those Pokemon truly are in our synthetic population. Horizontal lines that cross this gray reference line have successfully captured the population parameter. Those that miss have not accurately captured that Pokemon’s overall popularity in the sample. Since all of these are purportedly 95% confidence intervals, we would hope that they do so 95% of the time.\nIn short, the more solid lines we have, the worse we’re doing. And if we have more than 5% of sample that shows a solid line, then our MoE isn’t being constructed correctly.\n\n\n\n\n\n\n\n\nLooking at the first section with the Standard MoE things appear to be okay. Indeed, if we were to just eyeball it one might think that we’re actually getting 95% coverage. But we’re not. It’s actually only 91.4%. The degree to which this “standard” MoE is wrong will depend on how much we have to weight the sample versus an ideal case of perfect random sampling. This is known as the weighting efficiency. It’s possible for this to be closer to 95%. But it’s also possible for it to be further away depending on how much your weights have to work to bring values in-line with your population targets.\nIn the second section, we’ve actually hit what we were aiming for. Here, we took the design of the sampling into account and were able to achieve 95% coverage. Right on the money5. In practice, design-effect adjusted MoEs are slightly elongated to account for the aforementioned weighting efficiency. I’ll leave the technical details about that for another post. But, for now, know that it is not only possible but necessary to adjust a survey’s MoE when it has an oversample in order to accurately convey one’s uncertainty about the point estimate.\nLooking at the third row with the unweighted values, the coverage is absolutely abysmal. Only 25.2% of the confidence intervals contain the true population mean. And the only reason that it’s even this accurate is because Squirtle is about as popular with both players and non-players. Again, let it be said: ALLWAYS WEIGHT-IN YOUR OVERSAMPLES IF YOU’RE DOING TOPLINE VALUES."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html",
    "href": "posts/Qualtrics-random-groups/index.html",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "",
    "text": "Recently, some coauthors and I were working on a survey experiment in Qualtrics where we were assigning people into 4 different groups. This is something that Qualtrics can do really easily with its in-built randomizer function. If you use this, and keep evenly present elements on, your respondents will be sorted in roughly evenly. So if you have 16 respondents and 4 groups, on average you’ll get 4 people per group.\nBut we had a different situation. We had four groups but we didn’t want it split evenly. We wanted it so that group 1 was half of the survey, and groups 2, 3, and 4 took up thirds of the remaining half.\nUnfortunately, Qualtrics doesn’t seem to have an option that let’s you select unequal probabilities natively. If you deselect “evenly present elements” it’ll just randomize without any real concern towards whether your groups end up looking roughly equal. But that’s not what we needed. So I wanted to write about our solution to this so, hopefully, others in a similar situation might be able to find a work-around that works for them, too!\nOur solution used embedded data, then branches, and the standard randomizer. I’ll also talk about how you might be able to generalize this to any number of conditions (although some are more work than others)."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#starting-with-embedded-data",
    "href": "posts/Qualtrics-random-groups/index.html#starting-with-embedded-data",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "Starting with Embedded Data",
    "text": "Starting with Embedded Data\nAs you probably know: After you’re done with your Qualtrics survey, you can export it as a csv or as a xlsx file. In addition to the answers to your questions, it’ll also include some project and respondent metadata. But you can also program the survey for it to include you’re own custom metadata to be embedded in the project. You can do this by clicking on the “add new element here” button and adding embedded data to the project.\n\n\n\nOnce you’ve done that, you can customize your field to have whatever name you want. You can also set the value for this field. Here I set a variable named “test” equal to the value “1”. When I export the results for this survey, there will be a new column called “test” where the value for every respondent will be 1.\n Importantly, though, you can also set multiple conditions for the embedded data field! You do this by clicking “add below” and adding more embedded data sections.\n\nPop quiz! What would you get in the “test” column if you ran this survey as is?\nYou’d get 4 because, right now, you’re telling Qualtrics:\n\nInvent a column called test. Set it equal to 1.\nActually, remember test? Set it to 2.\nI lied. Make it 3 now.\nJk. 4.\n\nHere’s where we start using the randomizer."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#omg-so-random",
    "href": "posts/Qualtrics-random-groups/index.html#omg-so-random",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "“OMG so random…”",
    "text": "“OMG so random…”\nAs it stands now, you’re going to end up with a field called test where the value is set to 3 for all respondents. Instead of that, we can use the randomizer to make it so that 1/4 of the sample has a value of 1 for test, 1/4 has a value of 2, 1/4 has a value of 3, and 1/4 has a value of 4.\n You may be thinking “Cool. But this doesn’t do anything for me. I’m right back at equal probabilities for my experimental conditions.” And you’d be right if we were stopping here.\nBut we’re not stopping here.\n\nWe’re going to nest randomization conditions.\n\nThis programming is pretty similar to what we ended up using. Let’s follow the logic of the flow:\n\nRandomly pick either the test 1 condition with 50 % probability.\nIf it is not picked, then randomly select one of the remaining 3 conditions with 1/3 probability each.\n\nThis will mean that half of the respondents will get test1 and 1/6th will go to each test2, test3, and test4. Which is what the situation is in the second image and exactly what we were looking for in the real-life scenario.\nThis nesting is the key thing here. By nesting your randomizers strategically, there are few combinations that you won’t be able to achieve. This sort of set up can be really useful if you’re doing a multifactorial survey experiment.\nWe can wrap this all up with a neat bow through some branching."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#branch-it-out",
    "href": "posts/Qualtrics-random-groups/index.html#branch-it-out",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "Branch it out",
    "text": "Branch it out\nNow we can do some branches so that our participants only see the blocks that they’re intended to see. Then branches work off of boolean logic. If a condition is TRUE then it’ll run a particular course of action. If it’s FALSE then it won’t run that action. In Qualtrics, you can set the branching logic so that it reflects the metadata that was assigned to participants through the randomizer.\n After that, you can add the block that you wanted people with test condition 1 (and only test condition 1) to see. And if you repeat it out, then you can complete the whole survey flow. Here’s basically what our survey looked like once the logic is completed all the way to the end.\n\nOne of the things I really like about this approach (rather than, say, using a random number generator and a bunch of then branches) is that this makes your later analysis pretty easy. If you’re making pivot tables, you can group off of that variable’s value. If you’re doing ANOVA in SPSS, Stata, or R, you’ve got a single variable already to go without having to do any post-hoc coding. It also makes it easier to set dummy variables if you do a more advanced regression analysis. I’m personally a big fan of doing small-to-moderate amounts of work up-front to save myself larger amounts of work down the road. I’m a big believer that work borrowed from your future self compounds with interest."
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html",
    "href": "posts/quarto-custom-font-windows/index.html",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "",
    "text": "The Writing on the Wall\nCreated by Author & Easy Diffusion\nThis is one of those posts motivated by never ever wanting to subject anyone to the insanity that I experienced while struggling through what only seems to be a simple idea.\nIt was a simple dream, really. I was writing up a report in Quarto. I have an OpenType Google font that I’m fond of. I wanted to use said font in said document. Quarto supports custom fonts. It should be simple to put in my new font—right? RIGHT?!\n(Click here to go directly to the solutions. Keep reading on if you want a bit of commentary and description of the problem. If it sounds like I went crazy, it’s because I did.)"
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html#the-problem",
    "href": "posts/quarto-custom-font-windows/index.html#the-problem",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "The Problem",
    "text": "The Problem\nI’ve installed a Google font called Vollkorn—and I wanted to use it in a Quarto report that I was writing up. The Quarto documentation suggests that using custom fonts would be a pretty straightforward task: Just set mainfont in the YAML header to the name of your font family of choice and the fontspec Tex package would handle the rest on the back-end. And if I wanted to switch from the default font to something else already packaged with Windows (like Calibri or, because I’m just a troll at heart, Comic Sans), it’s very straight-forward!\n\n\n\n\nCode\n\n```{qmd}\n---\ntitle: \"Example\"\nauthor: \"Peter Licari\"\neditor: visual\n---\n\n## This is an example\n\nSphinx of black quartz, judge my vow!\n```\n\nOutput\n\n\n\nFigure 1: With standard font\n\n\n\n\n\n\n\nCode\n\n```{qmd}\n---\ntitle: \"Example 2\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Comic Sans MS'\neditor: visual\n---\n\n## This is an example\n\nSphinx of black quartz, judge my vow!\n```\n\nOutput\n\n\n\nFigure 2: With accursed font\n\n\n\n\nBut let’s take a look at what comes up when I try to use Vollkorn:\n```{r}\nrunning xelatex - 1\n  This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)\n   restricted \\write18 enabled.\n  entering extended mode\n  \nupdating tlmgr\n\nupdating existing packages\nfinding package for Vollkorn(-(Bold|Italic|Regular).*)?[.](tfm|afm|mf|otf|ttf)\n\ncompilation failed- no matching packages\nPackage fontspec Error: The font \"Vollkorn\" cannot be found.\n```\nWhich is incredibly frustrating because I can go to my Windows fonts1 I can clearly see the damn font there. All installed. Mockingly.\n\n\n\nFigure 3: Vollkorn and LaTeX mocking me by claiming that the former is not installed.\n\n\nIf I were to click-on and inspect the properties of the Vollkorn font and compare them to Comic Sans font, though, you’ll notice a couple of interesting things. First, as highlighted in yellow, the files are located in entirely separate roots! There are actually two ways we can solve this—but the bigger issue is with what’s highlighted in blue: Vollkorn is a Variable Font.\n\n\n\n\n\nFigure 4: File path for cursed font (Comic Sans)\n\n\n\n\n\n\n\n\nFigure 5: File path for Vollkorn\n\n\n\n\nVariable fonts are a specific kind of font that offers more flexibility for designers than simply having separate, static versions for, say, bold, Italic, bold italic, etc. It turns out, however, that the default Quarto pdf render engine, xelatex does not play well with variable fonts. And by well I mean at all—it straight-up doesn’t work.\nAnd what about the difference in file roots; what the heck is going on there? As I learned the hard way, this is actually standard behavior since at least Windows 10. When you install a font, you have to specify that you want it to be for all users. Only then will it go to the Windows/Fonts directory—which is where the fontspec LaTeX package looks by default. Otherwise, it’ll get installed within the AppData/Local directory. Hence why the error says the font couldn’t be found. I could find it, but it wasn’t where fontspec was looking! However, with Windows 11, the option to install for all users doesn’t initially appear when you right-click on the font to install it. Clicking “Install” will only install it locally.\n\n\n\nFigure 6: Standard install available at right-click won’t do what you think it should."
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html#sec-solutions",
    "href": "posts/quarto-custom-font-windows/index.html#sec-solutions",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "The Solution(s):",
    "text": "The Solution(s):\nThere are actually a couple solutions for this: One that’s relatively simple and another that’s more involved. I’ll go simple, then go more involved. (Even though that is the exact opposite way that I discovered these solutions for my own purposes…)\nBefore doing dealing with either the simple or involved routes though, let’s go ahead and knock out the file path; we’ll need to know where the file lives in order for either solution to work.\n\n1. Address the file’s, well, address.\nYou’ve got two options for this. The first is to uninstall the fonts2 and then re-install them so that they are installed for all users. You can accomplish this by selecting “Show more options” after right-clicking on the font file(s) you want to install. From there “Install for All Users” should be visible (it’ll often be accompanied by a little shield).\nIf you don’t want to reinstall the file, you can use the mainfontoptions parameter and redirect where fontspec is looking:\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    mainfontoptions:\n      - Path = C:/Users/prlic/AppData/Local/Microsoft/Windows/Fonts/\n      - Extension = .ttf \neditor: visual\n---\n```\nFor the remaining bit of advice, I’m going to assume that you’re going with the reinstall option. However, if you didn’t, all you’d have to do is make sure those two parts of mainfontoptions are specified.\n\n\n2. Handle the Variable Font Type\n\nThe easy way:\nWhile Quarto’s default Tex engine on Windows doesn’t play nice with variable fonts, one of the optional rendering engines, LuaLatex works well! So all you would have to do is specify the pdf-engine parameter:\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    pdf-engine: lualatex\neditor: visual\n---\n```\nWhy use xelatex over lulatex or vice-versa? There’s a couple interesting Reddit threads on the topic. It seems that LuaLatex is preferred but xelatex is a bit faster (possibly) and useful for backwards compatibility on older renders.\n\n\nThe more involved way\nBut let’s say that you’re committed to xelatex. Or you’re me and you didn’t realize that a way easier option existed the whole goddamn time prior to writing a blog post.3\nFortunately, many of the variable font styles available from Google Fonts also come pre-packaged with the so-called “Static” files (the aforementioned separate italic, bold, and bold-italic files). So instead of installing the variable-font, you would install all of the individual types, which you can set manually in mainfontoptions.\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    mainfontoptions:\n      - BoldFont = Vollkorn-Bold\n      - ItalicFont = Vollkorn-Italic\n      - BoldItalicFont= Vollkorn-Bolditalic\n      - Extension = .ttf \neditor: visual\n---\n```\n\n\nEt viola!\nEither way that you do it, you’ll be able to get the fonts loaded as you’d like!\n\n\n\nFigure 7: Success!\n\n\n\nI hope this was helpful to others and saved them the headache that I dealt with when trying to just make a simple report!"
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-understand-what-this-output-is-telling-me",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-understand-what-this-output-is-telling-me",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not understand what this output is telling me",
    "text": "I do not understand what this output is telling me\nThe first thing I do—and I know that this is going to sound just so incredibly pedantic, but please stay with me—is do my best to read the error message closely.\nDon’t get me wrong: A lot of these error messages have been written by software engineers—and software engineers aren’t known to be, uh, particularly verbose or descriptive. But, a lot of package developers, especially those in or adjacent to the Tidyverse, work to include more informative errors. But, even for these, you need to know what you’re looking for.\n\nDescriptive Errors\nLet’s say that I’m playing around with the mtcars dataset and I try to select a column that doesn’t exist:\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nmtcars %>%\n  select(type) %>%\n  head()\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `type` doesn't exist.\n\n\nThis error is very clear: it’s telling me that the column that I passed along to the select statement (type) is not a variable in the data frame. My resolution here is similarly clear: Fix what I’ve passed to the select statement so that I can get the proper output.\nA similarly descriptive error (at least once you’ve gotten the hang of it) will come up when you invariably put in one too many (or few) of some critical character in R: +, }, ,, ), ], etc. It will tell you that there’s an “unexpected character” somewhere and will provide you with where the incongruity is.\n\nggplot(mtcars, aes(x = wt y = mpg)) +\n  geom_point()\n\nError: <text>:1:27: unexpected symbol\n1: ggplot(mtcars, aes(x = wt y\n                              ^\n\n\nThis error is admittedly a bit tricky sometimes to get your head around at first because it ostensibly crops up in the absence of a symbol and not just the “unexpected presence” of one. But it makes more sense once you remember that y is indeed a symbol and it isn’t expected there—function arguments are supposed to be separated by ,s!\nNormally when I see unexpected symbol errors, my first thought is to make sure that I’m balanced on the number of left-hand symbols (i.e., [, (, {) and right hand symbols (], ), }); if you’ve got one on the left, you need one on the right and vice-versa. Then I’ll check for errant commas—or that they are where they’re supposed to be.\n\n\nLess Descriptive Errors\nSome errors require a bit more understanding of R as a programming language to parse before they become “obvious.” Say that I want to add a new column to the mtcars data. This will work:\n\nmtcars$new_col <- c(1,2)\n\nThis will not:\n\nmtcars$new_col <- c(1,2,3)\n\nError in `$<-.data.frame`(`*tmp*`, new_col, value = c(1, 2, 3)): replacement has 3 rows, data has 32\n\n\nYet this will:\n\nmtcars$new_col <- c(1,2,3,4)\n\nAnd, again, this will not.\n\nmtcars %>%\n  mutate(new_col = c(1,2))\n\nError in `mutate()`:\nℹ In argument: `new_col = c(1, 2)`.\nCaused by error:\n! `new_col` must be size 32 or 1, not 2.\n\n\nWhy does it matter that this vector has 3 and that the data has 32?! 2 and 4 seemed fine! And why did 2 work for base R but fail in the mutate context?\nWell, because R allows for this thing called “recycling.” Basically, if you try to put two unequal vectors together but one of them can evenly divide into the other, R will “recycle” the values over and over again until they fill out the space.\n\ndata.frame(x = 1:6, y = 1:2)\n\n  x y\n1 1 1\n2 2 2\n3 3 1\n4 4 2\n5 5 1\n6 6 2\n\n\n3 doesn’t divide evenly into 32, so an error crops up there. However, dplyr’s mutate doesn’t recycle values greater than 1. It’s a quirk of that package. You will come to learn many of the quirks of the packages you use most often. But, in both cases, R told you exactly what was wrong although it only told you why that was an issue with mutate.\n\n\nNondescriptive Errors\nLet’s say that you’re returning to your script in a new session. You were trying to filter mtcars by those with a value of 4 for the cyl column.\n\n\nWarning: 'dplyr' namespace cannot be unloaded:\n  namespace 'dplyr' is imported by 'tidyr' so cannot be unloaded\n\n\n\nmtcars %>%\n  filter(cyl == 4)\n\nError in filter(., cyl == 4): object 'cyl' not found\n\n\nOk. No problem. You’ll just go to a different part of your script where you saved a dataframe as df and looked at the top 6 values:\n\nhead(df)\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\nOk that’s not an error, but what even is it?!\nThere are other cases that I’ve come across where R will warn me about something going wrong with a Hessian, or that some object is or isn’t subsetable, or that something or another is singular. Sometimes, I will have absolutely no idea what this means! So what do I do? Google it!\n\n\nThe Common Solution\nUsually, I’ll copy the first full line or sentence of the error and pop it right into Google. However, I don’t usually copy in the full error if there’s text in there that I recognize as being driven by my specific input—like a column or object name. Including things that are only driven by what I’ve personally done in the search might reduce the number of helpful hits I’ll get.3 So I’ll go for the longest uninterrupted part of the error that doesn’t seem to contain something specific to my call.\nThis will usually bring me to a Stack Overflow or to another forum. Good ones tell you what to do with your code so that you can keep on trucking. The great ones are those that explain what the code they’re suggesting does on an intuitive level. The best ones do all that plus provide alternative ways of approaching the problem and/or explain in clear language why the original error came up in the first place."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-understand-the-output-plenty-but-dont-know-how-i-even-got-here",
    "href": "posts/r-punch-in-face-23/index.html#i-understand-the-output-plenty-but-dont-know-how-i-even-got-here",
    "title": "What to do when R punches you in the mouth",
    "section": "I understand the output plenty but don’t know how I even got here",
    "text": "I understand the output plenty but don’t know how I even got here\nLet me give an example of this happening on a project I did at my last job.\n\nRegressing in a Regression Problem\nI was working on functionalizing a type of analysis and spent some time with a logistic regression model. I wanted to predict outcomes from the approach I was testing to ensure it spat out reasonable estimates. First time I ran it, everything went fine—I got back a set of predicted probabilities all within the ballpark that I was expecting. I turned my attention to a marginal improvement I wanted to make in the function’s speed. Came back and re-ran the model. My predictions came back and everyone was either a 0 or a 1. Which is to say, not at all a reasonable set of predictions. Why did it just work before and why was it breaking all of a sudden?!\n\n\nData by Another Name\nHere’s another example that tosses an error that might be fairly familiar. Let’s say that I import in a data set and I try to summarize the mean values of a column I’m interested in (in this case, it’s the penguins data from the {palmerpenguins} package and I’m interested in bill_length_mm).\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.2.3\n\ndat4 <- penguins\n\ndata %>% \n  dplyr::group_by(species) %>%\n  dplyr::summarise(bill_length_mm)\n\nError in UseMethod(\"group_by\"): no applicable method for 'group_by' applied to an object of class \"function\"\n\n\nWhat do you mean a function?! I’m not passing a function, I’m passing data!\n\nOr, at least, I thought I was. This bit of code is short enough that you should be able to catch it—the data is called dat4 but I’m invoking data—which, until it’s assigned something else, is actually the name of a base R function. But if you’re working in a longer script and not being careful/deliberate with your names, this sort of thing can drive you crazy for a few minutes as you question your sanity.\n\n\nThe Common Solution\nThe causes for this sort of frustration usually boil down to people not working in what I’ll call a “linear” fashion. There’s a reason why folks who have been doing this a while insist that your script should be executable running it top to bottom without having to circle back or anything. But many practitioners (especially those who don’t have a strong coding background)4 will jump around in their scripts to tinker with this, try that out, maybe open up a new file and run some of the code in there. This also very frequently happens to folks who are new to RMarkdown or Quarto who have their test output coming out correctly because they ran chunks individually—but out of order. When it comes time to knit/render, they’re stymied by errors claiming that objects don’t exist but they can see the damn things in their environment—what do you mean they don’t exist?!.\nThe resulting analysis is one riddled with hidden dependencies or processes to work properly. This is pretty obvious with the second example, no data objects had been assigned to data so R still reserved that set of characters to refer to the function. (Which is actually a good tie-in to the first section: The error is telling you exactly what’s going wrong!). In the first example, my original data set had scaled predictors but then, in my quest to squash the minor thing, I overwrote that dataframe object to something with the same column names without scaled predictors. But I never updated the trained model. The result was that this “test” data had way more extreme values compared to the “training” data and, thus, everyone was given either 0s or 1s.\nMore often than not, if you’re getting an error or output that appears to make no sense in the context of the thing that you’re trying to do, the best advice is to take a short break and come back to the problem with fresh eyes. In order to prevent it from happening in the first place, frequently restart your session and make sure that you can get back right where you were with the code you’ve already written without having to do anything fancy. That means running your code top-to-bottom."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-this-code-return-the-type-of-output-i-expect",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-this-code-return-the-type-of-output-i-expect",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not know how to make this code return the type of output I expect",
    "text": "I do not know how to make this code return the type of output I expect\nA related, but distinct, issue from the previous one is when you have a clear vision in your mind for what you’d like to accomplish but you can’t quite seem to figure out how to get the data to behave the way you’d like. This often happens to me when I’m designing a function and I know how I want the output to look, but I’m having a tough time figuring out how to get there from the input. Another, perhaps more common, example is trying to ingest data and/or manipulate a dataframe so as to create something structured like the outcome you have in mind. (Say, for instance, that I’m trying to take wide-formatted data and pivot it into long formatted data, but I need summary statistics and only a select few columns). There are few things more frustrating than having a gist of what you’re hunting for but not being able to actually see it to fruition.\n\nCommon Solutions\n\nStart with the end in mind\nOftentimes these problems will come up because we are trying to get data of some sort “shaped” in a particular way—usually not for its own sake but to prepare it for some additional step. But I find that one of the most helpful first steps you can take is to take a moment to fully flesh out what this subsequent step will take for you to be successful. What type of input is necessary? Does it require a vector, a list, a dataframe? How many values, of what type(s), do they have to be named—if so, are their any requirements or restrictions? Can they have NA/NULL/Inf values? Sketch out5 what you need things to finish as. Just like solving a maze by starting at the end, sometimes this can clarify the direction you have to take.\nOnce you’ve done that, continue to work backwards. Compare the number of rows/columns/values in what you need compared to what you started with. Think about what data manipulation functions and/or summary operations you’ll need to do to go from each step to the next.6 This may require some trial and error so, once you’re done, please be sure to clean up your code so that it runs top-to-bottom so that you don’t run into the mistake we were just talking about in the previous section.\n\n\nStand on the shoulders of giants\nOftentimes, for the work performed the vast majority of the time, the maintainers of the packages you are using for your transformation(s) have probably created tutorials or vignettes on how to do at least some of the steps that you’re looking to do. You don’t need to relearn how to pivot tables from wide to long and vice-versa: that documentation already exists in a way to easily grock the concepts! So when I google for things with this type of headache, I tend to keep an eye out for tutorials (on personal blogs and R-Bloggers) and on package vignettes. You may have to string multiple vignettes together to fully traverse the gap between your start and ideal end-state, but these resources can be tremendously useful. My google strategy here is to type in “how to do xyz in R” where “xyz” will refer not to my overall aim but to the specific kind of function/operation that I want to perform to complete the next step. (And, if you know the package you’re using/need to use—add it to the search query).\nA tool that’s becoming increasingly useful for this is ChatGPT, especially GPT4, and other Large Language Models (LLMs). If you ask the model to not only help you take your input data and translate it to the output format you sketched earlier but to also explicitly detail the steps involved it will be able to do that pretty well. Here is an excerpt from an example of me doing this with GPT3 with scraping a table from Wikipedia.\n\nYou can also provide it your code as you currently have it and explain what you’re trying to do and it can help you debug and show you what to do instead to get to your desired endpoint.\nLLMs aren’t perfect, but if you have a sense of where you’re starting, and you know where you’re going, it can provide tremendous savings in your overall coding time. It may require you to troubleshoot some errors but it’ll often be a net benefit to have code that gets you 70-80% of the way there rather than try to code it all from scratch yourself. This may break down if you’re trying to do something particularly novel or niche though. In that case, it may help to return to the Internet and/or break the problem down into more discrete steps.\nIn any event: Whether you use an LLM, Google, or you brute-force it yourself—the most helpful solution to this I’ve found is to have a concrete vision of where you want to go, an understanding of where you are, and a roadmap for the discrete steps you will have to take to get there."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-r-do-something-that-i-think-it-should-be-able-to-do.",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-r-do-something-that-i-think-it-should-be-able-to-do.",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not know how to make R do something that I think it should be able to do.",
    "text": "I do not know how to make R do something that I think it should be able to do.\nFor me, very few things are as frustrating as knowing that I know how to do something but finding myself flailing when actually executing it. There’s a icy panic that hits as you grasp at tendrils of a solution only to realize that you’re merely catching smoke. Then a deep frustration, almost shame, at being failed by the organ that happens to also do the bulk of constituting what you think of as “you” —and cue you sitting here wondering why you’re under-performing: if you’re just tired, stressed, or experiencing early-onset dementia. (This happens to me a lot if you haven’t guessed).\nFor me, there are four common reasons for this to come up:\n\n1. You’ve done this kind of thing before, just can’t remember the steps.\nThis is why I unironically say that you should save every finalized line of code. Every. Line. And you should do your best to comment and format your code in such a way that you could return to it in 6 months and know what’s going on (e.g., using a consistent naming convention, commenting periodically, using names that make sense for your objects rather than just temp7). Storage is cheap and storing this will not be an issue for the vast majority of people who use R. There is absolutely no shame in going back and reusing your own code. Do not, I repeat, do not feel like you have to code every new project from scratch.\nIf you’ve lost your code or you didn’t format it so that it runs cleanly top-to-bottom, chances are that you had a little help before in the form of tutorials and vignettes. Ideally this time, you’ll vaguely recall a search string that at least got you going down the right rabbit hole (it helps if you remember where you got the info from so that you can add it to your search term).\n\n\n2. You’ve worked with this particular function before and are pretty sure its capable of doing what you envision.\nThe help documentation is your friend! For instance, I learned that {dplyr}’s case_when function has an option now for you to specify what the default output should be.8 But I couldn’t remember what it’s called. So I simply did this:\n\n?case_when\n\nWhich pops up a super handy pane that provides all of the help documentation the developers of the function have written for it.\n\nAnd, right there, we can see that the argument that I’m looking for is called .default!\nAnother example: I was trying to remove duplicated values from a dataset using {dplyr} ’s distinct. But when I got the data back, all I had was a single column. It wasn’t until I looked at the help documentation that I was reminded that the default behavior is for a parameter called .keep_all to be set to FALSE!\n\n\n3. You know that this package has a function that is useful but you can’t remember what it’s called.\nThis is where RStudio really comes in handy. If you type in the package name plus two colons (::) you can see all of the exported objects from the package. I frequently space-out on the name of the function is that drops NA values from dataframes. I know that it’s in the {tidyr} package though. So I’ll write out tidyr::…\n\nThat’s it! drop_na!\nThis helps me a lot when I have a vague sense of what the function I’m looking for is named and/or what it generally does. Worst case, I’ll just Google something like “how to drop NA values in R.” That is, I’ll do a search emphasizing the kind of action that I want to take.\n\n\n4. I feel like the sort of thing I want should be possible but I don’t know how to do it.\nI know that this will come as a terrible, Earth-shattering shock—but my recommendation here is to use either Google or a decent LLM. If you’re going the first route, it’s helpful to articulate what kind of end goal you have in mind. For instance, if I wanted to make a Waffle chart, I could Google “make waffle chart ggplot”. If I wanted to learn how to implement an upload box into my shiny app, I would go “add upload button shiny” or something to that effect. Even if I had a vaguer sense of what I wanted to do, I can type out what my end goal is and stick an “in R” at the end. (e.g., “how do I do bayesian regression in R?”, “how do I make interactive maps in R?”, “how can I connect to aws in R”, etc.). From there, find some code that others say works for the specific use case described on the page/in the forum/in the solution and apply it to yours. If you run into errors, follow the guidelines I’ve mentioned elsewhere in the post. As I mentioned at the top, learning R isn’t about knowing everything—it’s knowing that you don’t know, but learning that it’s necessary to ask!"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html",
    "href": "posts/rosie-game-sim-23/index.html",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "",
    "text": "Children learn as they play. Most importantly, in play children learn how to learn.\n- O. Fred Donaldson\n\nNot too long ago, my grandparents gifted my daughter her first board game: First Orchard. It’s a simple little game that aims to teach kids how to follow game rules and how to take turns, as well as reinforcing toddler basics like counting, hand-eye coordination1, and colors. Here’s the premise:\n\nThere are four, different colored “trees” in the gamespace (“orchard”; probably played on the floor) with four fruit apiece: Green apples, yellow pears, red apples, and blue plums.\nThere is a hungry crow that will eat all of the fruit if they make it into the orchard; they start the game five movements away.\nThere is a six-sided dice. Four of the sides correspond to the colors of the tree (green, yellow, red, blue), a basket that allows you to take from any tree, and one has a image of a crow.\nPlayers take turn rolling the dice. If they get a color, they take a fruit from the tree associated with that color. If all the fruit has already been picked, nothing happens. If a basket is rolled, then the player can pick a fruit from any tree. If the crow is rolled, the crow is advanced one space.\nThe game ends when all of the fruit has been picked from the tree (the players collectively win!) or when the crow reaches the orchard (the players lose).\n\nAs I spent time playing2 with my daughter, I realized that we were winning. A lot. Which made sense to me. One lesson that all children (and all us children grown tall) must learn is how to lose—but it’s admittedly far easier to teach a kid how to follow rules if they think there’s a strong chance that the rules will lead to success. (Though not a perfect chance so that the game is engaging and has risk and stakes). I wondered though: Were we just on a lucky streak or is the game actually biased towards the players? How long do the games usually take? Are there different average lengths between successes and failures? And if it is biased, how much more likely is success than failure?"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#simulating-the-game-in-python",
    "href": "posts/rosie-game-sim-23/index.html#simulating-the-game-in-python",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Simulating the game in python",
    "text": "Simulating the game in python\nIn order to see how generous this game is to the players, I decided to code it up in python.\n\nSet-up\nFirst I imported my libraries, made a quick little “roll the dice” function, and then setup an empty array for the results:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport scipy as sp\n\n\ndef die_roll():\n roll =  np.random.randint(1,7,1)[0]\n return(roll)\n\n\nresults = []\n\nFrom here, we can actually get down to articulating the rules/procedures of the game in python.\n\n\nEncoding the game rules in, well, code\n\nGame Start\nFirst, let’s get the game instantiated. We start with 4 different colored “trees” with 4 fruit apiece, a crow that needs to move 5 spaces before getting to the “orchard” (-5) , the number of turns that have elapsed (starting at zero), and the “orchard”—the collection of trees and the number of fruit that have yet to be picked (16 to start)\n\nred = 4\ngreen = 4\nyellow = 4\nblue = 4\ncrow = -5\nturns = 0  \n  \norchard = [red,green,yellow,blue]\norchard_sum = sum(orchard)\n\norchard_sum\n\n16\n\n\nThe game continues while either the sum of fruit in the orchard is greater than 0 or the crow’s distance is less than zero . So going to use a while loop. Every loop is going to be a turn; so every go round is going to increment turn and roll the dice.\n\nwhile orchard_sum != 0 and crow != 0:\n  turns += 1\n  roll = die_roll()\n\nDepending on how that roll lands we’re going to do one of a few things:\n\n\nAdvance the crow:\nMove the crow forward if the dice lands on a 6.\n\nif roll == 6:\n  crow += 1\n\n\n\nTake from a specific tree:\nSince I defined orchard as [red,green,yellow,blue], rolling a 1 will take from the red tree, 2 will take from green, 3 from yellow, 4 from blue—but only if the tree that was rolled actually has some fruit left! Otherwise, no action happens.\n\nelif 1 <= roll <= 4:\n  if orchard[roll-1] > 0:\n    orchard[roll-1] = orchard[roll-1] - 1\n  else:\n    continue\n\n\n\nTake from any tree\nThis one took a bit of thinking. What I decided to do was first ensure that all of the trees weren’t already at 0. If they weren’t then I decided to take from the tree with the most amount of fruit remaining. My thought being that, while it might feel good as a player to wrap-up a tree and bring it to zero, that would effectively prematurely eliminate one of the sides of the dice from doing anything productive. I had the feeling that a longer game would be biased towards the crow so I wanted to avoid that as much as possible.\n\nelif roll == 5:\n  if max(orchard) != 0:\n    largest_remaining = orchard.index(max(orchard))\n    orchard[largest_remaining] = orchard[largest_remaining] - 1\n  else :\n    continue\n\n\n\nEnding the turn (and game)\nFinally, determining the new sum in the orchard to determine if it’s time to kill the while loop. But, before the loop is killed, determining if the player or the crow is the winner.\n\norchard_sum = sum(orchard)\nif game_sum == 0:\n  winner = 1\nelif crow == 0:\n  winner = 0\n\nAnd then, once the loop is broken, recording whether the player was the winner and the number of turns it took.\n\nresults.append({'winner':winner, 'turns':turns})\n\n\n\n\nRunning the simulation\nLet’s go ahead and run this 10,000 times to get a sense of the game’s asymptotic behavior. You can click on the arrow to see how all the code comes together.\n\n\n10,000 simulated games\nnp.random.seed(131313)\n\nfor x in range(0,10000):\n\n  red = 4\n  green = 4\n  yellow = 4\n  blue = 4\n  crow = -5\n  turns = 0\n  \n  orchard = [red,green,yellow,blue]\n  orchard_sum = sum(orchard)\n  \n\n  \n  while orchard_sum != 0 and crow != 0:\n  \n    roll = die_roll()\n    turns += 1\n  \n    \n    if 1 <= roll <= 4:\n      if orchard[roll-1] > 0:\n        orchard[roll-1] = orchard[roll-1] - 1\n      else:\n        continue\n      \n    \n    elif roll == 5:\n      \n      if max(orchard) != 0:\n        largest_remaining = orchard.index(max(orchard))\n        orchard[largest_remaining] = orchard[largest_remaining] - 1\n      else :\n        continue\n  \n    elif roll == 6:\n      crow += 1\n    \n    \n    orchard_sum = sum(orchard)\n\n    \n    if orchard_sum == 0:\n      winner = 1\n    elif crow == 0:\n      winner = 0\n      \n  results.append({'winner':winner, 'turns':turns})\n\n\nAnd convert the results dictionary into a pandas dataframe\n\ngame_data = pd.DataFrame(results)"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#exploring-the-results",
    "href": "posts/rosie-game-sim-23/index.html#exploring-the-results",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Exploring the Results",
    "text": "Exploring the Results\nLet’s take a look at how often people win the game!\n\nsb.catplot(data = game_data, kind = \"count\", \nx = \"winner\")\n\n<seaborn.axisgrid.FacetGrid at 0x2a72bc2b7d0>\n\n\n\n\n\nSeems like the players are winning it about 65% of the time! Well\n\nprint(100 * game_data[\"winner\"].mean(),\"%\")\n\n63.27 %\n\n\nSpecifically.\nLet’s see if the number of turns vary:\n\ngame_data.groupby([\"winner\"]).mean().style.set_properties(**{'color':'black'})\n\n\n\n\n  \n    \n       \n      turns\n    \n    \n      winner\n       \n    \n  \n  \n    \n      0\n      18.655323\n    \n    \n      1\n      22.012170\n    \n  \n\n\n\nIt looks like it takes longer for the player to win over the crow; if the crow’s going to win, it’s going to do so in about 19 turns. If the player wins, it’s going to take about 22 turns. May not sound like much but you try to keep a toddler’s attention for three extra turns!\nGiven how many simulations I ran, I doubt that this will be anything other than “statistically significant” but it couldn’t hurt to check.\n\nsp.stats.ttest_ind(game_data['turns'], game_data['winner'], equal_var = False)\n\nTtest_indResult(statistic=489.4841664968918, pvalue=0.0)\n\n\nLMAO a t stat of 489. NOW THIS IS POD RACING P-HACKING."
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#conclusion",
    "href": "posts/rosie-game-sim-23/index.html#conclusion",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Conclusion:",
    "text": "Conclusion:\nIt looks like my daughter’s first game is pretty strongly biased towards the player winning! Simulating the game 10,000 times reveals that players can expect to win about 65% of the time. Crows tend to win faster than players; if the fruit gets munched by a pesky peckish corvid, it’s going to happen sooner rather than later. This suggests to me that if the crow wins in this particular game, it’s because the player has a streak of bad luck. I might at some point tweak the rules a bit or record more granular data to delve a bit deeper, but I think I’ll cut this for now so I can go back to, you know, playing the actual game with my daughter."
  },
  {
    "objectID": "posts/surveying-beautiful-endeavor/index.html",
    "href": "posts/surveying-beautiful-endeavor/index.html",
    "title": "Surveying is a Beautifully Foolish Endeavor",
    "section": "",
    "text": "I binged Hank Green’s A Beautifully Foolish Endeavor yesterday and it is, let me tell you, just fantastic. I don’t plan on writing a longer review because I didn’t really write any notes. I didn’t want to take notes. Not because it was bad, but because I didn’t want to peel my eyes away from the page for the short seconds it’d take for me to grab my pen and notebook. I was too engrossed to want to do anything but live in its pages for however long it’d let me. So, no review—but maybe that ardor could stand in place of one.\nBut aside from just being human, well-written, and the right mix of cynical and optimistic, it is also just an eminently and endlessly quotable book. There are quotes here that I plan on using when I talk about machine learning in my upcoming data science class. Quotes that just stabbed me in the chest with self-recognition (cough cough Miranda being a workaholic cough cough). And then there are quotes that make me feel that Green is one of the few science fiction authors who actually understand the sociology and psychology of being a practicing scientist (which, ya know, SciShow and his M.S.—so it makes sense). But then there’s this quote near the end of the book that so perfectly described being a survey researcher that it made me laugh out loud:\n\n[We wanted to] dive deep into our survey responses. Except there were just too many…no matter how we filtered, there just wasn’t a good way.\nApril and I were griping about this around the black marble countertop in the kitchen when Carl came in and overheard us.\n“Just do a search,” Carl said.\n“The searches take forever and we don’t know what to search for,” April replied. “It’s just a bunch of dumb data. Most of the useful stuff is in text responses, which is impossible to parse.”\n\nI loved and loathed the reductionism of “dumb data”—mostly because it’s right. Until we data geeks make the decisions on what to model, how to model it, and how to frame and embed our results into a digestable narrative, it is literally just that. Dumb data, (sometimes) intelligently collected. And also, it’s so, so spot-on to note that almost all of the really juicy stuff is in the open-ended responses where people just let you know what’s actually going on. Or at least their inwardly-biased interpretation of their messy cognitive processes—which is still super cool and really interesting!\nAs an example: for my dissertation, I had completely written-off games like Hearts as sociopolitically unimportant until some respondent mentioned that there are online lobbies where people use the game to chat about politics. I never would’ve learned that if I didn’t bother to read what my respondents had taken the time to write. And there have been a few papers that make the point that we should really pay more attention to the nuance that people write in rather than (often incorrectly) discretizing them on the basis of hardline, ex ante coding rules.\nI’m envious that April and Maya had an alien superintelligence to parse through all of that data and get to all the juicy bits. Sure we have NLP and algorithims for market segmentation—but do they come from a near-omniscient monkey? Didn’t think so.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{licari2020,\n  author = {Peter Licari},\n  title = {Surveying Is a {Beautifully} {Foolish} {Endeavor}},\n  date = {2020-08-03},\n  url = {www.peterlicari.com/posts/surveying-beautiful-endeavor},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeter Licari. 2020. “Surveying Is a Beautifully Foolish\nEndeavor.” August 3, 2020. www.peterlicari.com/posts/surveying-beautiful-endeavor."
  },
  {
    "objectID": "posts/theory-reality-book-review/index.html",
    "href": "posts/theory-reality-book-review/index.html",
    "title": "Book Review: Theory and Reality",
    "section": "",
    "text": "Theory and Reality is an ambitious book that aims to accessibly cover a large swath of historical and philosophical ground. In many ways, it lives up to that ambition, making it well worth the read. In other, important ways though, the book occasionally devolves into muddy paste, sometimes making a re-reading necessary to fully grok what’s been covered.\n\nWhen I went to graduate school, my very first class was called “Scope and Epistemology”—or “Scope” for short. It was a rite of passage for all who entered UF’s political science PhD progam. Taught by the brilliant (and brilliantly kind) Larry Dodd, the course syllabus was often mistaken as one of its textbooks. The damn thing was 30 pages and outlined thousands of pages of books and articles, from Kuhn to Gleick to Mayr, Gattone, and even freaking Asimov, that meant to show us not only the diversity of practices in social science but the equally remarkable diversity of beliefs over just what the heck this thing called “science” actually is.\nIt was a whirlwind tour of epistemology, philosophy, and sociology of science. (Predominantly social science given, ya know, it was a poli sci PhD program—but not exclusively so, much to the course’s benefit). I was not unfamiliar with either philosophy or epistemology (I was a hair’s breath away from double-majoring in philosophy in undergrad), but it was an intimidating program. We were quickly reassured by more advanced students that, no, we didn’t have to “read” all of it—at least not in the sense one would read things in undergrad (a time-honored advisory that I would oft repeat many times myself to newer students, furthering the tradition). I still own many of those books and refer back to a few when I feel the need. The course cemented my latent loves of philosophy, epistemology, and science into (what I’m sure to be) a lifelong fascination: one focused with not only practicing social science but also interrogating “what even is this thing we call (social) science?” in philosophical terms.\nMany of us came to realize that the syllabus for Scope was less a contract of expectations and more a gift. That “whirlwind tour” was a synthesis of decades of epistemological tumult that Dodd had lived through first hand. And since the issues were far from settled, I feel like he was also giving us a preview into many of the ideas and controversies we would have to grapple with ourselves over the course of our scientific careers.\nThis love is what drove me to pick up Theory and Reality by Peter Godfrey-Smith (2003) from Amazon’s Audible service. 1Theory and Reality is an ambitious book that aims to accessibly cover a large swath of historical and philosophical ground. In many ways, it lives up to that ambition. In other, important ways though, the book at times does feel like it devolves into, to use Godfrey-Smith’s own verbiage, a muddy paste. There were times where I would relisten to chapters (sometimes three times) just to make sure I fully understood what was covered. I feel like the book rewards this effort, but not everyone will want to exert it. Still, what it does well, it does very well—making this book well worth the read.\n\nAt the outset, Godfrey-Smith expresses that his overarching aim in writing this book is to convey a history of the philosophy of science; specifically a history in two senses of the word. The first is a timeline of the predominating ideas of the discipline (as well as from adjacent fields such as sociology of science): How have we conceptualized this thing we call “science,” as well the actions (both idealized and actual) of its practitioners? The second is a narrative of how said ideas (and their progenitors) interacted with each other.\nThis is more or less what he manages to do in the first half(ish) of the book. The first two substantive chapters start with brief discussion of early 20th century empiricism and its major philosophical tenets and issues (e.g., the issues of induction and confirmation), shift into a chapter on Popper, two on Kuhn (or, rather, two primarily on The Structure of Scientific Revolutions), a chapter on Lakatosh, Laudon, and Feyerabend (not apiece—collectively), a short chapter blitzing through the sociology of science in the middle third of the century, then a chapter on feminist contributions to the subject, before pivoting to naturalistic philosophy. He doesn’t do so as a impassive retrospective observer though; he frequently interjects the discussions with both critiques and endorsements to the material. This includes both those that are generally accepted in the field as well as some of his own—though his is admirably careful and forthright in which views fall into which category. He does have a tendency to be a bit dismissive of some ideas without motivating clear reasons why (and these instances are highlighted due to the equally discernible tendency for other objections to get more strenuous refutations), but his contributions, on balance, add rather than detract from the quality of the section.\nThe core argument of this first section can be crystalized as follows: The early-to-middle(late) century for philosophy of science was characterized by a schism between empiricists (e.g., positivits) and those who saw science as more an expression of social patterns rather than a distillation of empirically-identified phenomena. Both sides had incredibly novel and important insights, but both often proffered leads that turned out to be dead-ends—or overplayed their philosophical hands, cornering themselves into very strange positions (Popper’s articulation of corroboration and some sociologists’ assertion that the world is in fact created at a foundational level by the theories and paradigms of scientists2).\nIt’d be a lot of ground to cover even without the additional commentary. But Godfrey-Smith manages to do so at a high average quality, albeit with noticeable variation. Perhaps it owes to my experience in Scope and in my undergraduate coursework prior to grad school, but I found it pretty easy to follow-along with the first few chapters—and I found the most of the interjections to be illuminating. (I especially liked his approach to the Black-Raven, White-Shoe issue in accounting inductive evidence—especially the version detailed late in the book when he discusses evidence). He was at his best when he dedicated his time to discussing narrower philosophical issues or the contributions of individual philosophers. The chapters on evidence, Popper, Kuhn, and the ensuing Lakatosh-Laudon-Feyerabend lightening round were some of the best in the book. Where things got less good (though not necessarily “bad”, for my view) is when he tried to cover entire fields in singular chapters. The Sociology of Science and Feminism chapters were among those aforementioned thrice-listens. I initially thought that it was because I was unfamiliar with the material, then I thought it was because I just wasn’t grokking it; but on the final time I realized that he was simply moving way too fucking fast. I sympathize with the challenge of distilling such large amounts of work into such a a condensed space—but considering he made two chapters out of Kuhn alone, it seems like a self-inflected wound with an obvious remedy: Split it out into a couple more chapters.\nIf history was Godfrey-Smith’s sole aim, the book could have been cut here. But that ultimately isn’t his full intention. The second aim of the book is to discuss the contemporary paradigms (or perhaps I should say “research traditions”) that dominated philosophy of science at the writing of his book—and they largely continue to hold sway today. The final aim is his advancing his own solutions to the core problems he identifies in the second part—though they all have roots in the discussions predominating the first. Throughout the book, he refers to these aims as “thirds” of the book—but, in reality, it’s more like history takes up the first 45% of the book before pivoting to\nThe chapter discussing naturalism more or less coincides with the transition into this pivot. Naturalism’s uptake comes nearer the end of the 20th century; but so do the other new agendas and perspectives that he focuses on. So he more or less abandons the chronological approach he took with the first section (though this doesn’t come as a surprise; he announces it as it happens). The chapters to come tackle “naturalistic” philosophies broadly, first on its own terms and then with regards to the “social structure of science;” It then pivots to “scientific realism”, a chapter on “explanation”, one on Bayesianism and evidence, and then one trying to harmonize the differences between empiricism, naturalism, and scientific realism. In this chapter, he retells the story of a reviewer warning that this final chapter has the potential to devolve into a “muddy paste.” As you might be able to guess from my use of the phrase earlier, it’s a critique that I largely agree with.\nBut the thing about paste, muddy or otherwise, is that it’s still pretty sticky. So while the concepts tended to blur together here3, the chapters does a pretty good job at having you stick around. This was some of the most engrossing material in the book, and I found my head moving quite vigorously at different parts of the book. It was mostly vertical when talking about how the aim of science is to use different “representational vehicles” (some linguistic, some mathematical, some visual, some computationally simulated, etc.) to provide an accurate representation of phen omena and relations observed in the real world—and when defending the broader tendency of science to invoke heretofore unobservable structures as part of a broader aim to explain what happens in the world. It did shake in the other direction when discussing Bayesianism because, at times, the approach appeared too glib and dismissive (such as the odd treatment of “Dutch books” and the thin objection to it on the grounds that some people may be averse to gambling)4. And I definitely found myself agreeing with the general conclusion to the book: That science is both an individual exercise and a social one. That scientists do aim to model the real world and that it is possible (and indeed advisable) to conditionally accept inductive arguments that have survived repeated testing with risky procedures. But also that scientists are engaged in a foundationally social enterprise and that we can only best understand their work (and the philosophy of science as well) as being influenced (and at times, constrained) by these social forces.\n\nAll in all, I think Theory and Reality accomplishes what it sets out to do, though with varying degrees of clarity and persuasiveness. What it does well, it does very well—and there are moments where it manages to do exactly that throughout the whole book. But those moments are more heavily concentrated in the first half and in the concluding segments of a few of the final chapters; these successes, and their asymmetric spread, also serves to more severely accentuate the places where it doesn’t do as well. Places where the positions either expressed or taken weren’t as clear. Places where it felt too rushed to do such a large chapter-topic justice.\nBut I believe that this book is well deserving of a read by those interested in the philosophy of science broadly. It is a good survey of many ideas in the field, it incorporates and takes seriously competing intellectual programs that don’t often get coverage in other locations (i.e., feminist epistemology and critiques of science), and offers persuasive accounts for what scientists do, how, and why. Indeed, when I finished the book my first main feeling was sadness. Not because it was bad (because it was quite good), not because it was over (because it’s hard for any philosophy book to be that good) but because, just this last year, Larry Dodd retired from his position at UF. I was sad because this would have been a perfect addition to the Scope syllabus had he still been teaching it. But it has inspired me to keep better track of books like these so that, maybe someday, I’ll also be able to offer interested folks a suite of books that track the evolution of knowledge (both what we know and how we know) over my own career. I think it speaks very highly of this work that I think it has the potential to assist in cementing in others the same love of philosophy, science, and philosophy of science that Scope did for me.\nThough, if I ever get the chance, I promise that I’ll do my best to stay shy of 30-something pages5.\n\n\n\n\nFootnotes\n\n\nThe age of the book may discourage some from picking it up. Honestly, though, I didn’t even know it was this old as I was reading it.. It honestly reads more like a book from the time of its release onto Audible (2017) than from nearly two decades prior. I’m not sure if that is a comment of Godfrey-Smith’s perspicaciousness as a writer (and his ability to distill arguments into their more “timeless” elements) or on the rate of change in the discussed fields. To take a page from his book—literally—it’s probably something in between.↩︎\nThough I did at times wonder if he was taking this argument a bit too far and, consequently, sparring with its straw man. I think he has a fair critique when he observes that theories and frames can strongly influence what we see but that their strength is fundamentally constrained by a shared external reality.↩︎\nI feel like a lot of the differences between contemporary empiricism, naturalism, and scientific realism tend to either be real but highly technical or largely imagined but wielded fiercely by invested practitioners. In either case, largely inconsequential for the lay reader.↩︎\nFor the curious, this could be articulated quite easily as someone merely having a larger degree of faith in the continuation of the status quo and in greater expressed uncertainties in alternatives to it.↩︎\nI solemnly swear: 25 pages max.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{licari2022,\n  author = {Peter Licari},\n  title = {Book {Review:} {Theory} and {Reality}},\n  date = {2022-11-07},\n  url = {www.peterlicari.com/posts/theory-reality-book-review},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeter Licari. 2022. “Book Review: Theory and Reality.”\nNovember 7, 2022. www.peterlicari.com/posts/theory-reality-book-review."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html",
    "href": "posts/thoughts-prior-2020-election/index.html",
    "title": "Thoughts on the 2020 Election",
    "section": "",
    "text": "I thought I would post my prediction for the election and a few thoughts regarding it more generally. I’ll be peppering in some maps that I’ve made to help illustrate my general thought process."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#which-states-im-most-likely-to-be-wrong-about.",
    "href": "posts/thoughts-prior-2020-election/index.html#which-states-im-most-likely-to-be-wrong-about.",
    "title": "Thoughts on the 2020 Election",
    "section": "Which states I’m most likely to be wrong about.",
    "text": "Which states I’m most likely to be wrong about.\nThe states I’m most likely to get wrong are probably Georgia, North Carolina, Florida, and Maine-2. I think Maine-2 will be a decent indicator of how well the GOP’s senate chances will fair though; if that goes Blue, Collins is probably toast and any hopes that incumbents will be insulated from down-ballot effects stemming from Trump most likely go out the window. I’m less bullish on the senate flipping than others, but that will be a fair indicator of a bad night for the GOP all around."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#biden-has-a-strong-wall",
    "href": "posts/thoughts-prior-2020-election/index.html#biden-has-a-strong-wall",
    "title": "Thoughts on the 2020 Election",
    "section": "Biden has a strong wall",
    "text": "Biden has a strong wall\nThe reason I’m as bullish as I am for Biden (besides the appeal of the alliteration) is that the current toss-ups can literally all break for President Trump and Biden would still win—provided there aren’t any major surprises in the polling.\n\n\n\nNot all of these states are equal with regards to the President’s chances. No state’s return is independent of the others—if Biden won Texas, that means we’re looking at a bona fide blue malestrom, let alone a wave—but some states give more information about how other states are going to break than others. FiveThirtyEight has a tool that uses the correlations among polling errors to show how a victory in some states ripples out to signal what’s happening in others. A Trump win in Georgia doesn’t shift things much, nor does one in Texas, because frankly it’s a shock that they’re even in play. A win in Florida, however, drastically lowers Biden’s chances and suggests a map that looks like the one above. However, even in that case, the President is still not favored to win if only for the fact that Biden is still leading in the Electoral college.\n\nThe upshot however is that if Trump loses any of those states, his only path to victory lies in there being large, unprecedented degrees of polling error.\nI know, I know. Some of you might be thinking: “But Peter! The polls in 2016…” to which I will reply (for what I pray to all that is good and holy is the last time) that even the state level polls did reasonably well in 2016 in the aggregate—and more pollsters have been doing more high quality state-level polls than last time. There have also been adjustments across the industry to account for the biggest systematic source of polling error in 2016, which was not properly accounting for Trump’s support among White, non-college educated voters."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#bidens-best-cases",
    "href": "posts/thoughts-prior-2020-election/index.html#bidens-best-cases",
    "title": "Thoughts on the 2020 Election",
    "section": "Biden’s best case(s)",
    "text": "Biden’s best case(s)\nThe best case scenario for Biden is that he picks-up all of the toss-ups. But I should really emphasize that I don’t think this is a particularly likely outcome.\n\n\n\nA more realistic “great day” scenario for Biden is that he wins picks up Florida, North Carolina, Arizona, and Georgia while Trump gets Texas, Ohio, and Iowa.\n\n\n\nJudging from the correlations in polling errors, Florida will be a good (but not perfect) indicator of how Georgia and North Carolina go. (Georgia could be called first but that doesn’t necessarily mean that Florida will go with it; the leverage one state has over another is asymmetric. For example: North Carolina going red/blue means less for Texas’ eventual outcome than Texas going red/blue means for North Carolina.) Ohio will probably provide a good (but, again, not perfect) indication for how the other three states will break."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#without-pennsylvania",
    "href": "posts/thoughts-prior-2020-election/index.html#without-pennsylvania",
    "title": "Thoughts on the 2020 Election",
    "section": "Without Pennsylvania",
    "text": "Without Pennsylvania\nJust because I expect the President to lose doesn’t mean I think his loss is inevitable. But it’s tough. Really tough. And it’s made all the tougher because he’s currently not favored in Pennsylvania. And by “not favored in Pennsylvania” I mean that The Economist is giving Biden a 95% chance of carrying the state. Trump will have to outperform the polls by a strong margin to carry the day. The most likely states, given the margin of error for the most recent state level polls, is Nevada, New Hampshire, Maine-2 and Nebraska-2. That will give Trump 270 votes in the Electoral College.\n\n\n\nBut I want to stress how unlikely that is. Many of these wins are not all that correlated with each other—meaning that a win in any of them early on doesn’t necessarily portend a Biden defeat. (Other state combinations are also possible, but we’d be talking about even less likely polling errors.)"
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#with-pennsylvania",
    "href": "posts/thoughts-prior-2020-election/index.html#with-pennsylvania",
    "title": "Thoughts on the 2020 Election",
    "section": "With Pennsylvania",
    "text": "With Pennsylvania\nA trump victory in Pennsylvania dramatically improves the President’s chances. If the President wins there, it’s likely that he’s winning in a lot of other places that he’s currently not favored. However,it should be noted that even then he’d still need to pick up every one of the toss-up states. That can be helped by the fact that winning Pennsylvania would suggest a large degree of errors in the polls. But that degree of error is, I have to emphasize, really improbable. This outcome isn’t necessarily less likely than the one above, but pulling-off Pennsylvania would be an incredible upset. And, honestly, I would probably need to seriously reevaluate my life choices should it happen: especially my chosen profession.\n\n\n\nThe other point to consider when weighing out the importance of Pennsylvania is just how long that would make the contest. Basically, if Trump wins every toss-up state, then we’re not going to know the election’s outcome until about the end of the week at earliest. Because, as mentioned above, Pennsylvania counts their early votes last and haven’t started counting them at all. If Trump is ahead by the end of the night, we’ll just have to wait and see if the Biden advantage banked-up from early voting is enough. (Although, as a Floridian, I must confess that it’ll be nice to have all of the attention away from us for once.)\n\nIn short, in order for the President to win, he’s going to have to pull-off one helluva upset. That doesn’t make it impossible (upsets happen all the time) but, by definition, it makes it unlikely."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "This page houses the non-blog things I’ve written. Some are op-eds, others are in-depth analyses published in Medium publications (or self-published on the platform), others still are peer-reviewed. The list is more or less comprehensive from 2020 on. Though, prior to that, the things that aren’t on here…well, you’re honestly not missing much."
  },
  {
    "objectID": "writing.html#section",
    "href": "writing.html#section",
    "title": "Writing",
    "section": "2022",
    "text": "2022\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari (2022). “States with Anti-Abortion Trigger Laws Score Higher on Measure of Hostile Sexism”. 3Streams   link  github\n\n\n\nPeter Licari (2022). “I ran 80,000 simulations to investigate different p-value adjustments”. Towards Data Science   link  github"
  },
  {
    "objectID": "writing.html#section-1",
    "href": "writing.html#section-1",
    "title": "Writing",
    "section": "2021",
    "text": "2021\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari (2021). “White liberals view other races more warmly than they do Whites. Why?”. Medium (Self Published)   link\n\n\n\nPeter Licari (2021). “Political”Independents” Aren’t Moderates”. Medium (Self Published)   link\n\n\n\nStephen C. Philips, Alex P. Smith, Peter Licari (2021). “Philadelpha reconsidered: participant curation, the Gerry Committee, and US constitutional design”. Public Choice   link  pdf"
  },
  {
    "objectID": "writing.html#section-2",
    "href": "writing.html#section-2",
    "title": "Writing",
    "section": "2020",
    "text": "2020\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari (2020). “Sharp as a Fox: Are foxnews.com Visitors Less Politically Knoweldgeable?”. American Politics Research   link  pdf\n\n\n\nPeter Licari (2020). “Animal Crossing is Exactly What We Need Right Now”. Medium (Self Published)   link\n\n\n\nPeter Licari (2020). “Press B to March: The Effects of Video Games on Political Behavior”. Doctoral Dissertation   pdf"
  },
  {
    "objectID": "writing.html#section-3",
    "href": "writing.html#section-3",
    "title": "Writing",
    "section": "2019",
    "text": "2019\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari (2019). ““Serious” Fun: Social, Moral, and Political Content in Video Games”. The Strong Museum of Play Blog   link  pdf\n\n\n\nPeter Licari (2019). “Off to the Races: How proximity to a racetrack affected the vote to ban dog racing in Florida”. Towards Data Science   link  github\n\n\n\nPeter Licari, Michael Binder (2019). “The Hispanic Vote in Florida”. The 2016 Presidential Election in Florida: Ground Zero for America’s New Political Revolution   link"
  },
  {
    "objectID": "writing.html#section-4",
    "href": "writing.html#section-4",
    "title": "Writing",
    "section": "2017",
    "text": "2017\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari (2017). “Do 7 percent of Americans actually think that chocolate milk comes from brown cows?”. Politics Means Politics   link"
  },
  {
    "objectID": "writing.html#section-5",
    "href": "writing.html#section-5",
    "title": "Writing",
    "section": "2016",
    "text": "2016\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nPeter Licari, Jon Morris (2016). “The emotional election”. UF Journalism School Insights   link"
  },
  {
    "objectID": "writing.html#section-6",
    "href": "writing.html#section-6",
    "title": "Writing",
    "section": "2015",
    "text": "2015\n\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\nMichael McDonald, Peter Licari, Lia Merivaki (2015). “The big cost of using big data in elections”. The Washington Post   link"
  }
]