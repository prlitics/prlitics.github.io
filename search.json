[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Who am I?\nI’m a behavioral data scientist, writer, and content creator. I currently work as a data scientist for Universal Studios Orlando as part of their global strategies team. In addition to my day job, I started Technites, a research consultancy organization. (Though, considering it’s just me and my wife for now “organization” may be a bit grand—but I’ve got hopes and, more importantly, plans). Prior to that, I was a Director of Commercial Data Science at Morning Consult, where I led the data science work for our Advocacy and Government region. I earned my PhD in Political Science from the University of Florida in the fall of 2020, specializing in American Political Behavior and Research Methodology. I wrote my dissertation on the ways that video games can affect political attitudes and participation. It’s freely available online if the topic strikes you as interesting.\nApart from that, I am a husband to my incredible wife, Stephanie, a father to my boisterous daughter, Rosalina, fur father to a greyhound (Dude) and bombay (Asia), and a Quaker. (There. Now you can say that you know one who isn’t on the oatmeal box).\n\n\nWhat do I do?\nMuch of my work can categorized into four (often overlapping domains): Empirical, Methodological, Exploratory, and Ludic:\n\nMy empirical work focuses on political behavior. I am interested in the stuff that goes on in the space between our ears, how it is shaped by psychological, sociological, institutional, technological, and historical factors—and how it all comes to be expressed as attitudes and actions.\nMy methodological work is largely informed by the questions posed at my day job. (Some may call this relationship endogenous—and they’d be right). Accordingly, most of it centers around applied statistics and survey research: causal inference, survey experiments, multilevel regression modeling, scale construction and validation, time series forecasting, etc. Though most of my work is quantitative, my favorite stuff to consume and produce also incorporates qualitative and theoretical components. I’m deeply interested in the epistemological assumptions and implications undergirding our research practices as well as with the philosophical study of knowlege-generating practices more broadly.\nMy exploratory work tends to focus on sharing the science and history behind our endlessly fascinating social world. I love using my writing, YouTube channel, and data visualizations to tease apart—and find the common threads within—current events, academic research, pop-culture, and everyday observations.\nMy ludic work tends to boil down to puzzle-solving and tinkering. Luidic derives from the latin word for play (ludus), which pretty well describes what I do when I engage with it. Sometimes I stumble upon a question, idea, or challenge and I play around with models and solutions. I enjoy thinking about the interactions of actors, rules, incentives, and systems; I like to play within (and with) the constraints of everyday things to gain a deeper understanding of them—and sometimes create something new. I consider a lot of my programming, automation, and artistic outputs to fall under this category.\n\nWhen I’m not working, I can be found spending time with my wonderful family, reading, cooking, playing video games, training for my next road race, or overthinking popular media while binging it. (Often doing many of these things at the same time).\n\n\nWhy “Data and Other Fables”?\nGreat question! Fables are short, fantastical little yarns that speak to something true outside their context. The best data, given the right voice, are the same.\nThe title was inspired by two books and a quote: Economic Fables by Ariel Rubinstein, Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari and “All models are wrong, but some are useful” a la George Box. (Though if I had to trace the through-line further, I would say that it also owes some debt to the quip”artists use lies to tell the truth” from the film V for Vendetta. I was, uh, a bit into that movie growing up). The thread yoking them together is the idea that quantitative and theoretical models are fundamentally false in the same way that stories or other forms of narrative art are. At best, they are simplifications of reality. They have to be, lest they wind up being like Lewis Caroll’s map that’s a 1:1 replica of the terrain. That’s not to say that they contain zero truth—quite many powerful truths have been conveyed through fiction. Just that whatever true things are conveyed are only approximate articulations of the whole—and that these approximations come bundled up in emphases, omissions, explicit interpretations, tacit associations, and leaps of logic and faith. That is, comprised of all the stuff of human voice when it sets itself upon the task of storytelling.\nPlus, the title conveys the fact that I won’t just be talking data on this blog. If you couldn’t guess, I’ve got other interests outside of that. I like books, science, comics, heavy metal, philosophy, video games, poetry, art, and garbage anime. I figure that this can be just as much a home for reflections on that stuff as it is tutorials and sErIoUs CoMmEnTaRy."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Getting Worse to Get Better\n\n\nSome thoughts about improving in data science…and bowling, I guess.\n\n\n\n\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\nWhat I Learned From My First Non-Academic Conference\n\n\nReflections on ODSC East 2024\n\n\n\n\n\n\nApr 26, 2024\n\n\n\n\n\n\n\n\nHow Popular Are Birthdays Over Time?\n\n\nA datacentric way to celebrate turning 30\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\nSome reflections, hopes, and plans for 2024\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\nThe Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\nHow to add a Variable Font to a Quarto PDF on Windows\n\n\nOr: How I was driven completely insane by Google Fonts\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\nWhat to do when R punches you in the mouth\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n\n\n\n\n\n\nSimulating My Daughter’s First Board Game in Python\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\nBook Review: Theory and Reality\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\nThoughts on the 2020 Election\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\n\n\n\n\n\n\nHow to Do Unequal Randomization in Qualtrics Surveys\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\nSurveying is a Beautifully Foolish Endeavor\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\n\n\n\n\n\n\nCapturing a PowerPoint/Google Slides Lecture with Open Source Software\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data and Other Fables",
    "section": "",
    "text": "Getting Worse to Get Better\n\n\nSome thoughts about improving in data science…and bowling, I guess.\n\n\n\n\nreflection\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nWhat I Learned From My First Non-Academic Conference\n\n\nReflections on ODSC East 2024\n\n\n\n\nreflection\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nHow Popular Are Birthdays Over Time?\n\n\nA datacentric way to celebrate turning 30\n\n\n\n\nR\n\n\ndescription\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nSome reflections, hopes, and plans for 2024\n\n\n\n\n\n\n\nreflection\n\n\nlife\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nThe Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\nR\n\n\nstatistics\n\n\nsimulation\n\n\nsurveys\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nHow to add a Variable Font to a Quarto PDF on Windows\n\n\nOr: How I was driven completely insane by Google Fonts\n\n\n\n\nR\n\n\nQuarto\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nWhat to do when R punches you in the mouth\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nSimulating My Daughter’s First Board Game in Python\n\n\n\n\n\n\n\npython\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nBook Review: Theory and Reality\n\n\n\n\n\n\n\nreview\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on the 2020 Election\n\n\n\n\n\n\n\nelections\n\n\nreflection\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nHow to Do Unequal Randomization in Qualtrics Surveys\n\n\n\n\n\n\n\nsurveys\n\n\nqualtrics\n\n\nresearch design\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurveying is a Beautifully Foolish Endeavor\n\n\n\n\n\n\n\nreview\n\n\nsurveys\n\n\nfiction\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\nPeter Licari\n\n\n\n\n\n\n  \n\n\n\n\nCapturing a PowerPoint/Google Slides Lecture with Open Source Software\n\n\n\n\n\n\n\ntutorial\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\nPeter Licari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data and Other Fables",
    "section": "",
    "text": "Getting Worse to Get Better\n\n\nSome thoughts about improving in data science…and bowling, I guess.\n\n\n\n\n\n\nJun 20, 2024\n\n\nPeter Licari\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nWhat I Learned From My First Non-Academic Conference\n\n\nReflections on ODSC East 2024\n\n\n\n\n\n\nApr 26, 2024\n\n\nPeter Licari\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nHow Popular Are Birthdays Over Time?\n\n\nA datacentric way to celebrate turning 30\n\n\n\n\n\n\nApr 15, 2024\n\n\nPeter Licari\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nSome reflections, hopes, and plans for 2024\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nPeter Licari\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong\n\n\nIllustrated using Pokemon\n\n\n\n\n\n\nSep 13, 2023\n\n\nPeter Licari\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "other_things/dissertation/index.html",
    "href": "other_things/dissertation/index.html",
    "title": "Press B to March (Dissertation)",
    "section": "",
    "text": "Video games are one of the most popular entertainment mediums in America. The average player racks up an average of 7 hours a week and the industry hauls in billions of dollars a year. Despite decades of moral panic and hand-wringing that video games are corrupting the civic values of American youth, very little research has been done to actually see how games influence political behaviors: specifically, our attitudes and tendencies to participate in politics. This dissertation uses a mixture of quantitative and qualitative analyses to argue that certain game experiences—those that make players think about social, moral, and political issues as well as those that strengthen social ties—can affect political attitudes and inspire players to be more active in politics.\nThe dissertation uses a mix of survey data, archival work, content analysis, and randomized controlled experimentation to present 4 main arguments:\n\nJust like any other narrative medium, games tell stories to their consumers; stories that often concern or contain things that are relevant to our society at large. These stories are known to engender effects to our political behaviors when presented in the format of the news, movies, television shows, and novels. In this respect games are no different.\nUnlike those other forms of media, though, games are far more interactive. The player is acting upon their experiences in ways that they can’t with other media. They are active agents in these worlds. The socially, morally, and politically relevant content is often contingent or can otherwise be linked to the actions that players perform via their avatars. And the behaviors they practice and perform behind the screen can have ramifications beyond it.\nBut the effects of games are not limited to their content: video games have always been social experiences. They encouraging new relationships and strengthen those that already exist. We know that, generally, these behaviors lead to increased political action—and this dissertation argues that the networks established around games are no different. They also lead to increased activity through the creation and maintenance of social capital as well as increasing the opportunities people have to be exposed to political talk.\nThese effects are not haphazard. Game designers consciously put relevant content into their games and strive for their multiplayer experiences to create feelings of community. And they (treatments sociopolitical issues and multiplayer opportunities) can be readily found in the vast majority of the most popular games released from 2007–2017. And these effects are not rare or uncommon. Gaming is not just for the young, awkward, and/or antisocial. Nearly 2 out every 5 Americans play games that engage with sociopolitical issues—and more play games socially with friends.\n\nGames, in short, matter—in a lot of ways and for a lot of people. This dissertation provides one of Political Science’s first look into how and why.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/dumdum/index.html",
    "href": "other_things/dumdum/index.html",
    "title": "dumdum: an R pakage for dummy variables",
    "section": "",
    "text": "dumdum is an R package that makes making dummy variables easier. There are functions that do this sort of work in packages like caret, but I wanted to make a set of functions that would help other social scientists and/or analysts who were not as deep down the machine-learning rabbit hole.\nI made this fuction before I started doing more package development in R for my job. While I’m still proud of this little package—it was the best I could do at the time—I’m also proud of what I can do now. Such as waves arms at website.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/election-app/index.html",
    "href": "other_things/election-app/index.html",
    "title": "Florida Election App",
    "section": "",
    "text": "Inspired by all of the chaos and craziness that was mail-in and drop-off voting in the 2020 election, my students from my 2020 Election Data Science class and I created an app designed to help people find ballot-drop-off and early voting locations. The app received thousands of hits, helped at least a few people safely find ballot drop-off locations, and is now archived on my GitHub.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/infixit/index.html",
    "href": "other_things/infixit/index.html",
    "title": "infixit: Expanded infix operators in R",
    "section": "",
    "text": "{Infixit} is an R package that expands the standard R infix operators (things like, e.g., +, **, and, %in% that go in-between two arguments). This is mostly useful for programming but can also be very useful for data analysis (especially the %btwn% operator).\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/kanye-map/index.html",
    "href": "other_things/kanye-map/index.html",
    "title": "2020 Write-In Requirements by State",
    "section": "",
    "text": "Inspired by all of the talk about whether Kanye could “successfully pull off a third-party independent run” (God remember when that was in the news cycle?), I made this tableau data visualization showing the requirements to be a write-in candidate in 2020. TL;DR: Kanye wouldn’t have been able to do it even if it were more than a transparent publicity stunt.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things/wordle-letter-frequency/index.html",
    "href": "other_things/wordle-letter-frequency/index.html",
    "title": "Best Wordle Seed",
    "section": "",
    "text": "These are a pair of visualizations I made once Wordle started getting big (before it was bought out by the New York Times). There are a few different approaches to determining the best Wordle seed: The one I went with was to use the scrabble dictionary to identify 5-letter English words and identify the most frequently occurring letters as well as where they tend to occur.\n\n\nFor what it’s worth, I still use the seeds I identified from this analysis. My streak may have been broken by toddlers, illnesses, and busted phones but it hasn’t been done-in by the game itself: My accuracy rating is still at 100%.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "other_things.html",
    "href": "other_things.html",
    "title": "Other Things I’ve Made",
    "section": "",
    "text": "Here are some of the things that I’ve made that aren’t blog posts and are outside the scope of my ongoing creative projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfixit: Expanded infix operators in R\n\n\n\nR Package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest Wordle Seed\n\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPress B to March (Dissertation)\n\n\n\ndoctoral dissertation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020 Write-In Requirements by State\n\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlorida Election App\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndumdum: an R pakage for dummy variables\n\n\n\nR Package\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/birthdays_2024/index.html",
    "href": "posts/birthdays_2024/index.html",
    "title": "How Popular Are Birthdays Over Time?",
    "section": "",
    "text": "Here are two fun facts about 30:\nFolks who play with data often make the convenient assumption that birthdays are distributed uniformly along the dates of the Gregorian calendar. However, as with all convenient assumptions, we know that this isn’t actually true! Thanks to things like superstition, holidays, extreme weather events, and war, conception (and therefore births) don’t just happen at random within a population.\nSo, to celebrate my own birthday, I wanted to look at how birthdays are distributed within the US—and how this distribution might have changed over time."
  },
  {
    "objectID": "posts/birthdays_2024/index.html#the-data",
    "href": "posts/birthdays_2024/index.html#the-data",
    "title": "How Popular Are Birthdays Over Time?",
    "section": "The Data",
    "text": "The Data\nAs part of their 2016 article Some People Are Too Superstitious To Have A Baby On Friday The 13th, Carl Bialik (with analytical support provided by Andrew Flowers) published birthdate data sourced from the Centers for Disease Control and Prevention’s National Health Center for Health Statistics (spanning 1994-2003) and the Social Security Administration (spanning 2000-2014). Propitiously, 1994 is my year of birth! So I’m somewhere in that tally! (To deal with the overlap, I only look at data from 2004 on from the SSA data)2.\nI analyzed this data in R (R version 4.2.2 (2022-10-31 ucrt)) using the {Tidyverse} family of packages as well as {gganimate} to make a gif looking at how dates have shifted over time."
  },
  {
    "objectID": "posts/birthdays_2024/index.html#results",
    "href": "posts/birthdays_2024/index.html#results",
    "title": "How Popular Are Birthdays Over Time?",
    "section": "Results",
    "text": "Results\nHere is the code to read-in and merge the data—as well as tabulate the proportions. I first calculate the proportion within each year (so to see what proportion of births occurred on January 1 1994, 1995, 1996, etc) and then the proportions overall (to see how many happened on January 1 in the main dataset).3 Let’s first look at how things are overall.\n\n\nShow the code\n### Read-in CDC data\ncdc <- read_csv('data/US_births_1994-2003_CDC_NCHS.csv',\n                show_col_types = FALSE) %>%\n  select(births, year, date)\n\n### Read-in SSA data\nssa <- read_csv('data/US_births_2000-2014_SSA.csv',\n                show_col_types = FALSE) %>%\n  filter(year >= 2004) %>%\n  select(births, year, date)\n\n\nfull_bdays <- bind_rows(cdc,ssa) %>% # Merge data\n  group_by(year) %>%\n  mutate(sum_year = sum(births)) %>%\n  mutate(prop_year = births/sum_year) %>% # Daily sum by year\n  ungroup() %>%\n  mutate(md =  strftime((as.Date(date, format = \"%m/%d/%Y\")), format = \"%m%d\") ) %>%\n  group_by(md) %>%\n  mutate(md_sum = sum(births))%>%\n  ungroup() %>%\n  mutate(md_prop = md_sum/sum(births)) # Daily sum overall\n\n\n\nDistribution of US Birthdays 1994-2014\nBelow is an overall distribution of US birthdays spanning the full length of the data.\n\n\nShow the code\nfull_bdays %>%\nmutate(month = strftime((as.Date(date, format = \"%m/%d/%Y\")), format = \"%m\"),\n       day = strftime((as.Date(date, format = \"%m/%d/%Y\")), format = \"%d\")) %>%\n  distinct(md, md_sum, .keep_all = TRUE) %>%\nggplot() + \n  aes(y = day, x = month, fill = md_prop) +\n  geom_tile(color = '#fffcf9') +\n  scale_y_discrete(limits = rev) +\n  scale_x_discrete(labels = c(\"January\", \"Feburary\", \"March\",\n                              \"April\", \"May\", \"June\",\n                              \"July\", \"August\", \"September\",\n                              \"October\", \"November\", \"December\")) +\n  scale_fill_gradient2(high = \"#f18f01\",\n                       mid = \"#dfdfdf\",\n                       low = \"#A4BAF2\",\n                       midpoint = mean(full_bdays$md_prop),\n                       name = 'Daily proportion\\nof Births\\n') +\n  theme_minimal() +\n  xlab(\"\") +\n  ylab(\"\") +\n  labs(title = 'Distribution of US Birthdays (1994-2014)',\n       subtitle = 'Summer birthdays are most common in the US. Outside of Summer though, birthdays are more-or-less uniformly spread,\\n except for a handful of days where people DO NOT want to give birth.\\n',\n       caption = 'Sources: CDC NHC, SSA, compiled by FiveThirtyEight\\nVisualization by Peter Licari (@PRLPoliSci)') +\n  theme(text = element_text(color = \"#4E555B\"),\n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = '#fffcf9'),\n        axis.text.x = element_text(face = 'bold'),\n        axis.text.y = element_text(face = 'bold'),\n        plot.title = element_text(hjust = .5, , size = 18, face = 'bold',  color =\"#001021\"),\n        plot.subtitle = element_text(hjust = .5, size = 12, color = \"#4E555B\"),\n        plot.caption  = element_text(hjust = 1, size = 8, color = \"#4E555B\"))\n\n\n\n\n\n\n\nThere are a few really interesting things about this chart! First is the lack absolute variation. If birthdays were truly uniformly spread, we’d expect the probability to be around 0.0027—which perfectly matches the actual median value in the data ( 0.0027)! Second, to the extent that some days do go above this average, the most frequent days of birth tend to be in mid-late summer. Which makes sense! The average human gestation is about 40 weeks, putting the rough period of conception in the preceding fall—a period where many folks start to spend more time indoors after the summer4. Which means, well, more opportunities for conception.\nApart from that, what strikes me is that there appear to be more days where people try to avoid giving birth versus trying to give birth. The giant outlier at February 29th makes sense thanks to leap years—but it seems that people are trying their best to avoid giving birth on Christmas, New Years, and (to a lesser extent) the 4th of July and Halloween. The slight orange pattern in the days surrounding those excepions probably reflect the efforts of soon-to-be moms to avoid having kids born on those holidays. Though it does seem that some folks do try to schedule more births than average around Valentine’s Day? Interesting!\n\n\nDoes this distribution over time?\nThat all makes sense given the number of years we’re looking at as well as the US’ cultural context. But do these values change over time—and is there any structural pattern to any such change? The animated graph below shows how things might or might not change over time.\n\n\nShow the code\nfull_bdays %>%\n  mutate(month = strftime((as.Date(date, format = \"%m/%d/%Y\")), format = \"%m\"),\n       day = strftime((as.Date(date, format = \"%m/%d/%Y\")), format = \"%d\")) %>%\n  distinct(year, date, prop_year, .keep_all = TRUE) %>%\n  #filter(year == 2014) %>%\nggplot() + \n  aes(y = day, x = month, fill = prop_year) +\n  geom_tile(color = '#fffcf9') +\n  scale_y_discrete(limits = rev) +\n  scale_x_discrete(labels = c(\"Jan\", \"Feb\", \"Mar\",\n                              \"Apr\", \"May\", \"June\",\n                              \"July\", \"Aug\", \"Sept\",\n                              \"Oct\", \"Nov\", \"Dec\")) +\n  scale_fill_gradient2(high = \"#f18f01\",\n                       mid = \"#dfdfdf\",\n                       low = \"#A4BAF2\",\n                       midpoint = mean(full_bdays$prop_year),\n                       name = 'Daily proportion\\nof births in {closest_state}\\n') +\n  theme_minimal() +\n  xlab(\"\") +\n  ylab(\"\") +\n  labs(title = 'Distribution of US Birthdays ({closest_state})',\n       caption = '\\nSources: CDC NHC, SSA, compiled by FiveThirtyEight\\nVisualization by Peter Licari (@PRLPoliSci)') +\n  theme(text = element_text(color = \"#4E555B\"),\n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = '#fffcf9'),\n        axis.text.x = element_text(face = 'bold'),\n        axis.text.y = element_text(face = 'bold'),\n        plot.title = element_text(hjust = .5, , size = 18, face = 'bold',  color =\"#001021\"),\n        plot.subtitle = element_text(hjust = .5, size = 12, color = \"#4E555B\"),\n        plot.caption  = element_text(hjust = 1, size = 8, color = \"#4E555B\")) +\n  transition_states(year)\n\n\n\n\n\n\n\nI was blown away by this! I figured from the aggregate chart that there would also be a lot of gray here—but we’re actually seeing a lot of day-to-day variation within individual years! And look at that reverse Tetris patterning! It’s fascinating!! (Though, if you look closely, you’ll notice that Christmas, New Years, July 4th, Vallentine’s day, and Halloween tend to keep their characteristic patterns seen in the aggregate data).\nI was initially flabbergasted by the movement. But then I noticed that, most of the time, it’s consecutive chunks of 2 blues shifting next to 5 blocks of orange. Which, to me, suggested that people are generally trying to avoid having their kids on a weekend.\nLet’s investigate that! The chart below looks to see if this hunch is correct.\n\n\nShow the code\nfull_bdays %>%\n  mutate(wday = lubridate::wday(as.Date(date, format = \"%m/%d/%Y\"), label = T)) %>%\n  distinct(year, date, prop_year, .keep_all = TRUE) %>%\nggplot() +\n    aes(x = prop_year, fill = as.factor(wday)) +\n    geom_density() +\n  xlab('Distribution of Proportions') +\n  ylab('Density') +\n  labs(title = 'Distribution of Days of Birth (1994-2014)',\n       subtitle = 'Births are less likey on Saturday and Sunday compared to the remaining days of the week.') +\n  theme_minimal()+\n  theme(text = element_text(color = \"#4E555B\"),\n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = '#fffcf9'),\n        axis.text.x = element_text(face = 'bold'),\n        axis.text.y = element_text(face = 'bold'),\n        plot.title = element_text(hjust = .5, , size = 18, face = 'bold',  color =\"#001021\"),\n        plot.subtitle = element_text(hjust = .5, size = 12, color = \"#4E555B\"),\n        plot.caption  = element_text(hjust = 1, size = 8, color = \"#4E555B\")) +\n  ggthemes::scale_fill_colorblind(name = \"\")\n\n\n\n\n\n\n\nLooks like this hunch is spot-on! Sundays and Saturdays appear to be less frequent than any of the other 5 days of the week. After consulting with some experts (my mother and wife—the mother of my child), our hypothesis is that many doctors are off on weekends. This means that much of the medical team that soon-to-be-parents have spent most of their pregnancy with may not be available on the weekends. Women may thus be more likely to induce on a weekday or try to “hold on” (for want of a better term) until their care team is available. Basically, the thought is that a baby born on the weekend was coming on that day—no negotiations."
  },
  {
    "objectID": "posts/birthdays_2024/index.html#concluding-thoughts",
    "href": "posts/birthdays_2024/index.html#concluding-thoughts",
    "title": "How Popular Are Birthdays Over Time?",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nSo let’s wrap up what we’ve learned here!\n\nWhen looking at multiple years worth of data all aggregated up, it seems like the assumption that birthdays are uniform is not too terribly far off from the truth. More than anything, it seems that moms-to-be to their best to avoid very specific days to give birth on. (Though, apparently, Valentine’s Day may run counter to this!)\nWhen looking at individual years, a lot more variability between individual days emerges. When looking at year-over-year evolutions in an animated format, it creates a super tripy reverse-Tetris pattern. But said pattern revealed that the days comprising weekends have fewer births than days comprising weekdays.\n\nThis was a very fun way to ring-in my 4th decade!5 In the process, I’ve now found myself with a few more birthday-related questions that I want to dig in to! So it goes with research questions. Maybe I’ll try to investigate them in birthdays to come! Could be a fun little birthday tradition. Let’s see how it, and the next decade, go!"
  },
  {
    "objectID": "posts/covid-video-lecture-tutorial/index.html",
    "href": "posts/covid-video-lecture-tutorial/index.html",
    "title": "Capturing a PowerPoint/Google Slides Lecture with Open Source Software",
    "section": "",
    "text": "A number of schools are either shutting down or transitioning classes to an online format due to the novel corona virus (also known as COVID-19). (UF just let us know Yesterday that, while it isn’t yet mandatory, we should start transitioning sooner rather than later). This isn’t going to be an easy transition for a lot of us—students and faculty alike. Not everyone makes living on the Internet basically a second job. I’m planning on writing a workflow for my intended solution—hosting lectures live in a Discord server with password-protected Google Forms as attendance—in the next few days. But I know that many people are going to feel that simpler is better for them. I got this text from a good friend of mine yesterday as we were talking about it:\nAbsolutely! And it’s easy to do with some Open Source software. In fact, you can do it with whatever your presentation software of choice (PowerPoint, Google Slides, LibreOffice, Beamer, etc.). This is especially helpful because, while PowerPoint offers an easy recording option, the same can’t be said for everything. So, for this post, I’m going to walk y’all through how to use OBS to record yourself giving a PowerPoint presentation so you can upload it to your students."
  },
  {
    "objectID": "posts/covid-video-lecture-tutorial/index.html#tips-for-your-presentation",
    "href": "posts/covid-video-lecture-tutorial/index.html#tips-for-your-presentation",
    "title": "Capturing a PowerPoint/Google Slides Lecture with Open Source Software",
    "section": " Tips for Your Presentation",
    "text": "Tips for Your Presentation\nAs I ever-so-slyly referenced in the last paragraph, I’ve been making multimedia content for the web for a while now—pretty close to about 6-7 years. I’m not by any means an unimpeachable expert, but I’ve picked up a few things as I’ve gone along. So I wanted to pass along a few tips for when you’re recording your presentation:\n\nKeep it brief: Unless you’ve got a helluva radio voice (and an engaging personality to boot), keep your presentation shorter rather than longer. The best video educators on YouTube don’t tend to go much longer than 10-15 minutes—and they’ve usually got some kind of visual animation and music to accompany them. You won’t keep your students’ eyes on the screen for much longer than that. Even if they do, their eyes will be totally glassed over or they’ll have Facebook open in another page. (That may not sound much different than class normally for some, but trust me. It is.) I know that’s frustrating because you probably have a lot more material than that. Trim it down as best you can.\nIf you can’t keep it brief, do it in chunks: If you absolutely cannot cut your material down to below 10-15 minutes, make multiple, shorter videos. Find natural stopping points in your presentation and cut your videos down to about 5-7 minutes. Set up a silent alarm for about 6 minutes that’ll help you wrap it up. This technique won’t let you get away with uploading, like, 10 videos—but you can use it to upload 3-6 that’ll get far better engagement than if you just kept talking for 20-30 uninterrupted minutes.\nDon’t eat the mic: watch how close you’re getting to your audio input. You don’t want your lecture to get interrupted by crackles or Darth Vader-esque heavy breathing.\nTalk slower. Then talk slower than that: Some people (like me) are cursed with a rapid cadence. When we talk to people face-to-face, it’s easier to understand us because a lot of our meaning is translated through non-verbal means. But when it’s just you and the slides, fast talking will be the death of comprehension. Plus, if you’re not used to presenting using a microphone, you’re probably nervous about it. Even if you don’t think you’re nervous, you probably are. That’ll bleed over into how quickly you’re going through the material. So if you just finished a video and/or chunk and are thinking “wow, I sure managed to clear through those slides really quickly!”—you’re probably going to have to re-record it if you want students to know what the hell you were actually saying.\nAvoid being monotone: Your students probably understand that this isn’t what you signed-up for; no one’s asking for an Oscar-worthy performance. And unless you’ve got experience with acting, improv, or public speaking, it’ll probably come off as more cringy if you shoot for one. But don’t be like the teacher from Ferris Bueller’s Day Off, droning on about the Smoot-Hawley Tariff Act. Try to add some variation in your voice every now and then. Change up your volume, pitch, and tone just as you would if you were speaking before a group.\nDon’t strive for perfection: Don’t try to make this a Ted Talk. Don’t try to make this a YouTube video. Don’t make this into a job talk or conference presentation. Don’t think that you have to have no pauses, no flubs, no imperfections. If you would say “oops, excuse me, what I meant was…” in class, you can say it here. It’s fine. 9 times out of 10, you’ll notice it far more than anyone else will.\nPractice before you upload: Do a couple of short practice runs (1-2 minutes) to make sure you’ve got your pacing, microphone placing, and all-around jitters sorted out before you dive in to the whole thing. As I mentioned, I’ve been doing this for years and I still have to do this. We all do. It’s better to get all the little kinks fixed before you upload your videos and you realize, to your sinking horror, there’s something that makes it utterly unwatchable.\n\nThis is probably going to be a stressful time for both you and your students. This transition (from offline to online) has been abrupt for and it’s having very real consequences on the day-to-day lives of everyone involved. Hopefully, this tutorial will be able to ease that transition, and diminish some of those smaller issues."
  },
  {
    "objectID": "posts/getting_worse_getting_better_2024/index.html",
    "href": "posts/getting_worse_getting_better_2024/index.html",
    "title": "Getting Worse to Get Better",
    "section": "",
    "text": "It may come as a surprise to some people who know me, but I actually have other hobbies other than “doing stuff with data.” This post is concerns some experiences I’ve had with them. And, also, it’s about doing stuff with data. But before I talk about data, I want to briefly talk about bowling, golf, and running. (And then data, I promise).\n\nLooking for a family-friendly activity sheltered from Florida’s triple-digit heat index, my wife and I recently introduced our daughter to bowling. For context, the last time I had gone bowling was our second date 13 years ago. I’m atrocious at bowling. My daughter, the 4 year old, beat me in the first round.1 A kindly gentleman named Pasquale gave me advice on my form. It made me much more likely to get the ball straight down the alley, but it was hard to keep all of these new biomechanics in mind. And while I got more strikes and spares than before, the awkwardness of the new form also led me to have more gutterballs. So my score more or less was the same as before.1 She had bumpers and a ramp, but still!\nI’ve been golfing a lot longer, nearly 25 years—though some years I’ll maybe only have the chance to play one or two rounds. I’m only just now consistently shooting below 100. After a round in May where I managed to shoot some surprisingly long shots (including one par 4 where my tee shot put me within 10 yards of the green), I tried gunning for the hole from more challenging positions rather than laying it up and playing it safe. Most of these attempts didn’t pay off. I shot my worst front 9 in nearly 5 years.\nRunning is something I’ve been doing almost as long as golf (20 years) but with much more consistency. Apart from writing, it’s been my most successful hobby. I’ve been trying to get myself back into 16-17 minute 5k shape. I recently breezed through a 10:55 2 mile time trial; almost exactly on pace for my goals. So I tried an aggressive hill workout to improve my stamina. My first interval of a planned 6 was right on pace and felt easy. But by the time I was done with the very next interval, I was doubled-over, gasping for air. It didn’t get any better. I not only missed my splits, I didn’t even finish the workout as planned. I tapped out after interval number 4. I wanted to kick myself in the ass, but I couldn’t even lift my legs enough to do it.\n\nWe’ve all heard that progress isn’t linear. That you’ll frequently hit plateaus before reaching new heights. But these weren’t plateaus, they were downturns. And these weren’t merely coincident with a recent improvement or a reversion to my mean; just a random walk around my baseline level of skill at these things. They were downturns caused by the fact I had recently improved. The better I got, the more ambitious I got. And ambition often begets errors.\nThat’s the frustrating, almost paradoxical thing: When you try to structurally elevate your performance, when you aim not just to score better on some metric but to be a categorically better performer, the immediate result is often a worse performance than if had you stuck with what you were doing before.\nLet’s go ahead and tie this into data science now.\n\nFor better or worse, data science is a big-tent discipline. At its base is a chimerical mixture of statistics, engineering, and coding. This is then alloyed by whatever domains underlie the operation of the company writing your check (e.g., finance, law, marketing, psychology, chemistry, pharmacology, economics, user experience…) and whatever you need to effectively execute and communicate your work to others (design, IT, knowledge management, pedagogy, project management, people management…). It’s why there’s such high variance between different folks’ day-to-days; why some data scientists are doing cutting-edge LLM research, others are automating reports, and others still are working in Excel. All that is fine 2—but its far a from a uniform discipline.2 Except that some folks are so heavily exposed to Excel, all my homies hate Excel.\nThere’s a joke from Josh Wills about the field that puts it more pithily: A data scientist is “[a person] who is better at statistics than any software engineer and better at software engineering than any statistician.”3 Even if you’re not in a start-up or operating as your office’s de facto “data person”, the discipline forces you to wear many hats. It’s impossible to master literally everything within the field’s purview. And because of its breadth, you have countless ways to improve yourself—and because of how yoked it is to tech’s “bleeding edge”, self-improvement is a professional prerogative. This improvement can come vertically (digging deeper into a known domain), horizontally (learning about an adjacent domain), or diagonally (likely picking up even more hats to wear).3 For the happy majority who’ve never had to read an academic’s code or see how software engineers approach statistics, I should make clear that these aren’t exactly high bars.\nNo matter which route you take, though, I believe that taking data science seriously is to commit to continuous learning. To commit to continuous learning is to commit to being a continuous beginner. To commit to being a continuous beginner, for better or worse, is to commit to continuous mistakes.\n\nAs with all commitments, the commitment to continuous mistakes comes with some responsibility. Critically, it is not a carte blank check to fuck up. Indeed, a responsible commitment to being a perpetual beginner is to commit to enacting practices that constrain the frequency, severity, and extent of these mistakes. (After all, I may want to work on my fairway woods in golf but I should really balance that against the likelihood of accidentally slicing a ball through someone’s window).\nIt’s important to cater these practices to the potential threats you can anticipate:\n\nIf you’re migrating processes into the cloud, you might want to set limits on how much money you can spend so you don’t silently wrack-up a huge bill.\nIf you’re juggling identical values across different contexts in the same project, you can reduce the risk of copy-paste errors by placing these values in a single configuration file and then loading them in to your session.\nIf you’re using a new statistical model or ML method, you can see if it will do what you expect it to with simulation—either with familiar data or with data you’ve synthesized yourself.\nIf you’ve got a new data source, you can run tests after your transformations to ensure you’re getting the variables that you expect and that their values are sensible.\nIf you’re working in a novel substantive domain, showing your results to subject matter experts will help ensure that you’re not proposing something that is highly unlikely or downright impossible.\n\nI don’t want to make it sound like all these things are obvious. Some of them I’ve learned years ago, others I’ve learned within the last few weeks! And even those I’ve learned long ago, I’m constantly improving on how I go about practicing it. That’s part of why I said “the threats you can anticipate.” Embarking into new ways of doing things will expose you to novel “unknown unknowns” and you won’t be able to be proactive about threats you can’t yet anticipate. Even if you study-up and research this new domain (and you absolutely should do so as best as you reasonably can given the constraints you’re facing!), there are often subtle mistakes that will escape your best efforts until you’ve gained more experience. But I have noticed that, as you gain experience and expertise across a diverse range of practices, you’ll get better at anticipating potential mistakes. Not even just specific things to be looking out for, but broader types of concerns that you should at least consider preempting.44 For example, the cloud computing advice above isn’t something I learned from spinning-up stuff on AWS or Google or whatever—I learned it from generalizing an experience I had from when I started fielding surveys en mass.\nFor all these reasons, and more, I’ve found that the most effective error-prevention strategies I’ve picked up—across all that I strive to do—is being earnestly open to constructive feedback and a dedicated practice of introspection. The feedback is imperative to help me learn what comes next and to catch any errors that slipped my notice. And the introspection is crucial when I reflect, research the error’s source more critically, and design systems to prevent it from happening again.55 Sometimes, this designing is iterative. You might make a similar mistake more than once if you’ve misapprehended the deeper source of the original error. Or, maybe, you understood it perfectly but are constrained in your ability to perfectly solve it. That’s fine! It’s going to happen! It becomes a problem when literally the exact same error is allowed to happen multiple times.\n\nI’m someone who takes a lot of pride in what I do.6 I don’t think of research as a job but as craft—and I like to think, after all the years I’ve put in, I’ve gotten pretty good at it. It’s that pride that inspires me to improve. But that pride and craftsman self-image are also double-edged swords. Because the errors that come with leveling-up my skills directly confront that pride and self-image. After all, we don’t think of dedicated craftsmen as making mistakes; their proficiency is a big part of what sets them apart from “mere” practitioners. As errors mount from trying something new, it’s easy to feel like you’re just an imposter.6 As you could guess from my intro, that even extends to peripheral hobbies like bowling or pursuits central to my identity like running\nIt’s hard—but necessary—to remember that our observations of experts at work suffer from selection bias. We tend to see them performing at the top of their game. We tend to only see their end products and not what went into the production of them, or what hacky stuff they need to have happening behind the metaphorical curtain to make it work properly. And since their expertise is often distinct from our own, we may not even realize that mistakes are happening. As Annie Dillard writes in Pilgrim at Tinker Creek “I just don’t know what the lover knows; I just can’t see the artificial obvious that those in the know construct”.77 Like when I, a music novice, attend concerts with my wife, who has played instruments nearly her whole life: She notices technical errors that go completely over my head.\nI’m writing this out explicitly because, again, I struggle with it myself: the mere existence of errors is not a negative commentary on your rigor. Provided that you’re doing your due diligence—you’re checking things over carefully and you’re not just haphazardly rushing something to “completion”—the errors don’t mean that you’ve regressed, that you’re failing, that you’re not being careful. If anything, catching errors from a system of checks can be evidence in support of your rigor.8 It’s certainly better than not catching mistakes.8 Plus, you shouldn’t beat yourself up over the existence of more mundane errors anyways. You’re not a machine, nor should you aspire to be. To err is to be human. Again, as long as you’re being as diligent as you possibly can be, and only you can truly make that determination, then the best you can do is just…the best you can do!\nThe physicist Neils Bohr once remarked “an expert is merely someone who has made every possible mistake in a narrow domain.” More recently, the character Jake the Dog from Adventure Time quipped: “Sucking at something is the first step at getting sort of good at something.”99 I say more recently but do you know when that episode first aired? 2010! 14 fucking years ago! Jesus, I need to go lay down.\nIf you want to be proficient in several domains, which is almost required for broad fields like research and data science, you need to be prepared to make a lot of mistakes. And mistakes exhibit an interesting, weak ordinality: there are some mistakes that you can’t make until you’ve made others first. You can’t make expert mistakes until you stop making beginner mistakes. And many pursuits exhibit opportunity costs to endeavoring on the expert route. A high-risk, high-reward workout that explodes in your face comes at the expense of a solid one that maintains your current state. An ambitious tee shot that lands in the water costs you strokes compared to your usual “lay it up and play it safe” approach. At least until you’ve done the workout so often that it’s no longer “high risk”; and at least until you’ve played the hole enough that the tee shot is no longer ambitious.\nOnce you’ve hit a certain point, the only way to get better is, for a time at least, to get at least a little worse.\nBut only a little. And only for a time.\n\nI’d like to be able to conclude this by giving it some typical, self-help fairytale ending: “now I bowl over 200”; “now I shoot under par”; “now I win every road race I enter”; “now every analysis I make is worth its weight in gold”.10 But it’d be bullshit. I am noticing improvements though: even in the things I would call myself an expert in (which definitely isn’t golf or bowling).10 Considering the weight of a bit and the weight of a word, though, this one may actually be true.\nSo I can say I’m making progress. And since we are all always works in progress, that seems good for now. I’m getting worse. I’m getting better.\nI hope you are too.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{licari2024,\n  author = {Peter Licari},\n  title = {Getting {Worse} to {Get} {Better}},\n  date = {2024-06-20},\n  url = {https://www.peterlicari.com/posts/getting_worse_getting_better_2024},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeter Licari. 2024. “Getting Worse to Get Better.” June 20,\n2024. https://www.peterlicari.com/posts/getting_worse_getting_better_2024."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#scenario",
    "href": "posts/moe-subgroup-wrong/index.html#scenario",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Scenario",
    "text": "Scenario\nLet’s say that we are surveying some population of people with two sub-groups and we’re asking them about their favorite classic Pokemon starter plus Pikachu—because when you’re big enough to get your own balloon in the Thanksgiving Day parade, you get special privileges. So that brings us to four total: Charmander, Bulbasaur, Squirtle, and Pikachu. For the sake of this example, let’s say subgroup one are the folks who have played Pokemon and reflect 15% of the population. Among players, let’s imagine 40% choose Bulbasaur, 30% choose Charmander, 20% choose Squirtle, and 10% choose Pikachu4. Subgroup 2 reflects non-players and constitutes 85% of the population. 40% of these people choose Pikachu, 20% choose Charmander, 25% choose Squirtle, and 15% choose Bulbasaur. Among the full population then, 18.75% choose Bulbasaur, 24.25% choose Squirtle, 35.5% choose Pikachu, and 21.5% choose Charmander."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#creating-the-data",
    "href": "posts/moe-subgroup-wrong/index.html#creating-the-data",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Creating the Data",
    "text": "Creating the Data\nLet’s go ahead and simulate this population. I’ll be doing this in R (R version 4.2.2 (2022-10-31 ucrt)). I’m making it a total population of 10,000,000. The first part of the code chunk creates a dataframe that we’ll be using to sample from. The with_seed function comes from the {withr} package; I want to keep this reproducible on my machine since I’ll be using random sampling. The second part uses the {summarytools} package to make cross-tabs to illustrate that the data set construction worked as intended.\n\n\nShow the code\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(summarytools))\nsuppressPackageStartupMessages(library(withr))\nsuppressPackageStartupMessages(library(questionr))\n\nwith_seed(13, {\n\nplayers <-\n  tibble(\n    type = \"Player\",\n    preference = sample(\n    c(\"Bulbasaur\", \"Charmander\", \"Squirtle\", \"Pikachu\"),\n    prob = c(.40, .30, .20, .10),\n    size = 1500000,\n    replace = TRUE\n  ))\n\n\nnon_players <-\n  tibble(\n    type = \"Non Player\",\n    preference = sample(\n    c(\"Bulbasaur\", \"Charmander\", \"Squirtle\", \"Pikachu\"),\n    prob = c(.15, .20, .25, .40),\n    size = 8500000,\n    replace = TRUE\n  ))\n\nfull <- rbind(players, non_players)\n\nfull <- full %>%\n  dplyr::mutate(pop_id = row_number())\n\n})\n\n\nctable(full$preference, full$type, prop = 'c')\n\n\nCross-Tabulation, Column Proportions  \npreference * type  \nData Frame: full  \n\n------------ ------ ------------------ ------------------ -------------------\n               type         Non Player             Player               Total\n  preference                                                                 \n   Bulbasaur          1276579 ( 15.0%)    599477 ( 40.0%)    1876056 ( 18.8%)\n  Charmander          1700113 ( 20.0%)    450589 ( 30.0%)    2150702 ( 21.5%)\n     Pikachu          3399748 ( 40.0%)    149398 ( 10.0%)    3549146 ( 35.5%)\n    Squirtle          2123560 ( 25.0%)    300536 ( 20.0%)    2424096 ( 24.2%)\n       Total          8500000 (100.0%)   1500000 (100.0%)   10000000 (100.0%)\n------------ ------ ------------------ ------------------ -------------------\n\n\nLooks like this worked great! The subgroup proportions and the overall group proportions are where we expect them to be."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#sampling",
    "href": "posts/moe-subgroup-wrong/index.html#sampling",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Sampling",
    "text": "Sampling\nNow let’s say that we’re going to sample 1,000 respondents from this population. But! We’d like to also do analyses of Players as a subgroup. We’d only expect to get about 150 people, which isn’t nothing but it’s not a whole lot either. So we’ll oversample an additional 500 game players for a total of 1,500 people. We’ll then weight respondents in our sample so that the overall values are unaffected.\n\n\nShow the code\n## Main 1,000 sample\nmain_sample <- full %>%\n  sample_n(1000)\n\nmain_sample_ids <- main_sample$pop_id # Ids of those sampled\n\n## Oversample of 500\noversample <- full %>%\n  filter(type %in% \"Player\") %>%\n  filter(!pop_id %in% main_sample_ids) %>% #no double sampling!\n  sample_n(500)\n\n## Full sample\nfull_sample <- rbind(main_sample, oversample)\n\n## Calculating weights\nweights_df <- full_sample %>%\n  count(type) %>%\n  mutate(weight_prop = ifelse(type %in% \"Player\",.15,.85)) %>%\n  mutate(weight = (weight_prop * 1500)/n) %>%\n  select(type, weight)\n\nfull_sample <- left_join(full_sample, weights_df, by = \"type\")\n\n\nLooking at the table below, which compares the weighted versus non-weighted and the actual population proportions, we can see that the weighting does its job; it brought the actual topline values to be far closer to the actual population values than the unweighted version did. Incidentally this shows why you should always downweight your oversample—otherwise, your overall values will be off by a lot more!\n\n\n\npreferencePop ValuesUnweightedWeightedBulbasaur0.18760560.24400000.1755069Charmander0.21507020.26666670.2249455Pikachu0.35491460.25466670.3345803Squirtle0.24240960.23466670.2649673\n\n\nBut the question isn’t whether downweighting an oversample gets us back in the right ballpark. It’s whether or not the purported MoE from the full 1,500 people is correct. If it is, we’d expect that, should this sample be conducted an arbitrarily large umber of times, 95% of those confidence intervals will contain the true population value."
  },
  {
    "objectID": "posts/moe-subgroup-wrong/index.html#simulating-1000-pokemon-preference-surveys",
    "href": "posts/moe-subgroup-wrong/index.html#simulating-1000-pokemon-preference-surveys",
    "title": "The Traditional Margin of Error When You Have Oversamples in Your Survey is Wrong",
    "section": "Simulating 1,000 Pokemon Preference Surveys",
    "text": "Simulating 1,000 Pokemon Preference Surveys\nComputers may be amazing, but they can’t do something infinitely many times. But, just like with random sampling, doing something a bunch of times instead will get us in the right ball park.\nSo let’s conduct 1,000 samples of 1,500 from our full population data—1,000 people randomly selected from the full data (players and non-players alike) and then 500 randomly selected just amongst the 15% of people that are Pokemon players. We’ll then weight these respondents correctly such that the players won’t have an outsized influence on the topline results. I’ll report out on 3 different MoEs.\n\nThe first assuming the oversample is included and not downweighted, calculated with the standard MoE.\nThe second weighting the oversample properly, calculated with the standard MoE.\nThe third weighting the oversample properly and calculating it with an MoE that takes the oversampling into account.\n\nGiven that these are 95% confidence intervals, we should expect approximately 95% of intervals to contain the true population value.\nThe chart below visualizes the results of the simulation. Each facet reflects a type of confidence interval (rows) and a Pokemon (column). The vertical bars reflect the MoEs for each of the 1,000 simulated samples. Their order up and down the Y axis is arbitrary, they happen to go bottom up from the first simulated run to the last. Moving horizontal on the X axis are smaller–larger proportions of the synthetic population. The black vertical lines reflect how popular those Pokemon truly are in our synthetic population. Horizontal lines that cross this gray reference line have successfully captured the population parameter. Those that miss have not accurately captured that Pokemon’s overall popularity in the sample. Since all of these are purportedly 95% confidence intervals, we would hope that they do so 95% of the time.\nIn short, the more solid lines we have, the worse we’re doing. And if we have more than 5% of sample that shows a solid line, then our MoE isn’t being constructed correctly.\n\n\n\n\n\n\n\n\nLooking at the first section with the Standard MoE things appear to be okay. Indeed, if we were to just eyeball it one might think that we’re actually getting 95% coverage. But we’re not. It’s actually only 91.4%. The degree to which this “standard” MoE is wrong will depend on how much we have to weight the sample versus an ideal case of perfect random sampling. This is known as the weighting efficiency. It’s possible for this to be closer to 95%. But it’s also possible for it to be further away depending on how much your weights have to work to bring values in-line with your population targets.\nIn the second section, we’ve actually hit what we were aiming for. Here, we took the design of the sampling into account and were able to achieve 95% coverage. Right on the money5. In practice, design-effect adjusted MoEs are slightly elongated to account for the aforementioned weighting efficiency. I’ll leave the technical details about that for another post. But, for now, know that it is not only possible but necessary to adjust a survey’s MoE when it has an oversample in order to accurately convey one’s uncertainty about the point estimate.\nLooking at the third row with the unweighted values, the coverage is absolutely abysmal. Only 25.2% of the confidence intervals contain the true population mean. And the only reason that it’s even this accurate is because Squirtle is about as popular with both players and non-players. Again, let it be said: ALLWAYS WEIGHT-IN YOUR OVERSAMPLES IF YOU’RE DOING TOPLINE VALUES."
  },
  {
    "objectID": "posts/odsc-2024/index.html",
    "href": "posts/odsc-2024/index.html",
    "title": "What I Learned From My First Non-Academic Conference",
    "section": "",
    "text": "Me, hanging out with a Boston Dynamics robot. It’s a good robot doggo.\nThis week, I attended the 2024 Open Data Science Conference (ODSC) in Boston, Massachusetts. It was the first conference I attended since Covid1 and my first non-academic conference2. As you can see from the picture above, I even got to meet a celebrity while I was there! All in all, I had a great time and learned a ton.\nI wanted to write-up my reflections about the conference—about the whole thing generally and about what I think it reflects about Data Science as a field. I’m going to write-up what I learned in the specific panels I attended in a separate post.3"
  },
  {
    "objectID": "posts/odsc-2024/index.html#the-structural-roots-for-why-corporate-conferences-differ-from-academic-conferences",
    "href": "posts/odsc-2024/index.html#the-structural-roots-for-why-corporate-conferences-differ-from-academic-conferences",
    "title": "What I Learned From My First Non-Academic Conference",
    "section": "The structural roots for why corporate conferences differ from academic conferences",
    "text": "The structural roots for why corporate conferences differ from academic conferences\nI don’t have data for this, but I strongly suspect that typical attendees of an academic conference are primarily there to share their own work. Learning from others, serving as a moderator or discussant, networking, etc, are valued activities—but are ultimately secondary. Why? Because attendees frequently have limited financial support from their institutions—if any at all! And a good chunk of whatever support is available is either explicitly or de facto contingent on having something to present. Owing to the incentive structure of contemporary academia—where people’s job prospects are strongly yoked to the number of papers they publish as well as the apparent novelty of their content—most people aren’t going to attend if the conference doesn’t stand a good chance of adding something valuable to their CV.\nThis culminates in conferences predominantly oriented towards advanced discussion of bleeding-edge research—conveyed through talks that are incredibly information-dense. You plow through slides overflowing with text and packed with regression coefficients. And because the production of academic research is structurally adversarial5, these talks are further stuffed with various defensive maneuvers meant to persuade a presumed skeptical audience of the content’s validity. An hour-and-fifteen-minutes’ worth of time will frequently have 3-5 such talks—and then a discussant’s commentary followed by further question-and-answer from the audience. That’s not just exhausting for the attendees, it’s exhausting for the presenters! Many folks frequently have multiple papers/projects to present and/or have service responsibilities such as being a discussant or moderator. When you then account for the fact that folks will also want to support their friends/colleagues by attending their presentations, you can understand why most have limited emotional/cognitive bandwidth to learn or to “network.”\nBut, in the business world, it’s not necessarily expected that you’ll be the one footing all (or even most) of the bill! Most people are there on their company’s dime. And while most businesses are certainly glad to have employees capable of conceptualizing novel, bleeding-edge research ideas, that’s not the prevailing expectation. Their employees are expected to apply practices/concepts to their work—or to guide the business in procuring access to platforms or individuals capable of solving/expediting business critical tasks. Far, far more people have dispositions conducive towards the latter than the former. And because business environments and affiliations can change very quickly (especially in tech), you don’t know what your company—or you as an individual!—will need with 100% certainty in 6 months, 1 year, 5 years. It pays to have a large, diverse network to accommodate these shifting realities.\nFurther, for the minority of folks who are presenting, the company is usually instrumental in the production of whatever thing that they (presenter and company both) feel is worth disseminating. In academic settings, the institution’s involvement in the direction of the research is limited. Much of the time, they’ll only engage with it when it undergoes review by the IRB6 and/or, if it looks like it’ll make a big enough splash, when they publicize it after it’s been completed. This is by design: it’s so researchers are able to advance knowledge broadly and not just knowledge deigned “acceptable” by powerful figures administering or funding the institution. Academic research, then, is a much more individually-driven affair. But, outside of those spaces, the work being conducted is frequently performed using the company’s proprietary tools/platforms/methods/data/etc.\nWhat does all this mean? Quite a few things!\n\nPeople attending corporate conferences have different reasons for being there than those at academic conferences. Most corporate conference attendees don’t have the same predominant, unifying prerogative to present as academic conference attendees do. Corporate attendees are instead given a more nebulous prerogative: Acquire information to improve how we, the company, do things. That information can take the form of new knowledge, new techniques, new vendors, new sales prospects, or new interpersonal connections. In short, academic conference attendees are oft focused on production. Corporate conference attendees are instead focused on consumption.\nPeople presenting talks at corporate conferences have different incentives than those at academic conferences. Because the company is instrumental to the research presentation (both because they facilitate the research itself and foot the bill for conference attendance), presenters tend to discuss the work with a lot of excitement. And if their company is a start-up or is vying for market share, you can understand why they’d dedicate a fair chunk of presentation time towards advertising their products and services. (Especially since getting a booth on the showroom floor costs a bunch of money—both directly to the conference and in the aforementioned sweet, sweet swag). Additionally, their audience is not comprised solely of the other 7 people in the country (and their 3 besties) who a) comprehend and b) give a shit about the content of the talk. So they have to present with the expectation that many of the people watching are relatively (or entirely) new to the topic they’re presenting on.7 They are also (usually but not always) given more time, so the talks don’t feel like they have the information density of a black hole. There’s also a larger demand for case studies and workshops, things where presenters go into the gory details of implementation rather than the summary of a gajillion robustness tests.\n\nAs an attendee, my favorite talks were those approaching their slots as either workshops or lectures on substantive topics, with perhaps a bit of “look how cool we are, specifically” sprinkled throughout. Fortunately, I’d say that this was how the majority of talks I attended went. The best presenters, in my opinion, were those who limited their sales pitch to the first or last few minutes of the talk or who brought it up in very small doses throughout the whole presentation. (Bonus points for those who acknowledged competing products and services in the space!) That’s because I don’t mind becoming aware of how neat or nifty your widget is. In fact, I welcome it! As I said before, attendees are there to learn about the broader data science environment—which absolutely includes vendors! But I do mind feeling like I’m sitting through an infomercial or feeling like I was served the IRL equivalent of a click-bait video. Fortunately, though, that was only a handful of presentations that I came across.\nAs an upshot to all of this, it meant that I got to attend so many more talks and workshops than I would’ve attended at an academic conference. If I attended more than 3-4 sessions in a day, my brain would be dribbling out of my ears.8 I attended about double that amount at ODSC. Which meant that, even if the information density of each individual talk was lower, I learned a lot while I was there. I got tremendous breadth of information, a lot of which is conceivably actionable. Not “actionable” in the sense that I expect to immediately implement everything that I learned the second I get back to the office, but more that I have a couple of ideas that I’m itching to put into practice soon and a panoply of others that will give me a fantastic initial foothold should the right situation present itself.\nSo, in short, this was a deviation of my expectations—but frequently in a good or interesting way! And, really, those deviations make sense when you think about the structural differences between corporate conferences and academic conferences."
  },
  {
    "objectID": "posts/plans-for-2024/index.html",
    "href": "posts/plans-for-2024/index.html",
    "title": "Some reflections, hopes, and plans for 2024",
    "section": "",
    "text": "On the Persistence of Future Plans: AI Art (Copilot Designer)\n2023 has just about come to a close. I thought I was due for an audit on how things went and where (I think) things are going."
  },
  {
    "objectID": "posts/plans-for-2024/index.html#section",
    "href": "posts/plans-for-2024/index.html#section",
    "title": "Some reflections, hopes, and plans for 2024",
    "section": "2023",
    "text": "2023\nIf you had asked me on January 1st what I’d expect to happen for the year, just about all of my predictions would be wrong. Some of the gaps reflect shocks and major stressors. I never would’ve guessed that I was going to be laid off and unemployed for 4 months. I never would’ve guessed that I’d need to buy a new (used) car so that my family can accommodate my work travel. I never would’ve guessed my mortgage would skyrocket thanks to Florida’s collapsing home insurance market. I never would’ve guessed that my HVAC unit would die on me. (Or, perhaps worse, that my Xbox One would too1.\nBut many of the gaps reflect genuine joys. I never would’ve guessed that my wife’s health would improve (in the jagged, stochastic way that only chronic illness can) enough that she’d be able to start working again. I’d never guess that I’d find genuine enjoyment, as well as personal and professional satisfaction, from working a full-time gig so ostensibly askance to my academic background. I’d never guess that I’d get the chance to also be entrepreneurial and use free time to work on some of my best, most meaningful research to date (I helped field a survey for the NYU Queer Data Lab and co-wrotethis report on a survey I consulted on with Neighborly Faith about Christian Nationalism and religious pluralism in the US)—and that I’d get paid to do it. Fuck, honestly, I’d never guess that I’d have free time. And I did! Enough to finish nearly a dozen games, read/listen to about 70 books (Mangas count, damnit), train to run a respectable 5k time, pick-up writing for fun again, and make a useful R package! I never would’ve guessed at how deep my pride, love, and awe would stretch, watching my daughter grow into such a kind, sharp, adventurous, and forthright little human. She surprises me daily—and often more frequently than that—by her words and actions and also by how these things only deepen the reservoir of love I have for her. The greatest honor of my life is being her father."
  },
  {
    "objectID": "posts/plans-for-2024/index.html#hopes",
    "href": "posts/plans-for-2024/index.html#hopes",
    "title": "Some reflections, hopes, and plans for 2024",
    "section": "2024 Hopes",
    "text": "2024 Hopes\nI have some hopes for this year.\nI’d really like to not get laid off again; that’d be nice.\nI hope that I’ll continue to learn and find enjoyment in my work at Universal and to continue my consulting work. There are a few major expenses coming; I’m hoping that they’re spaced out in a way we can handle without adding more gray to my beard. We’re hoping to do some travel and my wife and I are hoping to spend more time together as a family—and I’m also hoping for quality time with family and friends too.\nI’m hoping we get another good year of health with my elderly dog and aging cat. I’m hoping my wife’s health continues to improve and she finds her vocations fulfilling.\nI’ve decided to run a half-matathon, I’m hoping for a new PR at the distance. Funny enough though, I’ve only ever ran full marathons—so a PR is gauranteed no matter how slow I go!\nSpeaking of running: After spending most of my ~20 year running career running shirtless and sunscreen-less under the increasingly fierce Florida sun, I’m finally taking care of my skin and seeing a dermatologist to assess the damage. It’s time to pay the piper; I’m hopeful his price ain’t too steep. (Seriously: wear your fucking sunscreen).\nAnd, of course, I’m hopeful that my daughter continues to be happy, healthy, curious, and kind.\n\nAbout the whole 30 thing\nThis next year marks the start of my 4th decade. I’m honestly more surprised by how little I’m phased about 30 being around the bend. I have a few friends who dreaded it; I’m pretty indifferent. People have been thinking I’m 30 since I was, like, 23 thanks to 1) my facial hair; 2) being in grad school; and 3) all the aforementioned shirtless running. So it’s almost feels like I’m stepping into the age that everyone’s assumed me to be anyhow. Though I’d be lying if I said that the increased prominence and density of gray hairs didn’t niggle me a little.\nI have a pretty strong vision of what I want my life to look like by 40. And while I expect some of those details to change, I don’t think there’s going to be as much change in vision from 30-40 as there was from 15-25 or 20-30. For better or worse, the stressors of the last few years have really forced a lot of things into perspective. My professional goals are tied less to titles, employers, and technologies than they are to the types of work I’m doing, the broader skills I’m learning, and the kinds of folks I’m doing it with and for. But these aims are all now largely in the service of other aspirations. Most of the vision actually constitutes things what I hope my body looks like, how I hope it’ll function vis my running, exercise, and play; how much time and resources I can expend on things that replenish me like travel, hobbies, and time with loved ones; quality time and adventures spent with my family, being there for events milestones; the capacity and positionality I hope to have to serve others; dedication to spiritual practice (I hope to one day graduate from “a bad Quaker” to “not a bad Quaker”); and proximity to family to help them as they undergo their own transitions this next decade. But I’d call the vision strong in the same way that I’d refer to a van Gogh as strong: the impression is solid and vibrant; the essence unmistakable not “despite” the lack of precise detail but because of the lack of precise detail."
  },
  {
    "objectID": "posts/plans-for-2024/index.html#goals-for-2024",
    "href": "posts/plans-for-2024/index.html#goals-for-2024",
    "title": "Some reflections, hopes, and plans for 2024",
    "section": "Goals for 2024",
    "text": "Goals for 2024\nI do have some more measurable goals for 2024:\n\nIngest 100 books (manga, comics, and Great Courses count)\nListen to 200 new-to-me albums\nCheck in to Duolingo 100 times\nPlay 10-15 new games (board/tabletop games count)\nLearn to cook 10-15 new recipes/foods (new cooking techniques to rehash old recipes count)\nWatch 10-15 new-to-me movies\nWatch 20-30 new/new-to me TV seasons.\nIngest 50 documentaries (YouTube videos count if they’re more than 20 minutes and educational; podcasts count if over 30 minutes)\nKeep my weight between 150-160lbs\nRun 1,200 miles\nVisit the gym 50 times\nDo 25 sessions of Yoga\nDo 100 sessions of core\nRun one race with a performance I’m proud of\nWrite 25-30 issues of Pulse of the Polis\nWrite 20-30 pieces of non-PotP content (blog posts, reviews, white/working papers, tutorials, poetry, micro-fiction, whatsver. So long as it took some effort to write)\nWrite 50 daily dairy entries\nPost 50 things of value to others on LinkedIn\nAttend at least one Quaker Meeting\nMake 5-10 videos; one at least 5 minutes long.\nContribute to/make 3 open source/code projects.\nDo 5-10 DIY projects (home repairs and improvements count so long as I’m the one actually doing them; digital art counts too so long as it’s something I’d hang in my house).\nHave 15-20 coffee chats with long-distance friends new and old (actually seeing them counts too).\n\nThese aren’t designed to be “resolutions.” If anything, they’re more like OKRs for my 2024. I fully expect to fail in a lot of these. Hell, maybe all of them. But in failing to hit these, I’ll still be doing things that are useful and meaningful to me. I love movies but I hardly watch them; the goal isn’t a limit or a chore but a return to something that I enjoy because I love overthinking pop culture—but I can’t do that if I don’t engage in pop culture! I love music; 200 albums is probably fewer than I’ve listened to this year and will help me on my own journey in drumming. And, you know what: 10 books read2 is better than 0 and mindlessly scrolling Reddit. 10 newsletters are better than 0 and me not staying abreast the literature. 1 game is better than 0. (And since I bought a new Xbox, I’d best get some use out of the damn thing). 100 miles is better than 0. 1 friend seen is better than 0.\nAnd in striving for these things, I’ll be improving habits that I’ve been working on for a while, like taking better notes of my day and of the things I’m reading, being present, showing myself love by taking care of myself and my body. And it serves to focus my efforts in those habits. It also helps me be more deliberate about my attention: I really don’t want my Reddit scrolling distance to be an appreciable fraction of the Earth’s fucking circumference. I’ll settle for an appreciable fraction of the length of a small country, though."
  },
  {
    "objectID": "posts/plans-for-2024/index.html#some-sketching-of-production-strategy",
    "href": "posts/plans-for-2024/index.html#some-sketching-of-production-strategy",
    "title": "Some reflections, hopes, and plans for 2024",
    "section": "Some sketching of production strategy",
    "text": "Some sketching of production strategy\nI’ll close out with some thoughts and plans about my personal tech stack and content strategy for the year ahead. While I hate to make it sound like my life is a commodity and my hobbies must be orchestrated as if the ghost of Frederick Winslow Taylor was clocking my productivity, I think that it’s the best verbiage and framework for my life right now. At one point I was so “in” on my own “personal brand strategy” that I was too paralyzed to actually make all that much without extrinsic motivation. That made me miserable. So then I relaxed, but I became so lackadaisical that I didn’t make anything! And that made me miserable. Turns out moderation is both desirable but also hard? Who knew!\nSo here’s what I’m planning both for this year and the future (at least as of today):\n\nI’m investing in (both in terms of time and probably money) generative AI plan to help my tech development work, to make custom images to accompany/accelerate my creative outputs, and to act as an editor for all of the writing I’ll be doing. I plan on doing my own writing though, it’s generally pretty mediocre at actually executing on written creative works and I don’t want to write mediocre things. It can be decent to bounce ideas off of, but I’ve spent years developing my voice. I’m far too pot-invested to cede it to an AI. (Though you can bet your ass I’m having it write code documentation and elements of projects where serviceable English is sufficient).\nI’m using Obsidian for my note taking, project sketching, and goal tracking. I may write a blog post about how I’ve set it up but that’s for another day.\nI’ll be keeping Substack for now because I couldn’t afford to run PotP with the features I want on another platform. I’m hoping to make the newsletter self-sustaining (or at least worth investment due to other payoffs it provides) in the next year. I’m also hoping, within that time, the platform will change its position on its decision to profit from openly White supremacist publications3.\nThis blog is written in Quarto. That’s probably how I’ll be doing most of my technical writing. Vanilla markdown is what I’ll do for my other writing.\nI’ll be writing “data journalism-y” things, my more direct and actionable tutorials, and my more timely reviews and (informed) thought pieces on Medium and its ilk. This is mostly for discovery: Medium is a highly-trafficked, well-indexed site.\nQuick hits (like random data visuals and quick—but interesting—facts, helpful insights and realizations), will be published here and on Social Media. I’m thinking of posting video versions on TikTok (ugh, I’m gonna get on that I guess) and/or YT Shorts depending on the content. (Since TikTok is new for me and less about “channels”, I’ll post a more eclectic range of things. YT shorts will tend towards “infotainment” rather than code).4\nI’ll be using osf for original research projects that are more about social science theory building, testing, exploration and about contributions to research methodology. Unless I work with academic coauthors, I don’t see myself submitting to traditional peer review (though I’ll happily review a few things). But I may ask for some actual peers to read over and review these outputs.\nLonger essays, reviews that are about more than just the piece of media itself, and syntheses of various media/individual topics/facts will likely live on YouTube proper. Some might instead/also get white paper treatments or essays on Medium. But their primary home will probably be YouTube.\nThis blog is going to be a grab bag of reviews, observations, announcements, code, artworks, poetry and short fiction (if it’s good enough to share)5, and short tutorials. The place where anything and everything can go because, why not, it’s my little home on the web—why not go maximalist with it? My home isn’t meant to be turned into grist for the algorithm; it’s meant to be a place that’s lived in.\nI’ll be primary using R and Python but don’t be surprised if some other languages crop up in pursuit of the year’s goals (namely Rust and JavaScript). Which means I may be using VScode more.\n\nThat’s all for now! It’s been a ride. Happy 2024! Also, Sundays are the start of my week so this blog totally counts towards my 2024 goals. Only 19 to goal!"
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html",
    "href": "posts/Qualtrics-random-groups/index.html",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "",
    "text": "Recently, some coauthors and I were working on a survey experiment in Qualtrics where we were assigning people into 4 different groups. This is something that Qualtrics can do really easily with its in-built randomizer function. If you use this, and keep evenly present elements on, your respondents will be sorted in roughly evenly. So if you have 16 respondents and 4 groups, on average you’ll get 4 people per group.\nBut we had a different situation. We had four groups but we didn’t want it split evenly. We wanted it so that group 1 was half of the survey, and groups 2, 3, and 4 took up thirds of the remaining half.\nUnfortunately, Qualtrics doesn’t seem to have an option that let’s you select unequal probabilities natively. If you deselect “evenly present elements” it’ll just randomize without any real concern towards whether your groups end up looking roughly equal. But that’s not what we needed. So I wanted to write about our solution to this so, hopefully, others in a similar situation might be able to find a work-around that works for them, too!\nOur solution used embedded data, then branches, and the standard randomizer. I’ll also talk about how you might be able to generalize this to any number of conditions (although some are more work than others)."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#starting-with-embedded-data",
    "href": "posts/Qualtrics-random-groups/index.html#starting-with-embedded-data",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "Starting with Embedded Data",
    "text": "Starting with Embedded Data\nAs you probably know: After you’re done with your Qualtrics survey, you can export it as a csv or as a xlsx file. In addition to the answers to your questions, it’ll also include some project and respondent metadata. But you can also program the survey for it to include you’re own custom metadata to be embedded in the project. You can do this by clicking on the “add new element here” button and adding embedded data to the project.\n\n\n\nOnce you’ve done that, you can customize your field to have whatever name you want. You can also set the value for this field. Here I set a variable named “test” equal to the value “1”. When I export the results for this survey, there will be a new column called “test” where the value for every respondent will be 1.\n Importantly, though, you can also set multiple conditions for the embedded data field! You do this by clicking “add below” and adding more embedded data sections.\n\nPop quiz! What would you get in the “test” column if you ran this survey as is?\nYou’d get 4 because, right now, you’re telling Qualtrics:\n\nInvent a column called test. Set it equal to 1.\nActually, remember test? Set it to 2.\nI lied. Make it 3 now.\nJk. 4.\n\nHere’s where we start using the randomizer."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#omg-so-random",
    "href": "posts/Qualtrics-random-groups/index.html#omg-so-random",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "“OMG so random…”",
    "text": "“OMG so random…”\nAs it stands now, you’re going to end up with a field called test where the value is set to 3 for all respondents. Instead of that, we can use the randomizer to make it so that 1/4 of the sample has a value of 1 for test, 1/4 has a value of 2, 1/4 has a value of 3, and 1/4 has a value of 4.\n You may be thinking “Cool. But this doesn’t do anything for me. I’m right back at equal probabilities for my experimental conditions.” And you’d be right if we were stopping here.\nBut we’re not stopping here.\n\nWe’re going to nest randomization conditions.\n\nThis programming is pretty similar to what we ended up using. Let’s follow the logic of the flow:\n\nRandomly pick either the test 1 condition with 50 % probability.\nIf it is not picked, then randomly select one of the remaining 3 conditions with 1/3 probability each.\n\nThis will mean that half of the respondents will get test1 and 1/6th will go to each test2, test3, and test4. Which is what the situation is in the second image and exactly what we were looking for in the real-life scenario.\nThis nesting is the key thing here. By nesting your randomizers strategically, there are few combinations that you won’t be able to achieve. This sort of set up can be really useful if you’re doing a multifactorial survey experiment.\nWe can wrap this all up with a neat bow through some branching."
  },
  {
    "objectID": "posts/Qualtrics-random-groups/index.html#branch-it-out",
    "href": "posts/Qualtrics-random-groups/index.html#branch-it-out",
    "title": "How to Do Unequal Randomization in Qualtrics Surveys",
    "section": "Branch it out",
    "text": "Branch it out\nNow we can do some branches so that our participants only see the blocks that they’re intended to see. Then branches work off of boolean logic. If a condition is TRUE then it’ll run a particular course of action. If it’s FALSE then it won’t run that action. In Qualtrics, you can set the branching logic so that it reflects the metadata that was assigned to participants through the randomizer.\n After that, you can add the block that you wanted people with test condition 1 (and only test condition 1) to see. And if you repeat it out, then you can complete the whole survey flow. Here’s basically what our survey looked like once the logic is completed all the way to the end.\n\nOne of the things I really like about this approach (rather than, say, using a random number generator and a bunch of then branches) is that this makes your later analysis pretty easy. If you’re making pivot tables, you can group off of that variable’s value. If you’re doing ANOVA in SPSS, Stata, or R, you’ve got a single variable already to go without having to do any post-hoc coding. It also makes it easier to set dummy variables if you do a more advanced regression analysis. I’m personally a big fan of doing small-to-moderate amounts of work up-front to save myself larger amounts of work down the road. I’m a big believer that work borrowed from your future self compounds with interest."
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html",
    "href": "posts/quarto-custom-font-windows/index.html",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "",
    "text": "The Writing on the Wall\nCreated by Author & Easy Diffusion\nThis is one of those posts motivated by never ever wanting to subject anyone to the insanity that I experienced while struggling through what only seems to be a simple idea.\nIt was a simple dream, really. I was writing up a report in Quarto. I have an OpenType Google font that I’m fond of. I wanted to use said font in said document. Quarto supports custom fonts. It should be simple to put in my new font—right? RIGHT?!\n(Click here to go directly to the solutions. Keep reading on if you want a bit of commentary and description of the problem. If it sounds like I went crazy, it’s because I did.)"
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html#the-problem",
    "href": "posts/quarto-custom-font-windows/index.html#the-problem",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "The Problem",
    "text": "The Problem\nI’ve installed a Google font called Vollkorn—and I wanted to use it in a Quarto report that I was writing up. The Quarto documentation suggests that using custom fonts would be a pretty straightforward task: Just set mainfont in the YAML header to the name of your font family of choice and the fontspec Tex package would handle the rest on the back-end. And if I wanted to switch from the default font to something else already packaged with Windows (like Calibri or, because I’m just a troll at heart, Comic Sans), it’s very straight-forward!\n\n\n\n\nCode\n\n```{qmd}\n---\ntitle: \"Example\"\nauthor: \"Peter Licari\"\neditor: visual\n---\n\n## This is an example\n\nSphinx of black quartz, judge my vow!\n```\n\nOutput\n\n\n\nFigure 1: With standard font\n\n\n\n\n\n\n\nCode\n\n```{qmd}\n---\ntitle: \"Example 2\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Comic Sans MS'\neditor: visual\n---\n\n## This is an example\n\nSphinx of black quartz, judge my vow!\n```\n\nOutput\n\n\n\nFigure 2: With accursed font\n\n\n\n\nBut let’s take a look at what comes up when I try to use Vollkorn:\n```{r}\nrunning xelatex - 1\n  This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)\n   restricted \\write18 enabled.\n  entering extended mode\n  \nupdating tlmgr\n\nupdating existing packages\nfinding package for Vollkorn(-(Bold|Italic|Regular).*)?[.](tfm|afm|mf|otf|ttf)\n\ncompilation failed- no matching packages\nPackage fontspec Error: The font \"Vollkorn\" cannot be found.\n```\nWhich is incredibly frustrating because I can go to my Windows fonts1 I can clearly see the damn font there. All installed. Mockingly.\n\n\n\nFigure 3: Vollkorn and LaTeX mocking me by claiming that the former is not installed.\n\n\nIf I were to click-on and inspect the properties of the Vollkorn font and compare them to Comic Sans font, though, you’ll notice a couple of interesting things. First, as highlighted in yellow, the files are located in entirely separate roots! There are actually two ways we can solve this—but the bigger issue is with what’s highlighted in blue: Vollkorn is a Variable Font.\n\n\n\n\n\nFigure 4: File path for cursed font (Comic Sans)\n\n\n\n\n\n\n\n\nFigure 5: File path for Vollkorn\n\n\n\n\nVariable fonts are a specific kind of font that offers more flexibility for designers than simply having separate, static versions for, say, bold, Italic, bold italic, etc. It turns out, however, that the default Quarto pdf render engine, xelatex does not play well with variable fonts. And by well I mean at all—it straight-up doesn’t work.\nAnd what about the difference in file roots; what the heck is going on there? As I learned the hard way, this is actually standard behavior since at least Windows 10. When you install a font, you have to specify that you want it to be for all users. Only then will it go to the Windows/Fonts directory—which is where the fontspec LaTeX package looks by default. Otherwise, it’ll get installed within the AppData/Local directory. Hence why the error says the font couldn’t be found. I could find it, but it wasn’t where fontspec was looking! However, with Windows 11, the option to install for all users doesn’t initially appear when you right-click on the font to install it. Clicking “Install” will only install it locally.\n\n\n\nFigure 6: Standard install available at right-click won’t do what you think it should."
  },
  {
    "objectID": "posts/quarto-custom-font-windows/index.html#sec-solutions",
    "href": "posts/quarto-custom-font-windows/index.html#sec-solutions",
    "title": "How to add a Variable Font to a Quarto PDF on Windows",
    "section": "The Solution(s):",
    "text": "The Solution(s):\nThere are actually a couple solutions for this: One that’s relatively simple and another that’s more involved. I’ll go simple, then go more involved. (Even though that is the exact opposite way that I discovered these solutions for my own purposes…)\nBefore doing dealing with either the simple or involved routes though, let’s go ahead and knock out the file path; we’ll need to know where the file lives in order for either solution to work.\n\n1. Address the file’s, well, address.\nYou’ve got two options for this. The first is to uninstall the fonts2 and then re-install them so that they are installed for all users. You can accomplish this by selecting “Show more options” after right-clicking on the font file(s) you want to install. From there “Install for All Users” should be visible (it’ll often be accompanied by a little shield).\nIf you don’t want to reinstall the file, you can use the mainfontoptions parameter and redirect where fontspec is looking:\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    mainfontoptions:\n      - Path = C:/Users/prlic/AppData/Local/Microsoft/Windows/Fonts/\n      - Extension = .ttf \neditor: visual\n---\n```\nFor the remaining bit of advice, I’m going to assume that you’re going with the reinstall option. However, if you didn’t, all you’d have to do is make sure those two parts of mainfontoptions are specified.\n\n\n2. Handle the Variable Font Type\n\nThe easy way:\nWhile Quarto’s default Tex engine on Windows doesn’t play nice with variable fonts, one of the optional rendering engines, LuaLatex works well! So all you would have to do is specify the pdf-engine parameter:\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    pdf-engine: lualatex\neditor: visual\n---\n```\nWhy use xelatex over lulatex or vice-versa? There’s a couple interesting Reddit threads on the topic. It seems that LuaLatex is preferred but xelatex is a bit faster (possibly) and useful for backwards compatibility on older renders.\n\n\nThe more involved way\nBut let’s say that you’re committed to xelatex. Or you’re me and you didn’t realize that a way easier option existed the whole goddamn time prior to writing a blog post.3\nFortunately, many of the variable font styles available from Google Fonts also come pre-packaged with the so-called “Static” files (the aforementioned separate italic, bold, and bold-italic files). So instead of installing the variable-font, you would install all of the individual types, which you can set manually in mainfontoptions.\n```{qmd}\n---\ntitle: \"Example 3\"\nauthor: \"Peter Licari\"\nformat: \n  pdf:\n    mainfont: 'Vollkorn-Regular'\n    mainfontoptions:\n      - BoldFont = Vollkorn-Bold\n      - ItalicFont = Vollkorn-Italic\n      - BoldItalicFont= Vollkorn-Bolditalic\n      - Extension = .ttf \neditor: visual\n---\n```\n\n\nEt viola!\nEither way that you do it, you’ll be able to get the fonts loaded as you’d like!\n\n\n\nFigure 7: Success!\n\n\n\nI hope this was helpful to others and saved them the headache that I dealt with when trying to just make a simple report!"
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-understand-what-this-output-is-telling-me",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-understand-what-this-output-is-telling-me",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not understand what this output is telling me",
    "text": "I do not understand what this output is telling me\nThe first thing I do—and I know that this is going to sound just so incredibly pedantic, but please stay with me—is do my best to read the error message closely.\nDon’t get me wrong: A lot of these error messages have been written by software engineers—and software engineers aren’t known to be, uh, particularly verbose or descriptive. But, a lot of package developers, especially those in or adjacent to the Tidyverse, work to include more informative errors. But, even for these, you need to know what you’re looking for.\n\nDescriptive Errors\nLet’s say that I’m playing around with the mtcars dataset and I try to select a column that doesn’t exist:\n\nsuppressPackageStartupMessages(library(tidyverse))\n\nmtcars %>%\n  select(type) %>%\n  head()\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `type` doesn't exist.\n\n\nThis error is very clear: it’s telling me that the column that I passed along to the select statement (type) is not a variable in the data frame. My resolution here is similarly clear: Fix what I’ve passed to the select statement so that I can get the proper output.\nA similarly descriptive error (at least once you’ve gotten the hang of it) will come up when you invariably put in one too many (or few) of some critical character in R: +, }, ,, ), ], etc. It will tell you that there’s an “unexpected character” somewhere and will provide you with where the incongruity is.\n\nggplot(mtcars, aes(x = wt y = mpg)) +\n  geom_point()\n\nError: <text>:1:27: unexpected symbol\n1: ggplot(mtcars, aes(x = wt y\n                              ^\n\n\nThis error is admittedly a bit tricky sometimes to get your head around at first because it ostensibly crops up in the absence of a symbol and not just the “unexpected presence” of one. But it makes more sense once you remember that y is indeed a symbol and it isn’t expected there—function arguments are supposed to be separated by ,s!\nNormally when I see unexpected symbol errors, my first thought is to make sure that I’m balanced on the number of left-hand symbols (i.e., [, (, {) and right hand symbols (], ), }); if you’ve got one on the left, you need one on the right and vice-versa. Then I’ll check for errant commas—or that they are where they’re supposed to be.\n\n\nLess Descriptive Errors\nSome errors require a bit more understanding of R as a programming language to parse before they become “obvious.” Say that I want to add a new column to the mtcars data. This will work:\n\nmtcars$new_col <- c(1,2)\n\nThis will not:\n\nmtcars$new_col <- c(1,2,3)\n\nError in `$<-.data.frame`(`*tmp*`, new_col, value = c(1, 2, 3)): replacement has 3 rows, data has 32\n\n\nYet this will:\n\nmtcars$new_col <- c(1,2,3,4)\n\nAnd, again, this will not.\n\nmtcars %>%\n  mutate(new_col = c(1,2))\n\nError in `mutate()`:\nℹ In argument: `new_col = c(1, 2)`.\nCaused by error:\n! `new_col` must be size 32 or 1, not 2.\n\n\nWhy does it matter that this vector has 3 and that the data has 32?! 2 and 4 seemed fine! And why did 2 work for base R but fail in the mutate context?\nWell, because R allows for this thing called “recycling.” Basically, if you try to put two unequal vectors together but one of them can evenly divide into the other, R will “recycle” the values over and over again until they fill out the space.\n\ndata.frame(x = 1:6, y = 1:2)\n\n  x y\n1 1 1\n2 2 2\n3 3 1\n4 4 2\n5 5 1\n6 6 2\n\n\n3 doesn’t divide evenly into 32, so an error crops up there. However, dplyr’s mutate doesn’t recycle values greater than 1. It’s a quirk of that package. You will come to learn many of the quirks of the packages you use most often. But, in both cases, R told you exactly what was wrong although it only told you why that was an issue with mutate.\n\n\nNondescriptive Errors\nLet’s say that you’re returning to your script in a new session. You were trying to filter mtcars by those with a value of 4 for the cyl column.\n\n\nWarning: 'dplyr' namespace cannot be unloaded:\n  namespace 'dplyr' is imported by 'tidyr' so cannot be unloaded\n\n\n\nmtcars %>%\n  filter(cyl == 4)\n\nError in filter(., cyl == 4): object 'cyl' not found\n\n\nOk. No problem. You’ll just go to a different part of your script where you saved a dataframe as df and looked at the top 6 values:\n\nhead(df)\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\nOk that’s not an error, but what even is it?!\nThere are other cases that I’ve come across where R will warn me about something going wrong with a Hessian, or that some object is or isn’t subsetable, or that something or another is singular. Sometimes, I will have absolutely no idea what this means! So what do I do? Google it!\n\n\nThe Common Solution\nUsually, I’ll copy the first full line or sentence of the error and pop it right into Google. However, I don’t usually copy in the full error if there’s text in there that I recognize as being driven by my specific input—like a column or object name. Including things that are only driven by what I’ve personally done in the search might reduce the number of helpful hits I’ll get.3 So I’ll go for the longest uninterrupted part of the error that doesn’t seem to contain something specific to my call.\nThis will usually bring me to a Stack Overflow or to another forum. Good ones tell you what to do with your code so that you can keep on trucking. The great ones are those that explain what the code they’re suggesting does on an intuitive level. The best ones do all that plus provide alternative ways of approaching the problem and/or explain in clear language why the original error came up in the first place."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-understand-the-output-plenty-but-dont-know-how-i-even-got-here",
    "href": "posts/r-punch-in-face-23/index.html#i-understand-the-output-plenty-but-dont-know-how-i-even-got-here",
    "title": "What to do when R punches you in the mouth",
    "section": "I understand the output plenty but don’t know how I even got here",
    "text": "I understand the output plenty but don’t know how I even got here\nLet me give an example of this happening on a project I did at my last job.\n\nRegressing in a Regression Problem\nI was working on functionalizing a type of analysis and spent some time with a logistic regression model. I wanted to predict outcomes from the approach I was testing to ensure it spat out reasonable estimates. First time I ran it, everything went fine—I got back a set of predicted probabilities all within the ballpark that I was expecting. I turned my attention to a marginal improvement I wanted to make in the function’s speed. Came back and re-ran the model. My predictions came back and everyone was either a 0 or a 1. Which is to say, not at all a reasonable set of predictions. Why did it just work before and why was it breaking all of a sudden?!\n\n\nData by Another Name\nHere’s another example that tosses an error that might be fairly familiar. Let’s say that I import in a data set and I try to summarize the mean values of a column I’m interested in (in this case, it’s the penguins data from the {palmerpenguins} package and I’m interested in bill_length_mm).\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.2.3\n\ndat4 <- penguins\n\ndata %>% \n  dplyr::group_by(species) %>%\n  dplyr::summarise(bill_length_mm)\n\nError in UseMethod(\"group_by\"): no applicable method for 'group_by' applied to an object of class \"function\"\n\n\nWhat do you mean a function?! I’m not passing a function, I’m passing data!\n\nOr, at least, I thought I was. This bit of code is short enough that you should be able to catch it—the data is called dat4 but I’m invoking data—which, until it’s assigned something else, is actually the name of a base R function. But if you’re working in a longer script and not being careful/deliberate with your names, this sort of thing can drive you crazy for a few minutes as you question your sanity.\n\n\nThe Common Solution\nThe causes for this sort of frustration usually boil down to people not working in what I’ll call a “linear” fashion. There’s a reason why folks who have been doing this a while insist that your script should be executable running it top to bottom without having to circle back or anything. But many practitioners (especially those who don’t have a strong coding background)4 will jump around in their scripts to tinker with this, try that out, maybe open up a new file and run some of the code in there. This also very frequently happens to folks who are new to RMarkdown or Quarto who have their test output coming out correctly because they ran chunks individually—but out of order. When it comes time to knit/render, they’re stymied by errors claiming that objects don’t exist but they can see the damn things in their environment—what do you mean they don’t exist?!.\nThe resulting analysis is one riddled with hidden dependencies or processes to work properly. This is pretty obvious with the second example, no data objects had been assigned to data so R still reserved that set of characters to refer to the function. (Which is actually a good tie-in to the first section: The error is telling you exactly what’s going wrong!). In the first example, my original data set had scaled predictors but then, in my quest to squash the minor thing, I overwrote that dataframe object to something with the same column names without scaled predictors. But I never updated the trained model. The result was that this “test” data had way more extreme values compared to the “training” data and, thus, everyone was given either 0s or 1s.\nMore often than not, if you’re getting an error or output that appears to make no sense in the context of the thing that you’re trying to do, the best advice is to take a short break and come back to the problem with fresh eyes. In order to prevent it from happening in the first place, frequently restart your session and make sure that you can get back right where you were with the code you’ve already written without having to do anything fancy. That means running your code top-to-bottom."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-this-code-return-the-type-of-output-i-expect",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-this-code-return-the-type-of-output-i-expect",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not know how to make this code return the type of output I expect",
    "text": "I do not know how to make this code return the type of output I expect\nA related, but distinct, issue from the previous one is when you have a clear vision in your mind for what you’d like to accomplish but you can’t quite seem to figure out how to get the data to behave the way you’d like. This often happens to me when I’m designing a function and I know how I want the output to look, but I’m having a tough time figuring out how to get there from the input. Another, perhaps more common, example is trying to ingest data and/or manipulate a dataframe so as to create something structured like the outcome you have in mind. (Say, for instance, that I’m trying to take wide-formatted data and pivot it into long formatted data, but I need summary statistics and only a select few columns). There are few things more frustrating than having a gist of what you’re hunting for but not being able to actually see it to fruition.\n\nCommon Solutions\n\nStart with the end in mind\nOftentimes these problems will come up because we are trying to get data of some sort “shaped” in a particular way—usually not for its own sake but to prepare it for some additional step. But I find that one of the most helpful first steps you can take is to take a moment to fully flesh out what this subsequent step will take for you to be successful. What type of input is necessary? Does it require a vector, a list, a dataframe? How many values, of what type(s), do they have to be named—if so, are their any requirements or restrictions? Can they have NA/NULL/Inf values? Sketch out5 what you need things to finish as. Just like solving a maze by starting at the end, sometimes this can clarify the direction you have to take.\nOnce you’ve done that, continue to work backwards. Compare the number of rows/columns/values in what you need compared to what you started with. Think about what data manipulation functions and/or summary operations you’ll need to do to go from each step to the next.6 This may require some trial and error so, once you’re done, please be sure to clean up your code so that it runs top-to-bottom so that you don’t run into the mistake we were just talking about in the previous section.\n\n\nStand on the shoulders of giants\nOftentimes, for the work performed the vast majority of the time, the maintainers of the packages you are using for your transformation(s) have probably created tutorials or vignettes on how to do at least some of the steps that you’re looking to do. You don’t need to relearn how to pivot tables from wide to long and vice-versa: that documentation already exists in a way to easily grock the concepts! So when I google for things with this type of headache, I tend to keep an eye out for tutorials (on personal blogs and R-Bloggers) and on package vignettes. You may have to string multiple vignettes together to fully traverse the gap between your start and ideal end-state, but these resources can be tremendously useful. My google strategy here is to type in “how to do xyz in R” where “xyz” will refer not to my overall aim but to the specific kind of function/operation that I want to perform to complete the next step. (And, if you know the package you’re using/need to use—add it to the search query).\nA tool that’s becoming increasingly useful for this is ChatGPT, especially GPT4, and other Large Language Models (LLMs). If you ask the model to not only help you take your input data and translate it to the output format you sketched earlier but to also explicitly detail the steps involved it will be able to do that pretty well. Here is an excerpt from an example of me doing this with GPT3 with scraping a table from Wikipedia.\n\nYou can also provide it your code as you currently have it and explain what you’re trying to do and it can help you debug and show you what to do instead to get to your desired endpoint.\nLLMs aren’t perfect, but if you have a sense of where you’re starting, and you know where you’re going, it can provide tremendous savings in your overall coding time. It may require you to troubleshoot some errors but it’ll often be a net benefit to have code that gets you 70-80% of the way there rather than try to code it all from scratch yourself. This may break down if you’re trying to do something particularly novel or niche though. In that case, it may help to return to the Internet and/or break the problem down into more discrete steps.\nIn any event: Whether you use an LLM, Google, or you brute-force it yourself—the most helpful solution to this I’ve found is to have a concrete vision of where you want to go, an understanding of where you are, and a roadmap for the discrete steps you will have to take to get there."
  },
  {
    "objectID": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-r-do-something-that-i-think-it-should-be-able-to-do.",
    "href": "posts/r-punch-in-face-23/index.html#i-do-not-know-how-to-make-r-do-something-that-i-think-it-should-be-able-to-do.",
    "title": "What to do when R punches you in the mouth",
    "section": "I do not know how to make R do something that I think it should be able to do.",
    "text": "I do not know how to make R do something that I think it should be able to do.\nFor me, very few things are as frustrating as knowing that I know how to do something but finding myself flailing when actually executing it. There’s a icy panic that hits as you grasp at tendrils of a solution only to realize that you’re merely catching smoke. Then a deep frustration, almost shame, at being failed by the organ that happens to also do the bulk of constituting what you think of as “you” —and cue you sitting here wondering why you’re under-performing: if you’re just tired, stressed, or experiencing early-onset dementia. (This happens to me a lot if you haven’t guessed).\nFor me, there are four common reasons for this to come up:\n\n1. You’ve done this kind of thing before, just can’t remember the steps.\nThis is why I unironically say that you should save every finalized line of code. Every. Line. And you should do your best to comment and format your code in such a way that you could return to it in 6 months and know what’s going on (e.g., using a consistent naming convention, commenting periodically, using names that make sense for your objects rather than just temp7). Storage is cheap and storing this will not be an issue for the vast majority of people who use R. There is absolutely no shame in going back and reusing your own code. Do not, I repeat, do not feel like you have to code every new project from scratch.\nIf you’ve lost your code or you didn’t format it so that it runs cleanly top-to-bottom, chances are that you had a little help before in the form of tutorials and vignettes. Ideally this time, you’ll vaguely recall a search string that at least got you going down the right rabbit hole (it helps if you remember where you got the info from so that you can add it to your search term).\n\n\n2. You’ve worked with this particular function before and are pretty sure its capable of doing what you envision.\nThe help documentation is your friend! For instance, I learned that {dplyr}’s case_when function has an option now for you to specify what the default output should be.8 But I couldn’t remember what it’s called. So I simply did this:\n\n?case_when\n\nWhich pops up a super handy pane that provides all of the help documentation the developers of the function have written for it.\n\nAnd, right there, we can see that the argument that I’m looking for is called .default!\nAnother example: I was trying to remove duplicated values from a dataset using {dplyr} ’s distinct. But when I got the data back, all I had was a single column. It wasn’t until I looked at the help documentation that I was reminded that the default behavior is for a parameter called .keep_all to be set to FALSE!\n\n\n3. You know that this package has a function that is useful but you can’t remember what it’s called.\nThis is where RStudio really comes in handy. If you type in the package name plus two colons (::) you can see all of the exported objects from the package. I frequently space-out on the name of the function is that drops NA values from dataframes. I know that it’s in the {tidyr} package though. So I’ll write out tidyr::…\n\nThat’s it! drop_na!\nThis helps me a lot when I have a vague sense of what the function I’m looking for is named and/or what it generally does. Worst case, I’ll just Google something like “how to drop NA values in R.” That is, I’ll do a search emphasizing the kind of action that I want to take.\n\n\n4. I feel like the sort of thing I want should be possible but I don’t know how to do it.\nI know that this will come as a terrible, Earth-shattering shock—but my recommendation here is to use either Google or a decent LLM. If you’re going the first route, it’s helpful to articulate what kind of end goal you have in mind. For instance, if I wanted to make a Waffle chart, I could Google “make waffle chart ggplot”. If I wanted to learn how to implement an upload box into my shiny app, I would go “add upload button shiny” or something to that effect. Even if I had a vaguer sense of what I wanted to do, I can type out what my end goal is and stick an “in R” at the end. (e.g., “how do I do bayesian regression in R?”, “how do I make interactive maps in R?”, “how can I connect to aws in R”, etc.). From there, find some code that others say works for the specific use case described on the page/in the forum/in the solution and apply it to yours. If you run into errors, follow the guidelines I’ve mentioned elsewhere in the post. As I mentioned at the top, learning R isn’t about knowing everything—it’s knowing that you don’t know, but learning that it’s necessary to ask!"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html",
    "href": "posts/rosie-game-sim-23/index.html",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "",
    "text": "Children learn as they play. Most importantly, in play children learn how to learn.\n- O. Fred Donaldson\n\nNot too long ago, my grandparents gifted my daughter her first board game: First Orchard. It’s a simple little game that aims to teach kids how to follow game rules and how to take turns, as well as reinforcing toddler basics like counting, hand-eye coordination1, and colors. Here’s the premise:\n\nThere are four, different colored “trees” in the gamespace (“orchard”; probably played on the floor) with four fruit apiece: Green apples, yellow pears, red apples, and blue plums.\nThere is a hungry crow that will eat all of the fruit if they make it into the orchard; they start the game five movements away.\nThere is a six-sided dice. Four of the sides correspond to the colors of the tree (green, yellow, red, blue), a basket that allows you to take from any tree, and one has a image of a crow.\nPlayers take turn rolling the dice. If they get a color, they take a fruit from the tree associated with that color. If all the fruit has already been picked, nothing happens. If a basket is rolled, then the player can pick a fruit from any tree. If the crow is rolled, the crow is advanced one space.\nThe game ends when all of the fruit has been picked from the tree (the players collectively win!) or when the crow reaches the orchard (the players lose).\n\nAs I spent time playing2 with my daughter, I realized that we were winning. A lot. Which made sense to me. One lesson that all children (and all us children grown tall) must learn is how to lose—but it’s admittedly far easier to teach a kid how to follow rules if they think there’s a strong chance that the rules will lead to success. (Though not a perfect chance so that the game is engaging and has risk and stakes). I wondered though: Were we just on a lucky streak or is the game actually biased towards the players? How long do the games usually take? Are there different average lengths between successes and failures? And if it is biased, how much more likely is success than failure?"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#simulating-the-game-in-python",
    "href": "posts/rosie-game-sim-23/index.html#simulating-the-game-in-python",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Simulating the game in python",
    "text": "Simulating the game in python\nIn order to see how generous this game is to the players, I decided to code it up in python.\n\nSet-up\nFirst I imported my libraries, made a quick little “roll the dice” function, and then setup an empty array for the results:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport scipy as sp\n\n\ndef die_roll():\n roll =  np.random.randint(1,7,1)[0]\n return(roll)\n\n\nresults = []\n\nFrom here, we can actually get down to articulating the rules/procedures of the game in python.\n\n\nEncoding the game rules in, well, code\n\nGame Start\nFirst, let’s get the game instantiated. We start with 4 different colored “trees” with 4 fruit apiece, a crow that needs to move 5 spaces before getting to the “orchard” (-5) , the number of turns that have elapsed (starting at zero), and the “orchard”—the collection of trees and the number of fruit that have yet to be picked (16 to start)\n\nred = 4\ngreen = 4\nyellow = 4\nblue = 4\ncrow = -5\nturns = 0  \n  \norchard = [red,green,yellow,blue]\norchard_sum = sum(orchard)\n\norchard_sum\n\n16\n\n\nThe game continues while either the sum of fruit in the orchard is greater than 0 or the crow’s distance is less than zero . So going to use a while loop. Every loop is going to be a turn; so every go round is going to increment turn and roll the dice.\n\nwhile orchard_sum != 0 and crow != 0:\n  turns += 1\n  roll = die_roll()\n\nDepending on how that roll lands we’re going to do one of a few things:\n\n\nAdvance the crow:\nMove the crow forward if the dice lands on a 6.\n\nif roll == 6:\n  crow += 1\n\n\n\nTake from a specific tree:\nSince I defined orchard as [red,green,yellow,blue], rolling a 1 will take from the red tree, 2 will take from green, 3 from yellow, 4 from blue—but only if the tree that was rolled actually has some fruit left! Otherwise, no action happens.\n\nelif 1 <= roll <= 4:\n  if orchard[roll-1] > 0:\n    orchard[roll-1] = orchard[roll-1] - 1\n  else:\n    continue\n\n\n\nTake from any tree\nThis one took a bit of thinking. What I decided to do was first ensure that all of the trees weren’t already at 0. If they weren’t then I decided to take from the tree with the most amount of fruit remaining. My thought being that, while it might feel good as a player to wrap-up a tree and bring it to zero, that would effectively prematurely eliminate one of the sides of the dice from doing anything productive. I had the feeling that a longer game would be biased towards the crow so I wanted to avoid that as much as possible.\n\nelif roll == 5:\n  if max(orchard) != 0:\n    largest_remaining = orchard.index(max(orchard))\n    orchard[largest_remaining] = orchard[largest_remaining] - 1\n  else :\n    continue\n\n\n\nEnding the turn (and game)\nFinally, determining the new sum in the orchard to determine if it’s time to kill the while loop. But, before the loop is killed, determining if the player or the crow is the winner.\n\norchard_sum = sum(orchard)\nif game_sum == 0:\n  winner = 1\nelif crow == 0:\n  winner = 0\n\nAnd then, once the loop is broken, recording whether the player was the winner and the number of turns it took.\n\nresults.append({'winner':winner, 'turns':turns})\n\n\n\n\nRunning the simulation\nLet’s go ahead and run this 10,000 times to get a sense of the game’s asymptotic behavior. You can click on the arrow to see how all the code comes together.\n\n\n10,000 simulated games\nnp.random.seed(131313)\n\nfor x in range(0,10000):\n\n  red = 4\n  green = 4\n  yellow = 4\n  blue = 4\n  crow = -5\n  turns = 0\n  \n  orchard = [red,green,yellow,blue]\n  orchard_sum = sum(orchard)\n  \n\n  \n  while orchard_sum != 0 and crow != 0:\n  \n    roll = die_roll()\n    turns += 1\n  \n    \n    if 1 <= roll <= 4:\n      if orchard[roll-1] > 0:\n        orchard[roll-1] = orchard[roll-1] - 1\n      else:\n        continue\n      \n    \n    elif roll == 5:\n      \n      if max(orchard) != 0:\n        largest_remaining = orchard.index(max(orchard))\n        orchard[largest_remaining] = orchard[largest_remaining] - 1\n      else :\n        continue\n  \n    elif roll == 6:\n      crow += 1\n    \n    \n    orchard_sum = sum(orchard)\n\n    \n    if orchard_sum == 0:\n      winner = 1\n    elif crow == 0:\n      winner = 0\n      \n  results.append({'winner':winner, 'turns':turns})\n\n\nAnd convert the results dictionary into a pandas dataframe\n\ngame_data = pd.DataFrame(results)"
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#exploring-the-results",
    "href": "posts/rosie-game-sim-23/index.html#exploring-the-results",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Exploring the Results",
    "text": "Exploring the Results\nLet’s take a look at how often people win the game!\n\nsb.catplot(data = game_data, kind = \"count\", \nx = \"winner\")\n\n<seaborn.axisgrid.FacetGrid at 0x2a72bc2b7d0>\n\n\n\n\n\nSeems like the players are winning it about 65% of the time! Well\n\nprint(100 * game_data[\"winner\"].mean(),\"%\")\n\n63.27 %\n\n\nSpecifically.\nLet’s see if the number of turns vary:\n\ngame_data.groupby([\"winner\"]).mean().style.set_properties(**{'color':'black'})\n\n\n\n\n  \n    \n       \n      turns\n    \n    \n      winner\n       \n    \n  \n  \n    \n      0\n      18.655323\n    \n    \n      1\n      22.012170\n    \n  \n\n\n\nIt looks like it takes longer for the player to win over the crow; if the crow’s going to win, it’s going to do so in about 19 turns. If the player wins, it’s going to take about 22 turns. May not sound like much but you try to keep a toddler’s attention for three extra turns!\nGiven how many simulations I ran, I doubt that this will be anything other than “statistically significant” but it couldn’t hurt to check.\n\nsp.stats.ttest_ind(game_data['turns'], game_data['winner'], equal_var = False)\n\nTtest_indResult(statistic=489.4841664968918, pvalue=0.0)\n\n\nLMAO a t stat of 489. NOW THIS IS POD RACING P-HACKING."
  },
  {
    "objectID": "posts/rosie-game-sim-23/index.html#conclusion",
    "href": "posts/rosie-game-sim-23/index.html#conclusion",
    "title": "Simulating My Daughter’s First Board Game in Python",
    "section": "Conclusion:",
    "text": "Conclusion:\nIt looks like my daughter’s first game is pretty strongly biased towards the player winning! Simulating the game 10,000 times reveals that players can expect to win about 65% of the time. Crows tend to win faster than players; if the fruit gets munched by a pesky peckish corvid, it’s going to happen sooner rather than later. This suggests to me that if the crow wins in this particular game, it’s because the player has a streak of bad luck. I might at some point tweak the rules a bit or record more granular data to delve a bit deeper, but I think I’ll cut this for now so I can go back to, you know, playing the actual game with my daughter."
  },
  {
    "objectID": "posts/surveying-beautiful-endeavor/index.html",
    "href": "posts/surveying-beautiful-endeavor/index.html",
    "title": "Surveying is a Beautifully Foolish Endeavor",
    "section": "",
    "text": "I binged Hank Green’s A Beautifully Foolish Endeavor yesterday and it is, let me tell you, just fantastic. I don’t plan on writing a longer review because I didn’t really write any notes. I didn’t want to take notes. Not because it was bad, but because I didn’t want to peel my eyes away from the page for the short seconds it’d take for me to grab my pen and notebook. I was too engrossed to want to do anything but live in its pages for however long it’d let me. So, no review—but maybe that ardor could stand in place of one.\nBut aside from just being human, well-written, and the right mix of cynical and optimistic, it is also just an eminently and endlessly quotable book. There are quotes here that I plan on using when I talk about machine learning in my upcoming data science class. Quotes that just stabbed me in the chest with self-recognition (cough cough Miranda being a workaholic cough cough). And then there are quotes that make me feel that Green is one of the few science fiction authors who actually understand the sociology and psychology of being a practicing scientist (which, ya know, SciShow and his M.S.—so it makes sense). But then there’s this quote near the end of the book that so perfectly described being a survey researcher that it made me laugh out loud:\n\n[We wanted to] dive deep into our survey responses. Except there were just too many…no matter how we filtered, there just wasn’t a good way.\nApril and I were griping about this around the black marble countertop in the kitchen when Carl came in and overheard us.\n“Just do a search,” Carl said.\n“The searches take forever and we don’t know what to search for,” April replied. “It’s just a bunch of dumb data. Most of the useful stuff is in text responses, which is impossible to parse.”\n\nI loved and loathed the reductionism of “dumb data”—mostly because it’s right. Until we data geeks make the decisions on what to model, how to model it, and how to frame and embed our results into a digestable narrative, it is literally just that. Dumb data, (sometimes) intelligently collected. And also, it’s so, so spot-on to note that almost all of the really juicy stuff is in the open-ended responses where people just let you know what’s actually going on. Or at least their inwardly-biased interpretation of their messy cognitive processes—which is still super cool and really interesting!\nAs an example: for my dissertation, I had completely written-off games like Hearts as sociopolitically unimportant until some respondent mentioned that there are online lobbies where people use the game to chat about politics. I never would’ve learned that if I didn’t bother to read what my respondents had taken the time to write. And there have been a few papers that make the point that we should really pay more attention to the nuance that people write in rather than (often incorrectly) discretizing them on the basis of hardline, ex ante coding rules.\nI’m envious that April and Maya had an alien superintelligence to parse through all of that data and get to all the juicy bits. Sure we have NLP and algorithims for market segmentation—but do they come from a near-omniscient monkey? Didn’t think so.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{licari2020,\n  author = {Peter Licari},\n  title = {Surveying Is a {Beautifully} {Foolish} {Endeavor}},\n  date = {2020-08-03},\n  url = {https://www.peterlicari.com/posts/surveying-beautiful-endeavor},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeter Licari. 2020. “Surveying Is a Beautifully Foolish\nEndeavor.” August 3, 2020. https://www.peterlicari.com/posts/surveying-beautiful-endeavor."
  },
  {
    "objectID": "posts/theory-reality-book-review/index.html",
    "href": "posts/theory-reality-book-review/index.html",
    "title": "Book Review: Theory and Reality",
    "section": "",
    "text": "Theory and Reality is an ambitious book that aims to accessibly cover a large swath of historical and philosophical ground. In many ways, it lives up to that ambition, making it well worth the read. In other, important ways though, the book occasionally devolves into muddy paste, sometimes making a re-reading necessary to fully grok what’s been covered.\n\nWhen I went to graduate school, my very first class was called “Scope and Epistemology”—or “Scope” for short. It was a rite of passage for all who entered UF’s political science PhD progam. Taught by the brilliant (and brilliantly kind) Larry Dodd, the course syllabus was often mistaken as one of its textbooks. The damn thing was 30 pages and outlined thousands of pages of books and articles, from Kuhn to Gleick to Mayr, Gattone, and even freaking Asimov, that meant to show us not only the diversity of practices in social science but the equally remarkable diversity of beliefs over just what the heck this thing called “science” actually is.\nIt was a whirlwind tour of epistemology, philosophy, and sociology of science. (Predominantly social science given, ya know, it was a poli sci PhD program—but not exclusively so, much to the course’s benefit). I was not unfamiliar with either philosophy or epistemology (I was a hair’s breath away from double-majoring in philosophy in undergrad), but it was an intimidating program. We were quickly reassured by more advanced students that, no, we didn’t have to “read” all of it—at least not in the sense one would read things in undergrad (a time-honored advisory that I would oft repeat many times myself to newer students, furthering the tradition). I still own many of those books and refer back to a few when I feel the need. The course cemented my latent loves of philosophy, epistemology, and science into (what I’m sure to be) a lifelong fascination: one focused with not only practicing social science but also interrogating “what even is this thing we call (social) science?” in philosophical terms.\nMany of us came to realize that the syllabus for Scope was less a contract of expectations and more a gift. That “whirlwind tour” was a synthesis of decades of epistemological tumult that Dodd had lived through first hand. And since the issues were far from settled, I feel like he was also giving us a preview into many of the ideas and controversies we would have to grapple with ourselves over the course of our scientific careers.\nThis love is what drove me to pick up Theory and Reality by Peter Godfrey-Smith (2003) from Amazon’s Audible service. 1Theory and Reality is an ambitious book that aims to accessibly cover a large swath of historical and philosophical ground. In many ways, it lives up to that ambition. In other, important ways though, the book at times does feel like it devolves into, to use Godfrey-Smith’s own verbiage, a muddy paste. There were times where I would relisten to chapters (sometimes three times) just to make sure I fully understood what was covered. I feel like the book rewards this effort, but not everyone will want to exert it. Still, what it does well, it does very well—making this book well worth the read.\n\nAt the outset, Godfrey-Smith expresses that his overarching aim in writing this book is to convey a history of the philosophy of science; specifically a history in two senses of the word. The first is a timeline of the predominating ideas of the discipline (as well as from adjacent fields such as sociology of science): How have we conceptualized this thing we call “science,” as well the actions (both idealized and actual) of its practitioners? The second is a narrative of how said ideas (and their progenitors) interacted with each other.\nThis is more or less what he manages to do in the first half(ish) of the book. The first two substantive chapters start with brief discussion of early 20th century empiricism and its major philosophical tenets and issues (e.g., the issues of induction and confirmation), shift into a chapter on Popper, two on Kuhn (or, rather, two primarily on The Structure of Scientific Revolutions), a chapter on Lakatosh, Laudon, and Feyerabend (not apiece—collectively), a short chapter blitzing through the sociology of science in the middle third of the century, then a chapter on feminist contributions to the subject, before pivoting to naturalistic philosophy. He doesn’t do so as a impassive retrospective observer though; he frequently interjects the discussions with both critiques and endorsements to the material. This includes both those that are generally accepted in the field as well as some of his own—though his is admirably careful and forthright in which views fall into which category. He does have a tendency to be a bit dismissive of some ideas without motivating clear reasons why (and these instances are highlighted due to the equally discernible tendency for other objections to get more strenuous refutations), but his contributions, on balance, add rather than detract from the quality of the section.\nThe core argument of this first section can be crystalized as follows: The early-to-middle(late) century for philosophy of science was characterized by a schism between empiricists (e.g., positivits) and those who saw science as more an expression of social patterns rather than a distillation of empirically-identified phenomena. Both sides had incredibly novel and important insights, but both often proffered leads that turned out to be dead-ends—or overplayed their philosophical hands, cornering themselves into very strange positions (Popper’s articulation of corroboration and some sociologists’ assertion that the world is in fact created at a foundational level by the theories and paradigms of scientists2).\nIt’d be a lot of ground to cover even without the additional commentary. But Godfrey-Smith manages to do so at a high average quality, albeit with noticeable variation. Perhaps it owes to my experience in Scope and in my undergraduate coursework prior to grad school, but I found it pretty easy to follow-along with the first few chapters—and I found the most of the interjections to be illuminating. (I especially liked his approach to the Black-Raven, White-Shoe issue in accounting inductive evidence—especially the version detailed late in the book when he discusses evidence). He was at his best when he dedicated his time to discussing narrower philosophical issues or the contributions of individual philosophers. The chapters on evidence, Popper, Kuhn, and the ensuing Lakatosh-Laudon-Feyerabend lightening round were some of the best in the book. Where things got less good (though not necessarily “bad”, for my view) is when he tried to cover entire fields in singular chapters. The Sociology of Science and Feminism chapters were among those aforementioned thrice-listens. I initially thought that it was because I was unfamiliar with the material, then I thought it was because I just wasn’t grokking it; but on the final time I realized that he was simply moving way too fucking fast. I sympathize with the challenge of distilling such large amounts of work into such a a condensed space—but considering he made two chapters out of Kuhn alone, it seems like a self-inflected wound with an obvious remedy: Split it out into a couple more chapters.\nIf history was Godfrey-Smith’s sole aim, the book could have been cut here. But that ultimately isn’t his full intention. The second aim of the book is to discuss the contemporary paradigms (or perhaps I should say “research traditions”) that dominated philosophy of science at the writing of his book—and they largely continue to hold sway today. The final aim is his advancing his own solutions to the core problems he identifies in the second part—though they all have roots in the discussions predominating the first. Throughout the book, he refers to these aims as “thirds” of the book—but, in reality, it’s more like history takes up the first 45% of the book before pivoting to\nThe chapter discussing naturalism more or less coincides with the transition into this pivot. Naturalism’s uptake comes nearer the end of the 20th century; but so do the other new agendas and perspectives that he focuses on. So he more or less abandons the chronological approach he took with the first section (though this doesn’t come as a surprise; he announces it as it happens). The chapters to come tackle “naturalistic” philosophies broadly, first on its own terms and then with regards to the “social structure of science;” It then pivots to “scientific realism”, a chapter on “explanation”, one on Bayesianism and evidence, and then one trying to harmonize the differences between empiricism, naturalism, and scientific realism. In this chapter, he retells the story of a reviewer warning that this final chapter has the potential to devolve into a “muddy paste.” As you might be able to guess from my use of the phrase earlier, it’s a critique that I largely agree with.\nBut the thing about paste, muddy or otherwise, is that it’s still pretty sticky. So while the concepts tended to blur together here3, the chapters does a pretty good job at having you stick around. This was some of the most engrossing material in the book, and I found my head moving quite vigorously at different parts of the book. It was mostly vertical when talking about how the aim of science is to use different “representational vehicles” (some linguistic, some mathematical, some visual, some computationally simulated, etc.) to provide an accurate representation of phen omena and relations observed in the real world—and when defending the broader tendency of science to invoke heretofore unobservable structures as part of a broader aim to explain what happens in the world. It did shake in the other direction when discussing Bayesianism because, at times, the approach appeared too glib and dismissive (such as the odd treatment of “Dutch books” and the thin objection to it on the grounds that some people may be averse to gambling)4. And I definitely found myself agreeing with the general conclusion to the book: That science is both an individual exercise and a social one. That scientists do aim to model the real world and that it is possible (and indeed advisable) to conditionally accept inductive arguments that have survived repeated testing with risky procedures. But also that scientists are engaged in a foundationally social enterprise and that we can only best understand their work (and the philosophy of science as well) as being influenced (and at times, constrained) by these social forces.\n\nAll in all, I think Theory and Reality accomplishes what it sets out to do, though with varying degrees of clarity and persuasiveness. What it does well, it does very well—and there are moments where it manages to do exactly that throughout the whole book. But those moments are more heavily concentrated in the first half and in the concluding segments of a few of the final chapters; these successes, and their asymmetric spread, also serves to more severely accentuate the places where it doesn’t do as well. Places where the positions either expressed or taken weren’t as clear. Places where it felt too rushed to do such a large chapter-topic justice.\nBut I believe that this book is well deserving of a read by those interested in the philosophy of science broadly. It is a good survey of many ideas in the field, it incorporates and takes seriously competing intellectual programs that don’t often get coverage in other locations (i.e., feminist epistemology and critiques of science), and offers persuasive accounts for what scientists do, how, and why. Indeed, when I finished the book my first main feeling was sadness. Not because it was bad (because it was quite good), not because it was over (because it’s hard for any philosophy book to be that good) but because, just this last year, Larry Dodd retired from his position at UF. I was sad because this would have been a perfect addition to the Scope syllabus had he still been teaching it. But it has inspired me to keep better track of books like these so that, maybe someday, I’ll also be able to offer interested folks a suite of books that track the evolution of knowledge (both what we know and how we know) over my own career. I think it speaks very highly of this work that I think it has the potential to assist in cementing in others the same love of philosophy, science, and philosophy of science that Scope did for me.\nThough, if I ever get the chance, I promise that I’ll do my best to stay shy of 30-something pages5.\n\n\n\n\nFootnotes\n\n\nThe age of the book may discourage some from picking it up. Honestly, though, I didn’t even know it was this old as I was reading it.. It honestly reads more like a book from the time of its release onto Audible (2017) than from nearly two decades prior. I’m not sure if that is a comment of Godfrey-Smith’s perspicaciousness as a writer (and his ability to distill arguments into their more “timeless” elements) or on the rate of change in the discussed fields. To take a page from his book—literally—it’s probably something in between.↩︎\nThough I did at times wonder if he was taking this argument a bit too far and, consequently, sparring with its straw man. I think he has a fair critique when he observes that theories and frames can strongly influence what we see but that their strength is fundamentally constrained by a shared external reality.↩︎\nI feel like a lot of the differences between contemporary empiricism, naturalism, and scientific realism tend to either be real but highly technical or largely imagined but wielded fiercely by invested practitioners. In either case, largely inconsequential for the lay reader.↩︎\nFor the curious, this could be articulated quite easily as someone merely having a larger degree of faith in the continuation of the status quo and in greater expressed uncertainties in alternatives to it.↩︎\nI solemnly swear: 25 pages max.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{licari2022,\n  author = {Peter Licari},\n  title = {Book {Review:} {Theory} and {Reality}},\n  date = {2022-11-07},\n  url = {https://www.peterlicari.com/posts/theory-reality-book-review},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPeter Licari. 2022. “Book Review: Theory and Reality.”\nNovember 7, 2022. https://www.peterlicari.com/posts/theory-reality-book-review."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html",
    "href": "posts/thoughts-prior-2020-election/index.html",
    "title": "Thoughts on the 2020 Election",
    "section": "",
    "text": "I thought I would post my prediction for the election and a few thoughts regarding it more generally. I’ll be peppering in some maps that I’ve made to help illustrate my general thought process."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#which-states-im-most-likely-to-be-wrong-about.",
    "href": "posts/thoughts-prior-2020-election/index.html#which-states-im-most-likely-to-be-wrong-about.",
    "title": "Thoughts on the 2020 Election",
    "section": "Which states I’m most likely to be wrong about.",
    "text": "Which states I’m most likely to be wrong about.\nThe states I’m most likely to get wrong are probably Georgia, North Carolina, Florida, and Maine-2. I think Maine-2 will be a decent indicator of how well the GOP’s senate chances will fair though; if that goes Blue, Collins is probably toast and any hopes that incumbents will be insulated from down-ballot effects stemming from Trump most likely go out the window. I’m less bullish on the senate flipping than others, but that will be a fair indicator of a bad night for the GOP all around."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#biden-has-a-strong-wall",
    "href": "posts/thoughts-prior-2020-election/index.html#biden-has-a-strong-wall",
    "title": "Thoughts on the 2020 Election",
    "section": "Biden has a strong wall",
    "text": "Biden has a strong wall\nThe reason I’m as bullish as I am for Biden (besides the appeal of the alliteration) is that the current toss-ups can literally all break for President Trump and Biden would still win—provided there aren’t any major surprises in the polling.\n\n\n\nNot all of these states are equal with regards to the President’s chances. No state’s return is independent of the others—if Biden won Texas, that means we’re looking at a bona fide blue malestrom, let alone a wave—but some states give more information about how other states are going to break than others. FiveThirtyEight has a tool that uses the correlations among polling errors to show how a victory in some states ripples out to signal what’s happening in others. A Trump win in Georgia doesn’t shift things much, nor does one in Texas, because frankly it’s a shock that they’re even in play. A win in Florida, however, drastically lowers Biden’s chances and suggests a map that looks like the one above. However, even in that case, the President is still not favored to win if only for the fact that Biden is still leading in the Electoral college.\n\nThe upshot however is that if Trump loses any of those states, his only path to victory lies in there being large, unprecedented degrees of polling error.\nI know, I know. Some of you might be thinking: “But Peter! The polls in 2016…” to which I will reply (for what I pray to all that is good and holy is the last time) that even the state level polls did reasonably well in 2016 in the aggregate—and more pollsters have been doing more high quality state-level polls than last time. There have also been adjustments across the industry to account for the biggest systematic source of polling error in 2016, which was not properly accounting for Trump’s support among White, non-college educated voters."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#bidens-best-cases",
    "href": "posts/thoughts-prior-2020-election/index.html#bidens-best-cases",
    "title": "Thoughts on the 2020 Election",
    "section": "Biden’s best case(s)",
    "text": "Biden’s best case(s)\nThe best case scenario for Biden is that he picks-up all of the toss-ups. But I should really emphasize that I don’t think this is a particularly likely outcome.\n\n\n\nA more realistic “great day” scenario for Biden is that he wins picks up Florida, North Carolina, Arizona, and Georgia while Trump gets Texas, Ohio, and Iowa.\n\n\n\nJudging from the correlations in polling errors, Florida will be a good (but not perfect) indicator of how Georgia and North Carolina go. (Georgia could be called first but that doesn’t necessarily mean that Florida will go with it; the leverage one state has over another is asymmetric. For example: North Carolina going red/blue means less for Texas’ eventual outcome than Texas going red/blue means for North Carolina.) Ohio will probably provide a good (but, again, not perfect) indication for how the other three states will break."
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#without-pennsylvania",
    "href": "posts/thoughts-prior-2020-election/index.html#without-pennsylvania",
    "title": "Thoughts on the 2020 Election",
    "section": "Without Pennsylvania",
    "text": "Without Pennsylvania\nJust because I expect the President to lose doesn’t mean I think his loss is inevitable. But it’s tough. Really tough. And it’s made all the tougher because he’s currently not favored in Pennsylvania. And by “not favored in Pennsylvania” I mean that The Economist is giving Biden a 95% chance of carrying the state. Trump will have to outperform the polls by a strong margin to carry the day. The most likely states, given the margin of error for the most recent state level polls, is Nevada, New Hampshire, Maine-2 and Nebraska-2. That will give Trump 270 votes in the Electoral College.\n\n\n\nBut I want to stress how unlikely that is. Many of these wins are not all that correlated with each other—meaning that a win in any of them early on doesn’t necessarily portend a Biden defeat. (Other state combinations are also possible, but we’d be talking about even less likely polling errors.)"
  },
  {
    "objectID": "posts/thoughts-prior-2020-election/index.html#with-pennsylvania",
    "href": "posts/thoughts-prior-2020-election/index.html#with-pennsylvania",
    "title": "Thoughts on the 2020 Election",
    "section": "With Pennsylvania",
    "text": "With Pennsylvania\nA trump victory in Pennsylvania dramatically improves the President’s chances. If the President wins there, it’s likely that he’s winning in a lot of other places that he’s currently not favored. However,it should be noted that even then he’d still need to pick up every one of the toss-up states. That can be helped by the fact that winning Pennsylvania would suggest a large degree of errors in the polls. But that degree of error is, I have to emphasize, really improbable. This outcome isn’t necessarily less likely than the one above, but pulling-off Pennsylvania would be an incredible upset. And, honestly, I would probably need to seriously reevaluate my life choices should it happen: especially my chosen profession.\n\n\n\nThe other point to consider when weighing out the importance of Pennsylvania is just how long that would make the contest. Basically, if Trump wins every toss-up state, then we’re not going to know the election’s outcome until about the end of the week at earliest. Because, as mentioned above, Pennsylvania counts their early votes last and haven’t started counting them at all. If Trump is ahead by the end of the night, we’ll just have to wait and see if the Biden advantage banked-up from early voting is enough. (Although, as a Floridian, I must confess that it’ll be nice to have all of the attention away from us for once.)\n\nIn short, in order for the President to win, he’s going to have to pull-off one helluva upset. That doesn’t make it impossible (upsets happen all the time) but, by definition, it makes it unlikely."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "This page houses the non-blog things I’ve written. Some are op-eds, others are in-depth analyses published in Medium publications (or self-published on the platform), others still are peer-reviewed. The list is more or less comprehensive from 2020 on. Though, prior to that, the things that aren’t on here…well, you’re honestly not missing much."
  },
  {
    "objectID": "writing.html#section",
    "href": "writing.html#section",
    "title": "Writing",
    "section": "2023",
    "text": "2023\n\n\n\nChris Stackaruk, PhD, Kevin Singer, Peter Licari (2023). “Christian Nationalism: A New Approach”. Neighborly Faith   link  pdf"
  },
  {
    "objectID": "writing.html#section-1",
    "href": "writing.html#section-1",
    "title": "Writing",
    "section": "2022",
    "text": "2022\n\n\n\nPeter Licari (2022). “States with Anti-Abortion Trigger Laws Score Higher on Measure of Hostile Sexism”. 3Streams   link  github\n\n\n\nPeter Licari (2022). “I ran 80,000 simulations to investigate different p-value adjustments”. Towards Data Science   link  github"
  },
  {
    "objectID": "writing.html#section-2",
    "href": "writing.html#section-2",
    "title": "Writing",
    "section": "2021",
    "text": "2021\n\n\n\nPeter Licari (2021). “White liberals view other races more warmly than they do Whites. Why?”. Medium (Self Published)   link\n\n\n\nPeter Licari (2021). “Political”Independents” Aren’t Moderates”. Medium (Self Published)   link\n\n\n\nStephen C. Philips, Alex P. Smith, Peter Licari (2021). “Philadelpha reconsidered: participant curation, the Gerry Committee, and US constitutional design”. Public Choice   link  pdf"
  },
  {
    "objectID": "writing.html#section-3",
    "href": "writing.html#section-3",
    "title": "Writing",
    "section": "2020",
    "text": "2020\n\n\n\nPeter Licari (2020). “Sharp as a Fox: Are foxnews.com Visitors Less Politically Knoweldgeable?”. American Politics Research   link  pdf\n\n\n\nPeter Licari (2020). “Animal Crossing is Exactly What We Need Right Now”. Medium (Self Published)   link\n\n\n\nPeter Licari (2020). “Press B to March: The Effects of Video Games on Political Behavior”. Doctoral Dissertation   pdf"
  },
  {
    "objectID": "writing.html#section-4",
    "href": "writing.html#section-4",
    "title": "Writing",
    "section": "2019",
    "text": "2019\n\n\n\nPeter Licari (2019). ““Serious” Fun: Social, Moral, and Political Content in Video Games”. The Strong Museum of Play Blog   link  pdf\n\n\n\nPeter Licari (2019). “Off to the Races: How proximity to a racetrack affected the vote to ban dog racing in Florida”. Towards Data Science   link  github\n\n\n\nPeter Licari, Michael Binder (2019). “The Hispanic Vote in Florida”. The 2016 Presidential Election in Florida: Ground Zero for America’s New Political Revolution   link"
  },
  {
    "objectID": "writing.html#section-5",
    "href": "writing.html#section-5",
    "title": "Writing",
    "section": "2017",
    "text": "2017\n\n\n\nPeter Licari (2017). “Do 7 percent of Americans actually think that chocolate milk comes from brown cows?”. Politics Means Politics   link"
  },
  {
    "objectID": "writing.html#section-6",
    "href": "writing.html#section-6",
    "title": "Writing",
    "section": "2016",
    "text": "2016\n\n\n\nPeter Licari, Jon Morris (2016). “The emotional election”. UF Journalism School Insights   link"
  },
  {
    "objectID": "writing.html#section-7",
    "href": "writing.html#section-7",
    "title": "Writing",
    "section": "2015",
    "text": "2015\n\n\n\nMichael McDonald, Peter Licari, Lia Merivaki (2015). “The big cost of using big data in elections”. The Washington Post   link"
  }
]