<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Peter Licari</title>
    <link>/tag/r/</link>
      <atom:link href="/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 09 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu7822abfb2f54ce27e2589169adb56259_3245_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>/tag/r/</link>
    </image>
    
    <item>
      <title>Investigating Why White Liberals rank non-White groups higher than Whites</title>
      <link>/post/white-liberals/</link>
      <pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/white-liberals/</guid>
      <description>
&lt;script src=&#34;/post/white-liberals/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is version 1.0.0 of this post. Read the note at the end for versioning information&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Over the last few weeks, I’ve been working on a project that investigates an interesting pattern in current American politics (or at least a pattern &lt;em&gt;I&lt;/em&gt; find interesting): White liberals tend to more strongly favor other races over Whites—at least when given the opportunity to rank the groups with a feeling thermometer. I’ve finished that project and I’ve published a general-audience write-up on Medium. You can check it out &lt;a href=&#34;#&#34;&gt;here&lt;/a&gt; if you’d like.&lt;/p&gt;
&lt;p&gt;The topic caught my attention because, generally, people tend to report greater feelings of favoritism towards their own race versus other races. That’s not to say that they rate other enthoracial groups &lt;em&gt;poorly&lt;/em&gt;—most people tend to rate other groups fairly highly, they just rate group that they identify with &lt;strong&gt;more&lt;/strong&gt; positively. This pro-ingroup bias is so widespread that the deviation of White liberals away from the pattern made the little hamster in my brain hop onto his wheel and start trotting. The last data on the topic that I could find was based off the 2018 ANES pilot study; would the pattern continue into 2020? What sort of factors might it be driven by?&lt;/p&gt;
&lt;p&gt;By and large, I found that White liberals continued to have this pattern of favoring other groups over Whites. The gaps were larger among respondents who were younger, who were more strongly liberal, who were more politically interested, and who had lower feelings of linked-fate with Whites in general.&lt;/p&gt;
&lt;p&gt;I wanted to write this post because I wanted to have a place where I shed some light on my methodology and coding in a way that isn’t as conducive to the kind of writing I was going for in the main piece. If you’re interested in that stuff, read on! If you’re simply interested in the code I used to analyze and make the figures, here’s a (link to the github repository where I’ve stored it)[&lt;a href=&#34;https://github.com/prlitics/Research-Projects/blob/main/White-Liberals-2021/analysis.R&#34; class=&#34;uri&#34;&gt;https://github.com/prlitics/Research-Projects/blob/main/White-Liberals-2021/analysis.R&lt;/a&gt;]. No hard feelings! You do you, boo.&lt;/p&gt;
&lt;p&gt;This post is broken down into four parts: Data transformation, hypotheses, modeling and anaysis, and visualization.&lt;/p&gt;
&lt;div id=&#34;data-transformation.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data transformation.&lt;/h2&gt;
&lt;p&gt;The data I used for the post came from the 2020 American National Election Survey (or ANES). The ANES is an indispensible resource for people studying political behavior in the American context. Thankfully, James Martherus has published the &lt;a href=&#34;https://github.com/jamesmartherus/anesr&#34;&gt;&lt;code&gt;{anesr}&lt;/code&gt; package&lt;/a&gt;, which has dang-near all of the data the project has collected over its decades-long span. I used this package to get a dataframe of the 2020 ANES.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If you don&amp;#39;t have the package, uncomment and run this next line
# devtools::install_github(&amp;quot;jamesmartherus/anesr&amp;quot;)
library(anesr)

data(&amp;quot;timeseries_2020&amp;quot;)
anes20 &amp;lt;- timeseries_2020
dim(anes20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8280 1381&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(anes20[1:5], n =10))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;version&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;V200001&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;V160001_orig&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;V200002&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;V200003&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;401318&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200022&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;300261&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200039&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;400181&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200046&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;300171&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200053&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;405145&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200060&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;400374&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200084&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;407013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200091&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;407174&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;406264&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ANES2020TimeSeries_20210324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;200114&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;402782&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The tibble has over &lt;em&gt;1,300&lt;/em&gt; columns!! There’s no reason I need that many variables. Also, the labels are given the official variable names given in the data set—which are less than informative. I downloaded the &lt;a href=&#34;https://electionstudies.org/wp-content/uploads/2021/07/anes_timeseries_2020_questionnaire_20210719.pdf&#34;&gt;questionnaire&lt;/a&gt; from the Election Studies site and found the variables I was most interested in, and renamed them to something more sensible. For this I needed &lt;code&gt;{dplyr}&lt;/code&gt;—but I figured that I might as well just bring in the whole &lt;code&gt;{tidyverse}&lt;/code&gt; in case I needed something from &lt;code&gt;{purrr}&lt;/code&gt; or &lt;code&gt;{tidyr}&lt;/code&gt; later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df &amp;lt;- anes20 %&amp;gt;%
  select(id = V200001,ideo = V201200, 
         feeling_blk = V202480,
         fate_wht = V202505, feeling_wht = V202482,
         feeling_asn = V202478, 
         feeling_hisp = V202479, 
         weight = V200010a,
         age = V201507x, race_eth = V201549x, sex = V201600, educ = V201511x,
         interest = V202406,
         pid = V201231x, mode = V200002)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was a bit of an iterative process. I knew that, at bare minimum, I needed to have political ideology and race so that I could hone-in on my audience of choice (White liberals), and I knew I’d need the feeling thermometer variables for the different racial groups. Everything else reflected things that I immediately thought my have explanatory power (age, education, gender), things I thought would be important to control for (party identity and whether the respondents were interviewed face-to-face or online), things that I realized would be important for future data transformation steps (respondent ID), or things that I realized could be interesting to include as I took time to reflect and read more into the codebook (political interest and linked-fate.) I feel like this is a really common pattern in research; the variables you select are both driven by theoretical priors but also by necessity and by curiosity driven by initial findings. (Full transparency, I ran initial models that didn’t include interest and linked fate and then thought that it’d be fun to include them afterwards. I feel it’s better to report on the full model that gets chosen rather than present it in a step-wise way—at least so long as you’re not cherry-picking whatever specification gives you the sexiest outcomes. A hard siren’s song to resist to be sure…)&lt;/p&gt;
&lt;p&gt;I have &lt;em&gt;ample&lt;/em&gt; prior history with the ANES (I used it a bunch in grad school, used it for my &lt;a href=&#34;https://journals.sagepub.com/doi/full/10.1177/1532673X20915222&#34;&gt;published research&lt;/a&gt;, and used it extensively when I taught classes prior to graduating and leaving UF)—so I knew that data that could broadly be considered “missing” (the question not being asked, people not providing answers &lt;em&gt;when&lt;/em&gt; asked, etc.) are given negative values or values over 100. So I made an anonymous function in a &lt;code&gt;mutate(across())&lt;/code&gt; call that would assign these values, more appropriately in my personal opinion, to &lt;code&gt;NA&lt;/code&gt;. This drops them from the analysis I planned to run. After that, I filtered the dataset so that I was only looking at those who identified as liberal and as non-Hispanic White.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(across( c(everything(),-id), function(x){ifelse(x &amp;lt; 0 | x &amp;gt;= 100, NA, x)})) %&amp;gt;%
  filter(ideo %in% c(1:3) &amp;amp; race_eth == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this last bit, I constructed my dependent variables—the gaps between people’s ratings of Whites and their ratings of Asians, Hispanics, and Blacks (when they offered answers to the question). I then reduced my education variable down to a dummy that looked at whether they had gotten a college degree or not. (I was thinking that there could be an interaction effect betweeh that level of degree attainment and age on the size of the gap, but I ultimately didn’t find much in support of that). Last but not least, I selected only the variables that I was going to need for the rest of the analysis. I tend to like only working with as much as I need; calculations go faster that way and I can look at things more clearly without poking around across other variables that I don’t actually need or care about.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(feel_bw = feeling_blk - feeling_wht,
         feel_aw = feeling_asn - feeling_wht,
         feel_hw = feeling_hisp - feeling_wht
  ) %&amp;gt;%
  mutate(college = ifelse(educ %in% 4:5,1,0)) %&amp;gt;%
  select(starts_with(&amp;#39;feel_&amp;#39;), fate_wht, id, ideo, age, sex, college, pid, mode, interest)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypotheses&lt;/h2&gt;
&lt;p&gt;As I mentioned above, my hypothesis-generating process was a bit reflexive; I had some variables I was originally interested in to be sure, but then others came along after I got my initial run of the analysis.&lt;/p&gt;
&lt;p&gt;My first set of hypotheses were basically these:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Age would be negatively associated with levels of pro-outgroup bias. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Education would be positively associated with levels of pro-outgroup bias. Among White liberals, I expected education would matter.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. There’d be a significant interaction between age and education. Basically, I thought that young, &lt;em&gt;educated&lt;/em&gt; White liberals would be different than young White liberals without a college degree—and that the latter group would be more similar to older liberals, educated or otherwise. Basically, I was wondering to what extent this was driven by socialization factors among more recent graduates who have more experience investigating concepts like race more critically.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Those who identified as “more” liberal would have higher levels of pro-outgroup bias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From this first flight, it turns out only 1 and 4 turned out to have any support in the data. &lt;em&gt;c’est la vie.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After that, I spent a few days ruminating on the findings and thought “I wonder if political interest matters. At the very least, it should probably be a control variable…” This is because people who are interested in politics are not only exposed to more in-party cues on salient topics, they’re also, broadly, more receptive. Since Race is a central part of the Democratic party’s identity right now, those who pay more attention probably had larger out-group biases. Hence hypothesis number 5&#34;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. White liberals who are more interested in politics will have higher pro-outgroup biases than those with little to know interest.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To be entirely honest, hypothesis 6 came as I scrolled through the codebook to rectify a coding error. (This error initially had me coming away with the opposite conclusion. The lesson, kids: Always check that your renamed variables are based off the variables you &lt;em&gt;think&lt;/em&gt; they are). I happend to see linked fate in the description and thought “oh yeah! I remember reading up on that in my political socialization class. Fun times. You’ll probably see lower out-group biases in people with lower affective linkages to Whites overall; let’s chuck that in!”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. I chucked it in for exactly the reason I wrote a second ago.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All in all, 1, 4, 5, and 6 found significant evidence supporting them. At least with my modeling strategy.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;And that, friends, is called a “segue.”&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-and-anaysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling and Anaysis&lt;/h2&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling&lt;/h3&gt;
&lt;p&gt;A fair portion of my model choice stemmed from a single phrase from the piece where the pro-outgroup bias fact originated from (so far as I can tell at least): Work by PhD Candidate Zach Goldberg. It inspired a pretty famous piece by Matt Yglesias and Goldberg followed it up with a longer essay. It was here where I found the phrase.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A related graph below displays the average differences in feelings of warmth (measured along a 0-100 scale) toward whites vs. nonwhites (i.e., Asians, Hispanics, and blacks) across different subgroups.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The word &lt;em&gt;“average”&lt;/em&gt; is what got me. How would one calculate the average? Simply add up the gaps from Blacks vs Whites, Asian vs Whites, and Hispanics vs Whites and divide by 3? That would only work if respondents provided non-empty answers for all three, and that wasn’t the case. I could just narrow it down to those people who &lt;em&gt;did&lt;/em&gt; provide non-empty answers for all three, but I felt that was unnecessarily limiting the available data.&lt;/p&gt;
&lt;p&gt;Since I’ve been doing a fair amount of multilevel modeling at work, I figured that I’d transform the data so that &lt;em&gt;each individual comparison&lt;/em&gt; would be a row, with up to 3 observations nested within each respondent. With that nesting, I opted for a multilevel model where I could make respondent-level random slopes. This would account for the fact that some people may be inclined to rank out-groups higher/lower in general—and the partial pooling would help for those who had fewer comparisons. I could then add a set of dummy variables to the model that controlled for which of the three comparisons was being made. And since the dependent variable could theoretically span between -100–100 (though, in practice, it spanned from -70–90), I went with a linear model. After converting the data to long via &lt;code&gt;pivot_longer&lt;/code&gt;, I used the &lt;code&gt;lme&lt;/code&gt; function from &lt;code&gt;{lme4}&lt;/code&gt;. The &lt;code&gt;(1|id)&lt;/code&gt; syntax means that I am specifying random intercepts based off of respondent id.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;lme4&amp;#39; was built under R version 4.0.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- df %&amp;gt;%
  pivot_longer(!c(id, ideo, age, sex, college, fate_wht, pid, mode, interest))

mod &amp;lt;- lmer(value ~ ideo + fate_wht + age*as.factor(college) + as.factor(sex) + 
              pid + as.factor(mode) + as.factor(name) + interest + (1|id), data = df1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Analysis&lt;/h3&gt;
&lt;p&gt;The code below generates the summary of the &lt;code&gt;lme&lt;/code&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: value ~ ideo + fate_wht + age * as.factor(college) + as.factor(sex) +  
##     pid + as.factor(mode) + as.factor(name) + interest + (1 |      id)
##    Data: df1
## 
## REML criterion at convergence: 21890
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.3445 -0.1458 -0.0312  0.1481  4.5567 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  id       (Intercept) 200.87   14.173  
##  Residual              34.59    5.882  
## Number of obs: 2920, groups:  id, 1228
## 
## Fixed effects:
##                         Estimate Std. Error t value
## (Intercept)             31.22241    3.61862   8.628
## ideo                    -4.06849    0.66269  -6.139
## fate_wht                 1.96045    0.52235   3.753
## age                     -0.29996    0.04062  -7.384
## as.factor(college)1     -3.70728    2.82027  -1.315
## as.factor(sex)2          1.92603    0.87248   2.208
## pid                     -1.37348    0.33628  -4.084
## as.factor(mode)2         4.33555    4.62419   0.938
## as.factor(mode)3        -0.98535    1.80398  -0.546
## as.factor(name)feel_bw   1.03711    0.32352   3.206
## as.factor(name)feel_hw   0.70037    0.32388   2.162
## interest                -2.20656    0.60049  -3.675
## age:as.factor(college)1  0.09430    0.05087   1.854&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;I honestly struggled &lt;em&gt;a lot&lt;/em&gt; with the visualizations in this piece. I’m a big fan of the conept of showing as much of the data as possible without sacrificing the intended message of visualiztion. I honestly believe that the sizes of the associations between the significant findings are (mostly) substantial. We’re talking about gaps of 5-15 points on a 100 point scale—one where most people cluster towards the middle-high values. However, there’s so much heterogeneity—and so many outliers—that I worried that, in some cases, showing literally all of the data would make the effects appear smaller, and less consequential than they are. So I made the decision, when dealing with the raw data, to truncate the data to the the middle 95%. The analyses were run on the full set, but I felt that this would show both the substantial heterogeneity as well as the significant overall trend.&lt;/p&gt;
&lt;p&gt;I ran into a similar issue when trying to plot the individual effects. It dawns on me now that I should’ve just the &lt;code&gt;dydx&lt;/code&gt; of the margins rather than the predicted levels—but I think that the predictions still delivers the point across. Here too, I was tempted to show the actual data but the gaps were (understandably) different in the raw counts than suggested by the regression model. Not so much that the raw data contradicts what the models show—I’m a big fan of the idea that, if it doesn’t exist in the cross-tabs, it probably doesn’t (really) exist in the regression model—but that the difference in magnitude might cause confusion to the readers. I didn’t want single points, either because they seemed too certain. So I opted for bootstrapped confidence intervals. Which, I hope, accurately conveys both the pattern as well as the uncertainty in the point estimates that it rests on.&lt;/p&gt;
&lt;div id=&#34;thats-it-thank-you-for-reading-please-let-me-know-what-you-think&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;That’s it! Thank you for reading! Please let me know what you think!&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;This is version &lt;strong&gt;1.0.0&lt;/strong&gt; of this analysis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I believe all work benefits from readers’ constructive feedback as well as the writer’s own revisiting and reflection. But not all work is fit for academic publication. To that end, for the sake of transparency, I have decided to visibly index what point my non-academic projects are at once they go live in a format where others will (hopefully) come into contact with it. Minor revisions (such as grammar or minor image formatting issues) lead to an increase in the third digit. Major revisions inspired by my own revisiting and minor revisions inspired by the suggestion of readers leads to an increase in the second digit. Major revisions driven by the suggestion of readers or by future reflection and revisiting the project leads to the an increase in the first digit. After 6 months of no updates, a version should be considered “final.” The current version was published 10/9/21. All versions of this post are maintained on &lt;a href=&#34;https://github.com/prlitics/Research-Projects/tree/main/White-Liberals-2021&#34;&gt;Github&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>dumdum</title>
      <link>/project/fl-ballot-app/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/project/fl-ballot-app/</guid>
      <description>&lt;p&gt;Inspired by all of the chaos and craziness that was mail-in and drop-off voting in the 2020 election, my students from my 2020 Election Data Science class and I created an app designed to help people find ballot-drop-off and early voting locations. The app received thousands of hits, helped at least a few people safely find ballot drop-off locations, and is now archived on my GitHub.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>dumdum</title>
      <link>/project/dumdum/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/dumdum/</guid>
      <description>&lt;p&gt;&lt;code&gt;dumdum&lt;/code&gt; is an R package that makes making dummy variables easier. There are functions that do this sort of work in packages like &lt;code&gt;caret&lt;/code&gt;, but I wanted to make a set of functions that would help other social scientists and/or analysts who were not as deep down the machine-learning rabbit hole. Click above to the GitHub site for how to download it and use it yourself!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
